{
    "docs": [
        {
            "location": "/",
            "text": "La Era de las m\u00e1quinas inteligentes\n\n\n\n\nLa \nInteligencia Artificial\n es una tecnolog\u00eda innovadora que comienza a abandonar el laboratorio y los libros de ciencia ficci\u00f3n para convertirse en una realidad que puede transformar nuestra sociedad. La podemos encontrar en nuestras casas, en la oficina, y dem\u00e1s instituciones que impulsan la econom\u00eda global. Desde \nAlexa\n a \nNest\n, a \nSiri\n, a \nUber\n, y a \nWaze\n, estamos rodeados de m\u00e1quinas inteligentes que funcionan en plataformas incre\u00edblemente poderosas de software de aprendizaje. Y esto es s\u00f3lo el principio.\n\n\nHasta la fecha, hemos estado disfrutando, sin pr\u00e1cticamente darnos cuenta, diversas formas de \nInteligencia Artificial d\u00e9bil\n. Con ella por ejemplo \nAmazon\n realiza sus recomendaciones; o \nNetflix\n sugiere la pel\u00edcula perfecta para una noche de domingo; o \nFacebook\n decide que nos puede interesar ver en nuestra newsfeed. Estas formas de \nIA\n son como peque\u00f1os ayudantes, que hacen que nuestros d\u00edas sean un poco m\u00e1s f\u00e1ciles y m\u00e1s divertidos.\n\n\nLas nuevas m\u00e1quinas\n\n\nA partir de los nuevos desarrollos tecnol\u00f3gicos, sobre todo de empresas que compiten por el conocimiento, como \nFacebook\n o \nGoogle\n est\u00e1n comenzado a surgir un nuevo tipo de m\u00e1quinas; a los que podr\u00edamos llamar \nsistemas inteligentes\n. Estos \nsistemas inteligentes\n combinan \nsoftware\n, en la forma de \nalgoritmos\n, reglas de negocio, \nMachine Learning\n, an\u00e1lisis predictivos, etc; \nhardware\n, servidores, sensores, dispositivos m\u00f3viles y conectividad; \nDatos\n, ya sean contextualizados o en tiempo real; junto el aporte de nuestra experiencia y conocimientos adquiridos. \nLas principales caracter\u00edsticas que hacen a estos \nsistemas inteligentes\n son entonces:\n\n\n\n\n\n\nSoftware que aprende\n: El software detr\u00e1s de estas \nnuevas m\u00e1quinas\n va a ser algo muy distinto a lo que ven\u00edamos viendo. Por primera vez en la historia vamos a tener herramientas que se \nmodifican a s\u00ed mismas\n. Con la ayuda de \nMachine Learning\n estos sistemas mejorar\u00e1n en forma aut\u00f3noma. El sistema aprender\u00e1 como reconocer patrones y encontrar informaci\u00f3n oculta en los datos, todo ello sin ser expl\u00edcitamente programado para ello.\n\n\n\n\n\n\nHardware con un poder masivo de computo\n: Durante las \u00faltimas d\u00e9cadas, hemos visto como el poder de computo de nuestro hardware se ha incrementado exponencialmente. De acuerdo a la \nley de Moore\n el poder de procesamiento de nuestras computadoras se duplica cada dos a\u00f1os. Si a este crecimiento, le sumamos la innovaci\u00f3n tecnol\u00f3gica de la \ncomputaci\u00f3n en la nube\n, podemos contar con una infraestructura de computo ultra poderosa que nos permite procesar gran cantidad de informaci\u00f3n en segundos.\n\n\n\n\n\n\nGrandes cantidades de datos\n: Los datos van a ser el combustible de esta nueva econom\u00eda. Gracias a la explosi\u00f3n de los dispositivos m\u00f3viles con sensores y a las redes inal\u00e1mbricas se ha hecho sumamente sencillo recolectar datos de casi cualquier cosa que se nos pueda imaginar. Cualquier movimiento que hagamos, deja una huella en forma de datos que estas nuevas m\u00e1quinas pueden utilizar para aprender u obtener informaci\u00f3n.  \n\n\n\n\n\n\nEstas tres caracter\u00edsticas combinadas son las que le dar\u00e1n \nvida\n a estas \nnuevas m\u00e1quinas\n o \nsistemas inteligentes\n.\n\n\nLos datos son el nuevo petr\u00f3leo\n\n\nCada revoluci\u00f3n industrial puede ser caracterizada por una materia prima: el carb\u00f3n, el acero, el petr\u00f3leo, la electricidad. En esta nueva revoluci\u00f3n que esta surgiendo, el ingrediente principal son los \nDatos\n. Como el petr\u00f3leo, deben ser \nminados\n, \nrefinados\n, y \ndistribuidos\n; pero con la diferencia de que los \nDatos\n son polifac\u00e9ticos y son un recurso pr\u00e1cticamente infinito que puede crecer y escalar en forma muy r\u00e1pida. Los \nDatos\n pueden ser incre\u00edblemente valiosos o valer absolutamente nada dependiendo de la forma y los \nlentes\n con que se manejan. Los l\u00edderes del ma\u00f1ana van a necesitar entender como trabajar con este recurso, el cual esta disponible para cualquiera de nosotros, para poder transformarlos en una ventaja competitiva. \n\n\nEl boom de la automatizaci\u00f3n\n\n\nActualmente, cada vez son m\u00e1s las tareas repetitivas que se est\u00e1n automatizando; pero una vez que las \nnuevas m\u00e1quinas\n sean utilizadas en este proceso, cada vez ser\u00e1 m\u00e1s grande el abanico de tareas que se ir\u00e1n automatizando. Incluso aquellas tareas que cre\u00edamos exclusivas de los humanos por su complejidad intelectual caer\u00e1n en la automatizaci\u00f3n. El costo que se ahorre por los nuevos niveles de automatizaci\u00f3n que surja de este proceso, proporcionar\u00e1 el dinero necesario para invertir en nuevas ideas y mercados. La automatizaci\u00f3n no es un fin en s\u00ed mismo, sino un medio que nos permitir\u00e1 liberarnos para utilizar nuestras energ\u00edas en nuevos desaf\u00edos.  \n\n\nLa cuarta revoluci\u00f3n industrial\n\n\nLa Primera Revoluci\u00f3n Industrial fue impulsada por la invenci\u00f3n del telar, la segunda por la m\u00e1quina de vapor, y la tercera por la l\u00ednea de montaje; ahora ya se comienza a hablar de una cuarta revoluci\u00f3n industrial que ser\u00e1 alimentada por \nm\u00e1quinas que parecen pensar\n o \nsistemas de inteligentes\n.\nA partir del crecimiento de est\u00e1s tecnolog\u00edas, los l\u00edderes y gerentes de las organizaciones pueden tener un conocimiento  continuo de todo lo que est\u00e1 ocurriendo con sus operaciones. Donde antes sol\u00edamos adivinar, ahora podremos predecirlo con una alta probabilidad de acertar. Estas nuevas m\u00e1quinas \nsiempre alertas\n, \nsiempre aprendiendo\n y \nconstantemente pensando\n pronto desafiar\u00e1n y mejorar\u00e1n el intelecto y la experiencia de los profesionales m\u00e1s experimentados en todos los sectores. No hay manera de escapar de la revoluci\u00f3n que proponen estas nuevas m\u00e1quinas y los modelos de negocio que permiten aprovecharlos. Decidir qu\u00e9 hacer con estas nuevas m\u00e1quinas inteligentes -este nuevo c\u00f3ctel de \nIA\n, \nalgoritmos\n, \nbots\n y \nBig Data\n, ser\u00e1 el factor determinante del \u00e9xito futuro .\n\n\nIniciativas desde IAAR\n\n\nDesde \nIAAR\n, la comunidad argentina de \nInteligencia Artificial\n, creemos que es muy importante ser conscientes de los cambios que puede provocar esta tecnolog\u00eda disruptiva y qu\u00e9 debemos comenzar a debatir sobre los mismos. Por esta raz\u00f3n tenemos iniciativas como nuestro \ngrupo de debate\n y este mismo \nsitio\n con lo que queremos acercar al p\u00fablico general conceptos tales como \nInteligencia Artificial\n, \nDeep Learning\n, \nMachine Learning\n, \nCiencia de datos\n, \nBig Data\n, \nInternet de las cosas\n, \nprocesamiento del lenguaje natural\n; y dem\u00e1s. Los invitamos a disfrutar del contenido y unirse a la comunidad.",
            "title": "Introducci\u00f3n"
        },
        {
            "location": "/#la-era-de-las-maquinas-inteligentes",
            "text": "La  Inteligencia Artificial  es una tecnolog\u00eda innovadora que comienza a abandonar el laboratorio y los libros de ciencia ficci\u00f3n para convertirse en una realidad que puede transformar nuestra sociedad. La podemos encontrar en nuestras casas, en la oficina, y dem\u00e1s instituciones que impulsan la econom\u00eda global. Desde  Alexa  a  Nest , a  Siri , a  Uber , y a  Waze , estamos rodeados de m\u00e1quinas inteligentes que funcionan en plataformas incre\u00edblemente poderosas de software de aprendizaje. Y esto es s\u00f3lo el principio.  Hasta la fecha, hemos estado disfrutando, sin pr\u00e1cticamente darnos cuenta, diversas formas de  Inteligencia Artificial d\u00e9bil . Con ella por ejemplo  Amazon  realiza sus recomendaciones; o  Netflix  sugiere la pel\u00edcula perfecta para una noche de domingo; o  Facebook  decide que nos puede interesar ver en nuestra newsfeed. Estas formas de  IA  son como peque\u00f1os ayudantes, que hacen que nuestros d\u00edas sean un poco m\u00e1s f\u00e1ciles y m\u00e1s divertidos.",
            "title": "La Era de las m\u00e1quinas inteligentes"
        },
        {
            "location": "/#las-nuevas-maquinas",
            "text": "A partir de los nuevos desarrollos tecnol\u00f3gicos, sobre todo de empresas que compiten por el conocimiento, como  Facebook  o  Google  est\u00e1n comenzado a surgir un nuevo tipo de m\u00e1quinas; a los que podr\u00edamos llamar  sistemas inteligentes . Estos  sistemas inteligentes  combinan  software , en la forma de  algoritmos , reglas de negocio,  Machine Learning , an\u00e1lisis predictivos, etc;  hardware , servidores, sensores, dispositivos m\u00f3viles y conectividad;  Datos , ya sean contextualizados o en tiempo real; junto el aporte de nuestra experiencia y conocimientos adquiridos. \nLas principales caracter\u00edsticas que hacen a estos  sistemas inteligentes  son entonces:    Software que aprende : El software detr\u00e1s de estas  nuevas m\u00e1quinas  va a ser algo muy distinto a lo que ven\u00edamos viendo. Por primera vez en la historia vamos a tener herramientas que se  modifican a s\u00ed mismas . Con la ayuda de  Machine Learning  estos sistemas mejorar\u00e1n en forma aut\u00f3noma. El sistema aprender\u00e1 como reconocer patrones y encontrar informaci\u00f3n oculta en los datos, todo ello sin ser expl\u00edcitamente programado para ello.    Hardware con un poder masivo de computo : Durante las \u00faltimas d\u00e9cadas, hemos visto como el poder de computo de nuestro hardware se ha incrementado exponencialmente. De acuerdo a la  ley de Moore  el poder de procesamiento de nuestras computadoras se duplica cada dos a\u00f1os. Si a este crecimiento, le sumamos la innovaci\u00f3n tecnol\u00f3gica de la  computaci\u00f3n en la nube , podemos contar con una infraestructura de computo ultra poderosa que nos permite procesar gran cantidad de informaci\u00f3n en segundos.    Grandes cantidades de datos : Los datos van a ser el combustible de esta nueva econom\u00eda. Gracias a la explosi\u00f3n de los dispositivos m\u00f3viles con sensores y a las redes inal\u00e1mbricas se ha hecho sumamente sencillo recolectar datos de casi cualquier cosa que se nos pueda imaginar. Cualquier movimiento que hagamos, deja una huella en forma de datos que estas nuevas m\u00e1quinas pueden utilizar para aprender u obtener informaci\u00f3n.      Estas tres caracter\u00edsticas combinadas son las que le dar\u00e1n  vida  a estas  nuevas m\u00e1quinas  o  sistemas inteligentes .",
            "title": "Las nuevas m\u00e1quinas"
        },
        {
            "location": "/#los-datos-son-el-nuevo-petroleo",
            "text": "Cada revoluci\u00f3n industrial puede ser caracterizada por una materia prima: el carb\u00f3n, el acero, el petr\u00f3leo, la electricidad. En esta nueva revoluci\u00f3n que esta surgiendo, el ingrediente principal son los  Datos . Como el petr\u00f3leo, deben ser  minados ,  refinados , y  distribuidos ; pero con la diferencia de que los  Datos  son polifac\u00e9ticos y son un recurso pr\u00e1cticamente infinito que puede crecer y escalar en forma muy r\u00e1pida. Los  Datos  pueden ser incre\u00edblemente valiosos o valer absolutamente nada dependiendo de la forma y los  lentes  con que se manejan. Los l\u00edderes del ma\u00f1ana van a necesitar entender como trabajar con este recurso, el cual esta disponible para cualquiera de nosotros, para poder transformarlos en una ventaja competitiva.",
            "title": "Los datos son el nuevo petr\u00f3leo"
        },
        {
            "location": "/#el-boom-de-la-automatizacion",
            "text": "Actualmente, cada vez son m\u00e1s las tareas repetitivas que se est\u00e1n automatizando; pero una vez que las  nuevas m\u00e1quinas  sean utilizadas en este proceso, cada vez ser\u00e1 m\u00e1s grande el abanico de tareas que se ir\u00e1n automatizando. Incluso aquellas tareas que cre\u00edamos exclusivas de los humanos por su complejidad intelectual caer\u00e1n en la automatizaci\u00f3n. El costo que se ahorre por los nuevos niveles de automatizaci\u00f3n que surja de este proceso, proporcionar\u00e1 el dinero necesario para invertir en nuevas ideas y mercados. La automatizaci\u00f3n no es un fin en s\u00ed mismo, sino un medio que nos permitir\u00e1 liberarnos para utilizar nuestras energ\u00edas en nuevos desaf\u00edos.",
            "title": "El boom de la automatizaci\u00f3n"
        },
        {
            "location": "/#la-cuarta-revolucion-industrial",
            "text": "La Primera Revoluci\u00f3n Industrial fue impulsada por la invenci\u00f3n del telar, la segunda por la m\u00e1quina de vapor, y la tercera por la l\u00ednea de montaje; ahora ya se comienza a hablar de una cuarta revoluci\u00f3n industrial que ser\u00e1 alimentada por  m\u00e1quinas que parecen pensar  o  sistemas de inteligentes .\nA partir del crecimiento de est\u00e1s tecnolog\u00edas, los l\u00edderes y gerentes de las organizaciones pueden tener un conocimiento  continuo de todo lo que est\u00e1 ocurriendo con sus operaciones. Donde antes sol\u00edamos adivinar, ahora podremos predecirlo con una alta probabilidad de acertar. Estas nuevas m\u00e1quinas  siempre alertas ,  siempre aprendiendo  y  constantemente pensando  pronto desafiar\u00e1n y mejorar\u00e1n el intelecto y la experiencia de los profesionales m\u00e1s experimentados en todos los sectores. No hay manera de escapar de la revoluci\u00f3n que proponen estas nuevas m\u00e1quinas y los modelos de negocio que permiten aprovecharlos. Decidir qu\u00e9 hacer con estas nuevas m\u00e1quinas inteligentes -este nuevo c\u00f3ctel de  IA ,  algoritmos ,  bots  y  Big Data , ser\u00e1 el factor determinante del \u00e9xito futuro .",
            "title": "La cuarta revoluci\u00f3n industrial"
        },
        {
            "location": "/#iniciativas-desde-iaar",
            "text": "Desde  IAAR , la comunidad argentina de  Inteligencia Artificial , creemos que es muy importante ser conscientes de los cambios que puede provocar esta tecnolog\u00eda disruptiva y qu\u00e9 debemos comenzar a debatir sobre los mismos. Por esta raz\u00f3n tenemos iniciativas como nuestro  grupo de debate  y este mismo  sitio  con lo que queremos acercar al p\u00fablico general conceptos tales como  Inteligencia Artificial ,  Deep Learning ,  Machine Learning ,  Ciencia de datos ,  Big Data ,  Internet de las cosas ,  procesamiento del lenguaje natural ; y dem\u00e1s. Los invitamos a disfrutar del contenido y unirse a la comunidad.",
            "title": "Iniciativas desde IAAR"
        },
        {
            "location": "/inteligencia-artificial/",
            "text": "Introducci\u00f3n a la Inteligencia Artificial\n\n\n\n\nEl cerebro es el \u00f3rgano m\u00e1s incre\u00edble del cuerpo humano. Establece la forma en que percibimos las im\u00e1genes, el sonido, los olores, los sabores y el tacto. Nos permite almacenar recuerdos, experimentar emociones e incluso so\u00f1ar. Sin el, ser\u00edamos organismos primitivos, incapaces de otra cosa que el m\u00e1s simple de los reflejos. El cerebro es, en definitiva, lo que nos hace inteligentes.\nDurante d\u00e9cadas hemos so\u00f1ado con construir m\u00e1quinas inteligentes con cerebros como los nuestros; asistentes robotizados para limpiar nuestras casas, coches que se conducen por s\u00ed mismos, microscopios que detecten enfermedades autom\u00e1ticamente. Pero construir estas m\u00e1quinas \nartificialmente inteligentes\n nos obliga a resolver aldgunos de los problemas computacionales m\u00e1s complejos que hemos tenido; problemas que nuestros cerebros ya pueden resolver en una fracci\u00f3n de segundos. La forma de atacar y resolver estos problemas, es el campo de estudio de la \nInteligencia Artificial\n.\n\n\n\u00bfQu\u00e9 es la Inteligencia Artificial?\n\n\nDefinir el concepto de \nInteligencia Artificial\n no es nada f\u00e1cil. Una definici\u00f3n sumamente general ser\u00eda que la \nIA\n es el estudio de la \ninfrom\u00e1tica\n centr\u00e1ndose en el desarrollo de software o \nm\u00e1quinas que exhiben una inteligencia humana\n. \n\n\nObjetivos de la Inteligencia Artificial\n\n\nLos objetivos principales de la \nIA\n incluyen la deducci\u00f3n y el razonamiento, la representaci\u00f3n del conocimiento, la planificaci\u00f3n, el procesamiento del lenguaje natural (\nNLP\n), el aprendizaje, la percepci\u00f3n y la capacidad de manipular y mover objetos. Los objetivos a largo plazo incluyen el logro de la Creatividad, la Inteligencia Social y la Inteligencia General (a nivel Humano).\n\n\nCuatro enfoques distintos\n\n\nPodemos distinguir cuatro enfoques distintos de abordar el problema de la \nInteligencia Artificial\n. \n\n\n\n\n\n\nSistemas que se comportan como humanos\n: Aqu\u00ed la idea es desarrollar m\u00e1quinas capaces de realizar funciones para las cuales se requerir\u00eda un humano inteligente. Dentro de este enfoque podemos encontrar la famosa \nPrueba de Turing\n. Para poder superar esta prueba, la m\u00e1quina deber\u00eda poseer las siguientes capacidades:\n\n\n\nProcesamiento de lenguaje natural\n, que le permita comunicarse satisfactoriamente.\n\n\nRepresentaci\u00f3n del conocimiento\n, para almacenar lo que se conoce o se siente.\n\n\nRazonamiento autom\u00e1tico\n, para utilizar la informaci\u00f3n almacenada para responder a preguntas y extraer nuevas conclusiones.\n\n\nAprendizaje autom\u00e1tico\n, para adaptarse a nuevas circunstancias y para detectar y extrapolar patrones.\n\n\nVisi\u00f3n computacional\n, para percibir objetos.\n\n\nRob\u00f3tica\n, para manipular y mover objetos. \n \n\n\n\n\n\n\n\nSistemas que piensan como humanos\n: Aqu\u00ed la idea es hacer que las m\u00e1quinas piensen como humanos en el sentido m\u00e1s literal; es decir, que tengan capacidades cognitivas de toma de decisiones, resoluci\u00f3n de problemas, aprendizaje, etc. Dentro de este enfoque podemos encontrar al campo\ninterdisciplinario de la \nciencia cognitiva\n, en el cual convergen modelos computacionales de \nIA\n y\nt\u00e9cnicas experimentales de \npsicolog\u00eda\n intentando elaborar teor\u00edas precisas y verificables sobre el funcionamiento de la mente humana.\n\n\n\n\n\n\nSistemas que piensan racionalmente\n: Aqu\u00ed la idea es descubrir los c\u00e1lculos que hacen posible percibir, razonar y actuar; es decir, encontrar las \nleyes\n que rigen el pensamiento racional. Dentro de este enfoque podemos encontrar a la \nL\u00f3gica\n, que intenta expresar las \nleyes\n que gobiernan la manera de operar de la mente.\n\n\n\n\n\n\nSistemas que se comportan racionalmente\n: Aqu\u00ed la idea es dise\u00f1ar \nagentes\n inteligentes. Dentro de este enfoque un \nagente racional\n es aquel que act\u00faa con la intenci\u00f3n de alcanzar el mejor resultado o, cuando hay incertidumbre, el mejor resultado esperado. Un elemento importante a tener en cuenta es que tarde o temprano uno se dar\u00e1 cuenta de que obtener una racionalidad perfecta (hacer siempre lo correcto) no es del todo posible en entornos complejos. La demanda computacional que esto implica es demasiado grande, por lo que debemos conformarnos con una racionalidad limitada. Como lo que se busca en este enfoque es realizar inferencias correctas, se necesitan las mismas habilidades que para la \nPrueba de Turing\n, es decir, es necesario contar con la \ncapacidad para representar el conocimiento y razonar bas\u00e1ndonos en \u00e9l\n, porque ello permitir\u00e1 alcanzar decisiones correctas en una amplia gama de situaciones. Es necesario \nser capaz de generar sentencias comprensibles en lenguaje natural\n, ya que el enunciado de tales oraciones permite a los agentes desenvolverse en una sociedad compleja. El \naprendizaje\n no se lleva a cabo por erudici\u00f3n exclusivamente, sino que \nprofundizar en el conocimiento\n de c\u00f3mo funciona el mundo facilita la concepci\u00f3n de estrategias mejores para manejarse en \u00e9l.\n\n\n\n\n\n\nFundamentos de la Inteligencia artificial\n\n\nExisten varias disciplinas que han contribuido con ideas, puntos de vista y t\u00e9cnicas al desarrollo del campo de la \nInteligencia Artificial\n. Ellas son:\n\n\nFilosof\u00eda \n\n\nMuchas han sido las contribuciones de la \nFilosof\u00eda\n a las ciencias. En el campo de la \nInteligencia Artificial\n a contribuido con varios aportes entre los que se destacan los conceptos de \nIA d\u00e9bil\n y \nIA fuerte\n. \n\n\nLa \nIA d\u00e9bil\n se define como la inteligencia artificial racional que se centra t\u00edpicamente en una tarea estrecha. La inteligencia de la \nIA d\u00e9bil\n es limitada, no hay autoconciencia o inteligencia genuina. \nSiri\n es un buen ejemplo de una \nIA d\u00e9bil\n que combina varias t\u00e9cnicas de \nIA d\u00e9bil\n para funcionar. \nSiri\n puede hacer un mont\u00f3n de cosas por nosotros, pero a medida que intentamos tener conversaciones con el asistente virtual, nos damos cuenta de cuan limitada es.\n\n\nLa \nIA fuerte\n es aquella \ninteligencia artificial\n que iguala o excede la inteligencia humana promedio. Este tipo de \nAI\n  ser\u00e1 capaz de realizar todas las tareas que un ser humano podr\u00eda hacer. Hay mucha investigaci\u00f3n en este campo, pero todav\u00eda no han habido grandes avances.\n\n\nMuchos son los debates filos\u00f3ficos alrededor de la \ninteligencia artificial\n, para aquellos interesados en los aspectos filos\u00f3ficos les recomiendo inscribirse en nuestro \ngrupo de debate de IAAR\n\n\nMatem\u00e1ticas\n\n\nSi de ciencias aplicadas se trata, no puede faltar el aporte de las \nMatem\u00e1ticas\n. Para entender y desarrollar los principales \nalgoritmos\n que se utilizan en el campo de la \nInteligencia Artificial\n, deber\u00edamos tener nociones de: \n\n\n\u00c1lgebra lineal\n\n\nEl \n\u00e1lgebra lineal\n es una rama de las matem\u00e1ticas que estudia conceptos tales como vectores, matrices, tensores, sistemas de ecuaciones lineales y en su enfoque de manera m\u00e1s formal, espacios vectoriales y sus transformaciones lineales. Una buena comprensi\u00f3n del \n\u00e1lgebra lineal\n es esencial para entender y trabajar con muchos algoritmos de \nMachine Learning\n, y especialmente para los algoritmos de \nDeep Learning\n.\n\n\nC\u00e1lculo\n\n\nEl \nC\u00e1lculo\n es el campo de la matem\u00e1tica que incluye el estudio de los l\u00edmites, derivadas, integrales y series infinitas, y m\u00e1s concretamente se puede decir que es el estudio del \ncambio\n. Particularmente para el campo de la \nInteligencia Artificial\n algunos conceptos que se deber\u00edan conocer incluyen: \nC\u00e1lculo Diferencial e Integral\n, \nDerivadas Parciales\n, Funciones de Valores Vectoriales, y \nGradientes\n.\n\n\nOptimizaci\u00f3n matem\u00e1tica\n\n\nLa \nOptimizaci\u00f3n matem\u00e1tica\n es la herramienta matem\u00e1tica que nos permite optimizar decisiones, es decir, seleccionar la mejor alternativa de un conjunto de criterios disponibles. Su comprensi\u00f3n es fundamental para poder entender la eficiencia computacional y la escalabilidad de los principales algoritmos de \nMachine Learning\n y \nDeep Learning\n, los cuales suelen trabajar con matrices dispersas de gran tama\u00f1o.\n\n\nProbabilidad y estad\u00edstica\n\n\nLa \nProbabilidad y estad\u00edstica\n es la rama de la matem\u00e1tica que trata con la \nincertidumbre\n, la \naleatoriedad\n y la \ninferencia\n. Sus conceptos son fundamentales para cualquier algoritmo de \nMachine Learning\n o \nDeep Learning\n.\n\n\nUna buena introducci\u00f3n a cada uno de estos campos de las matem\u00e1ticas que son fundamentales para la \nInteligencia Artificial\n, la pueden encontrar en mi \nblog\n.\n\n\nLing\u00fc\u00edstica\n\n\nLa \nLing\u00fc\u00edstica\n moderna y la \nInteligencia Artificial\n nacieron al mismo tiempo y maduraron juntas, solap\u00e1ndose en un campo h\u00edbrido llamado ling\u00fc\u00edstica computacional o \nprocesamiento de lenguaje natural\n. El entendimiento del lenguaje requiere la comprensi\u00f3n de la materia bajo estudio y de su contexto, y no solamente el entendimiento de la estructura de las sentencias; lo que lo convierte en un problema bastante complejo de abordar.\n\n\nNeurociencias\n\n\nLa \nNeurociencia\n es el estudio del sistema neurol\u00f3gico, y en especial del cerebro. La forma exacta en la que en un cerebro se genera el pensamiento es uno de los grandes misterios de la ciencia. El hecho de que una colecci\u00f3n de simples c\u00e9lulas puede llegar a generar razonamiento, acci\u00f3n, y conciencia es un enigma a resolver. Cerebros y computadores realizan tareas bastante diferentes y tienen propiedades muy distintas. Seg\u00fan los c\u00e1lculos de los expertos se estima que para el 2020 las computadoras igualaran la capacidad de procesamiento de los cerebros. Muchos modelos de \nIA\n fueron inspirados en la estructura y el funcionamiento de nuestro cerebro.\n\n\nPsicolog\u00eda\n\n\nLa \nPsicolog\u00eda\n trata sobre el estudio y an\u00e1lisis de la conducta y los procesos mentales de los individuos y grupos humanos. La rama que m\u00e1s influencia ha tenido para la \nInteligencia Artificial\n es la de la \npsicolog\u00eda cognitiva\n que se encarga del estudio de la cognici\u00f3n; es decir, de los procesos mentales implicados en el conocimiento. Tiene como objeto de estudio los mecanismos b\u00e1sicos y profundos por los que se elabora el conocimiento, desde la percepci\u00f3n, la memoria y el aprendizaje, hasta la formaci\u00f3n de conceptos y razonamiento l\u00f3gico. Las teor\u00eda descritas por esta rama han sido utilizados para desarrollar varios modelos de \nInteligencia Artificial\n y \nMachine Learning\n\n\nRamas de la Inteligencia artificial\n\n\nDentro de la \nInteligencia Artificial\n podemos encontrar distintas ramas, entre las que se destacan:\n\n\nMachine Learning\n\n\nEl \nMachine Learning\n es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del \nMachine Learning\n es \ngeneralizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada\n. El ejemplo m\u00e1s t\u00edpico donde podemos ver el uso del Machine Learning es en el filtrado de los correo basura o spam. Mediante la observaci\u00f3n de miles de correos electr\u00f3nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos. \n\n\nEl \nMachine Learning\n tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito, an\u00e1lisis del mercado de valores, clasificaci\u00f3n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob\u00f3tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distinguir los distintos \ntipos de problemas de \nMachine Learning\n con los que nos podemos encontrar.\n\n\nAprendizaje supervisado\n\n\nEn los problemas de \naprendizaje supervisado\n se ense\u00f1a o entrena al algoritmo a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos, el algoritmo podr\u00e1 generalizar en una forma m\u00e1s precisa. Una vez concluido el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el algoritmo de aprendizaje utiliza la experiencia pasada que adquiri\u00f3 durante la etapa de entrenamiento para predecir un resultado.\n\n\nAprendizaje no supervisado\n\n\nEn los problemas de \naprendizaje no supervisado\n, el algoritmo es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al algoritmo lo que representan los datos. La idea es que el algoritmo pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos. \n\n\nAprendizaje por refuerzo\n\n\nEn los problemas de \naprendizaje por refuerzo\n, el algoritmo aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m\u00e1s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar\u00e1 creando una estrategia ganadora.\n\n\nDeep Learning\n\n\nEl \nDeep Learning\n constituye un conjunto particular de algoritmos de \nMachine Learning\n que utilizan estructuras profundas de \nredes neuronales\n para encontrar patrones en los datos. Estos tipos de algoritmos cuentan actualmente con un gran inter\u00e9s, ya que han demostrado ser sumamente exitosos para resolver determinados tipos de problemas; como por ejemplo, el reconocimiento de im\u00e1genes. Muchos consideran que este tipo de modelos son los que en el futuro nos llevaran a resolver definitivamente el problema de la \nInteligencia Artificial\n.\n\n\nRazonamiento probabil\u00edstico\n\n\nEl \nrazonamiento probabil\u00edstico\n se encarga de lidiar con la incertidumbre inherente de todo proceso de aprendizaje. El problema para crear una \nInteligencia Artificial\n entonces se convierte en encontrar la forma de trabajar con informaci\u00f3n ruidosa, incompleta e incluso muchas veces contradictoria. Estos algoritmos est\u00e1n sumamente ligados a la \nestad\u00edstica bayesiana\n; y la principal herramienta en la que se apoyan es en el \nteorema de Bayes\n.\n\n\nAlgoritmos gen\u00e9ticos\n\n\nLos \nalgoritmos gen\u00e9ticos\n se basan en la idea de que la madre de todo aprendizaje es la \nselecci\u00f3n natural\n. Si la Naturaleza pudo crearnos, puede crear cualquier cosa; por tal motivo lo \u00fanico que deber\u00edamos hacer para alcanzar una \nInteligencia Artificial\n es simular sus mecanismos en una computadora. La idea de estos algoritmos es imitar a la Evoluci\u00f3n; funcionan seleccionando individuos de una poblaci\u00f3n de soluciones candidatas, y luego intentando producir nuevas generaciones de soluciones mejores que las anteriores una y otra vez hasta aproximarse a una soluci\u00f3n perfecta.\n\n\nAplicaciones de la Inteligencia artificial\n\n\nLas t\u00e9cnicas de la \nInteligencia Artificial\n pueden ser aplicadas en una gran variedad de industrias y situaciones, como ser:\n\n\nMedicina\n\n\nApoy\u00e1ndose en las herramientas que proporciona la \nInteligencia Artificial\n, los doctores podr\u00edan realizar diagn\u00f3sticos m\u00e1s certeros y oportunos, lo que llevar\u00eda a mejores tratamientos y m\u00e1s vidas salvadas. \n\n\nAutos aut\u00f3nomos\n\n\nUtilizando \nInteligencia Artificial\n podr\u00edamos crear autos aut\u00f3nomos que aprendan de los datos y experiencias de millones de otros autos, mejorando el tr\u00e1fico y haciendo mucho m\u00e1s segura la conducci\u00f3n.\n\n\nBancos\n\n\nUtilizando t\u00e9cnicas de \nMachine Learning\n los bancos pueden detectar fraudes antes de que ocurran por medio de analizar los patrones de comportamiento de gastos e identificando r\u00e1pidamente actividades sospechosas. \n\n\nAgricultura\n\n\nEn Agricultura se podr\u00eda optimizar el rendimiento de los cultivos por medio de la utilizaci\u00f3n de las t\u00e9cnicas de \nInteligencia Artificial\n para analizar los datos del suelo y del clima en tiempo real, logrando producir m\u00e1s alimentos incluso con climas perjudiciales. \n\n\nEducaci\u00f3n\n\n\nEn la Educaci\u00f3n se podr\u00edan utilizar las t\u00e9cnicas de la \nInteligencia Artificial\n para dise\u00f1ar programas de estudios personalizados basados en datos que mejoren el rendimiento y el ritmo de aprendizaje de los alumnos.\n\n\nLa \u00e9tica y los riesgos de desarrollar una Inteligencia Artificial\n\n\nActualmente tambi\u00e9n ha surgido un debate \u00e9tico alrededor de la \nInteligencia Artificial\n. Algunos de los pensadores m\u00e1s importantes del planeta han establecido su preocupaci\u00f3n sobre el progreso de la \nIA\n. Entre los problemas que puede traer aparejado el desarrollo de la \nInteligencia Artificial\n, podemos encontrar los siguientes:\n\n\n\n\nLas personas podr\u00edan perder sus trabajos por la automatizaci\u00f3n.\n\n\nLas personas podr\u00edan tener demasiado (o muy poco) tiempo de ocio.\n\n\nLas personas podr\u00edan perder el sentido de ser \u00fanicos.\n\n\nLas personas podr\u00edan perder algunos de sus derechos privados.\n\n\nLa utilizaci\u00f3n de los sistemas de \nIA\n podr\u00eda llevar a la p\u00e9rdida de responsabilidad.\n\n\nEl \u00e9xito de la \nIA\n podr\u00eda significar el fin de la raza humana.\n\n\n\n\nEl debate sobre los beneficios y riesgos del desarrollo de la \nInteligencia Artificial\n est\u00e1 todav\u00eda abierto.\n\n\n\u00bfC\u00f3mo iniciarse en el campo de la Inteligencia artificial?\n\n\nSi luego de leer esta introducci\u00f3n, te has quedado fascinado por el campo de la \nInteligencia Artificial\n y quieres incursionar en el mismo, aqu\u00ed te dejo algunas recomendaciones para iniciarse.\n\n\nIAAR\n\n\nIAAR\n es la comunidad argentina de inteligencia artificial. Agrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo \u00e9tico y humanitario de las tecnolog\u00edas cognitivas. Para comenzar a formar parte de la comunidad pueden inscribirse en los grupos de facebook: \nIAAR\n, \nDebates\n, \nProyectos\n, \nCapacitaci\u00f3n\n; y/o en el \nmeetup\n.\n\n\nProgramaci\u00f3n\n\n\nPara poder trabajar en problemas relacionados al campo de la \nInteligencia Artificial\n es necesario saber programar. Los principales lenguajes que se utilizan son \nPython\n y \nR\n. En los repositorios de \nAcademia de IAAR\n van a poder encontrar material sobre estos lenguajes.\n\n\nFrameworks\n\n\nExisten varios frameworks open source que nos facilitan el trabajar con modelos de \nDeep Learning\n, entre los que se destacan:\n\n\n\n\nTensorFlow\n: \nTensorFlow\n es un frameworks desarrollado por Google. Es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos que utiliza el lenguaje \nPython\n. \n\n\nPyTorch\n: \nPyTorch\n es un framework de \nDeep Learning\n que utiliza el lenguaje \nPython\n y cuenta con el apoyo de Facebook.\n\n\nCaffe\n: \nCaffe\n es un framework de \nDeep Learning\n hecho con expresi\u00f3n, velocidad y modularidad en mente, el cual es desarrollado por la universidad de Berkeley.\n\n\nCNTK\n: \nCNTK\n es un conjunto de herramientas, desarrolladas por Microsoft, f\u00e1ciles de usar, de c\u00f3digo abierto que entrena algoritmos de \nDeep Learning\n para aprender como el cerebro humano.\n\n\nTheano\n: \nTheano\n es una librer\u00eda de \nPython\n que permite definir, optimizar y evaluar expresiones matem\u00e1ticas que involucran tensores de manera eficiente. \n\n\nDeepLearning4j\n: \nDeepLearning4j\n Es una librer\u00eda open source para trabajar con modelos de \nDeep Learning\n distribuidos utilizando el lenguaje \nJava\n.\n\n\n\n\nBots\n\n\nUna de las ramas con mayor crecimiento y que m\u00e1s se ha beneficiado con el boom de la \nInteligencia Artificial\n es la de los \nBots\n. Generar peque\u00f1os \nBots\n que puedan tener conversaciones b\u00e1sicas con los usuarios es bastante simple. Pueden encontrar una gu\u00eda con una gran n\u00famero de herramientas en el \nblog de IAAR\n.",
            "title": "Inteligencia Artificial"
        },
        {
            "location": "/inteligencia-artificial/#introduccion-a-la-inteligencia-artificial",
            "text": "El cerebro es el \u00f3rgano m\u00e1s incre\u00edble del cuerpo humano. Establece la forma en que percibimos las im\u00e1genes, el sonido, los olores, los sabores y el tacto. Nos permite almacenar recuerdos, experimentar emociones e incluso so\u00f1ar. Sin el, ser\u00edamos organismos primitivos, incapaces de otra cosa que el m\u00e1s simple de los reflejos. El cerebro es, en definitiva, lo que nos hace inteligentes.\nDurante d\u00e9cadas hemos so\u00f1ado con construir m\u00e1quinas inteligentes con cerebros como los nuestros; asistentes robotizados para limpiar nuestras casas, coches que se conducen por s\u00ed mismos, microscopios que detecten enfermedades autom\u00e1ticamente. Pero construir estas m\u00e1quinas  artificialmente inteligentes  nos obliga a resolver aldgunos de los problemas computacionales m\u00e1s complejos que hemos tenido; problemas que nuestros cerebros ya pueden resolver en una fracci\u00f3n de segundos. La forma de atacar y resolver estos problemas, es el campo de estudio de la  Inteligencia Artificial .",
            "title": "Introducci\u00f3n a la Inteligencia Artificial"
        },
        {
            "location": "/inteligencia-artificial/#que-es-la-inteligencia-artificial",
            "text": "Definir el concepto de  Inteligencia Artificial  no es nada f\u00e1cil. Una definici\u00f3n sumamente general ser\u00eda que la  IA  es el estudio de la  infrom\u00e1tica  centr\u00e1ndose en el desarrollo de software o  m\u00e1quinas que exhiben una inteligencia humana .",
            "title": "\u00bfQu\u00e9 es la Inteligencia Artificial?"
        },
        {
            "location": "/inteligencia-artificial/#objetivos-de-la-inteligencia-artificial",
            "text": "Los objetivos principales de la  IA  incluyen la deducci\u00f3n y el razonamiento, la representaci\u00f3n del conocimiento, la planificaci\u00f3n, el procesamiento del lenguaje natural ( NLP ), el aprendizaje, la percepci\u00f3n y la capacidad de manipular y mover objetos. Los objetivos a largo plazo incluyen el logro de la Creatividad, la Inteligencia Social y la Inteligencia General (a nivel Humano).",
            "title": "Objetivos de la Inteligencia Artificial"
        },
        {
            "location": "/inteligencia-artificial/#cuatro-enfoques-distintos",
            "text": "Podemos distinguir cuatro enfoques distintos de abordar el problema de la  Inteligencia Artificial .     Sistemas que se comportan como humanos : Aqu\u00ed la idea es desarrollar m\u00e1quinas capaces de realizar funciones para las cuales se requerir\u00eda un humano inteligente. Dentro de este enfoque podemos encontrar la famosa  Prueba de Turing . Para poder superar esta prueba, la m\u00e1quina deber\u00eda poseer las siguientes capacidades:  Procesamiento de lenguaje natural , que le permita comunicarse satisfactoriamente.  Representaci\u00f3n del conocimiento , para almacenar lo que se conoce o se siente.  Razonamiento autom\u00e1tico , para utilizar la informaci\u00f3n almacenada para responder a preguntas y extraer nuevas conclusiones.  Aprendizaje autom\u00e1tico , para adaptarse a nuevas circunstancias y para detectar y extrapolar patrones.  Visi\u00f3n computacional , para percibir objetos.  Rob\u00f3tica , para manipular y mover objetos.       Sistemas que piensan como humanos : Aqu\u00ed la idea es hacer que las m\u00e1quinas piensen como humanos en el sentido m\u00e1s literal; es decir, que tengan capacidades cognitivas de toma de decisiones, resoluci\u00f3n de problemas, aprendizaje, etc. Dentro de este enfoque podemos encontrar al campo\ninterdisciplinario de la  ciencia cognitiva , en el cual convergen modelos computacionales de  IA  y\nt\u00e9cnicas experimentales de  psicolog\u00eda  intentando elaborar teor\u00edas precisas y verificables sobre el funcionamiento de la mente humana.    Sistemas que piensan racionalmente : Aqu\u00ed la idea es descubrir los c\u00e1lculos que hacen posible percibir, razonar y actuar; es decir, encontrar las  leyes  que rigen el pensamiento racional. Dentro de este enfoque podemos encontrar a la  L\u00f3gica , que intenta expresar las  leyes  que gobiernan la manera de operar de la mente.    Sistemas que se comportan racionalmente : Aqu\u00ed la idea es dise\u00f1ar  agentes  inteligentes. Dentro de este enfoque un  agente racional  es aquel que act\u00faa con la intenci\u00f3n de alcanzar el mejor resultado o, cuando hay incertidumbre, el mejor resultado esperado. Un elemento importante a tener en cuenta es que tarde o temprano uno se dar\u00e1 cuenta de que obtener una racionalidad perfecta (hacer siempre lo correcto) no es del todo posible en entornos complejos. La demanda computacional que esto implica es demasiado grande, por lo que debemos conformarnos con una racionalidad limitada. Como lo que se busca en este enfoque es realizar inferencias correctas, se necesitan las mismas habilidades que para la  Prueba de Turing , es decir, es necesario contar con la  capacidad para representar el conocimiento y razonar bas\u00e1ndonos en \u00e9l , porque ello permitir\u00e1 alcanzar decisiones correctas en una amplia gama de situaciones. Es necesario  ser capaz de generar sentencias comprensibles en lenguaje natural , ya que el enunciado de tales oraciones permite a los agentes desenvolverse en una sociedad compleja. El  aprendizaje  no se lleva a cabo por erudici\u00f3n exclusivamente, sino que  profundizar en el conocimiento  de c\u00f3mo funciona el mundo facilita la concepci\u00f3n de estrategias mejores para manejarse en \u00e9l.",
            "title": "Cuatro enfoques distintos"
        },
        {
            "location": "/inteligencia-artificial/#fundamentos-de-la-inteligencia-artificial",
            "text": "Existen varias disciplinas que han contribuido con ideas, puntos de vista y t\u00e9cnicas al desarrollo del campo de la  Inteligencia Artificial . Ellas son:",
            "title": "Fundamentos de la Inteligencia artificial"
        },
        {
            "location": "/inteligencia-artificial/#filosofia",
            "text": "Muchas han sido las contribuciones de la  Filosof\u00eda  a las ciencias. En el campo de la  Inteligencia Artificial  a contribuido con varios aportes entre los que se destacan los conceptos de  IA d\u00e9bil  y  IA fuerte .   La  IA d\u00e9bil  se define como la inteligencia artificial racional que se centra t\u00edpicamente en una tarea estrecha. La inteligencia de la  IA d\u00e9bil  es limitada, no hay autoconciencia o inteligencia genuina.  Siri  es un buen ejemplo de una  IA d\u00e9bil  que combina varias t\u00e9cnicas de  IA d\u00e9bil  para funcionar.  Siri  puede hacer un mont\u00f3n de cosas por nosotros, pero a medida que intentamos tener conversaciones con el asistente virtual, nos damos cuenta de cuan limitada es.  La  IA fuerte  es aquella  inteligencia artificial  que iguala o excede la inteligencia humana promedio. Este tipo de  AI   ser\u00e1 capaz de realizar todas las tareas que un ser humano podr\u00eda hacer. Hay mucha investigaci\u00f3n en este campo, pero todav\u00eda no han habido grandes avances.  Muchos son los debates filos\u00f3ficos alrededor de la  inteligencia artificial , para aquellos interesados en los aspectos filos\u00f3ficos les recomiendo inscribirse en nuestro  grupo de debate de IAAR",
            "title": "Filosof\u00eda "
        },
        {
            "location": "/inteligencia-artificial/#matematicas",
            "text": "Si de ciencias aplicadas se trata, no puede faltar el aporte de las  Matem\u00e1ticas . Para entender y desarrollar los principales  algoritmos  que se utilizan en el campo de la  Inteligencia Artificial , deber\u00edamos tener nociones de:",
            "title": "Matem\u00e1ticas"
        },
        {
            "location": "/inteligencia-artificial/#algebra-lineal",
            "text": "El  \u00e1lgebra lineal  es una rama de las matem\u00e1ticas que estudia conceptos tales como vectores, matrices, tensores, sistemas de ecuaciones lineales y en su enfoque de manera m\u00e1s formal, espacios vectoriales y sus transformaciones lineales. Una buena comprensi\u00f3n del  \u00e1lgebra lineal  es esencial para entender y trabajar con muchos algoritmos de  Machine Learning , y especialmente para los algoritmos de  Deep Learning .",
            "title": "\u00c1lgebra lineal"
        },
        {
            "location": "/inteligencia-artificial/#calculo",
            "text": "El  C\u00e1lculo  es el campo de la matem\u00e1tica que incluye el estudio de los l\u00edmites, derivadas, integrales y series infinitas, y m\u00e1s concretamente se puede decir que es el estudio del  cambio . Particularmente para el campo de la  Inteligencia Artificial  algunos conceptos que se deber\u00edan conocer incluyen:  C\u00e1lculo Diferencial e Integral ,  Derivadas Parciales , Funciones de Valores Vectoriales, y  Gradientes .",
            "title": "C\u00e1lculo"
        },
        {
            "location": "/inteligencia-artificial/#optimizacion-matematica",
            "text": "La  Optimizaci\u00f3n matem\u00e1tica  es la herramienta matem\u00e1tica que nos permite optimizar decisiones, es decir, seleccionar la mejor alternativa de un conjunto de criterios disponibles. Su comprensi\u00f3n es fundamental para poder entender la eficiencia computacional y la escalabilidad de los principales algoritmos de  Machine Learning  y  Deep Learning , los cuales suelen trabajar con matrices dispersas de gran tama\u00f1o.",
            "title": "Optimizaci\u00f3n matem\u00e1tica"
        },
        {
            "location": "/inteligencia-artificial/#probabilidad-y-estadistica",
            "text": "La  Probabilidad y estad\u00edstica  es la rama de la matem\u00e1tica que trata con la  incertidumbre , la  aleatoriedad  y la  inferencia . Sus conceptos son fundamentales para cualquier algoritmo de  Machine Learning  o  Deep Learning .  Una buena introducci\u00f3n a cada uno de estos campos de las matem\u00e1ticas que son fundamentales para la  Inteligencia Artificial , la pueden encontrar en mi  blog .",
            "title": "Probabilidad y estad\u00edstica"
        },
        {
            "location": "/inteligencia-artificial/#linguistica",
            "text": "La  Ling\u00fc\u00edstica  moderna y la  Inteligencia Artificial  nacieron al mismo tiempo y maduraron juntas, solap\u00e1ndose en un campo h\u00edbrido llamado ling\u00fc\u00edstica computacional o  procesamiento de lenguaje natural . El entendimiento del lenguaje requiere la comprensi\u00f3n de la materia bajo estudio y de su contexto, y no solamente el entendimiento de la estructura de las sentencias; lo que lo convierte en un problema bastante complejo de abordar.",
            "title": "Ling\u00fc\u00edstica"
        },
        {
            "location": "/inteligencia-artificial/#neurociencias",
            "text": "La  Neurociencia  es el estudio del sistema neurol\u00f3gico, y en especial del cerebro. La forma exacta en la que en un cerebro se genera el pensamiento es uno de los grandes misterios de la ciencia. El hecho de que una colecci\u00f3n de simples c\u00e9lulas puede llegar a generar razonamiento, acci\u00f3n, y conciencia es un enigma a resolver. Cerebros y computadores realizan tareas bastante diferentes y tienen propiedades muy distintas. Seg\u00fan los c\u00e1lculos de los expertos se estima que para el 2020 las computadoras igualaran la capacidad de procesamiento de los cerebros. Muchos modelos de  IA  fueron inspirados en la estructura y el funcionamiento de nuestro cerebro.",
            "title": "Neurociencias"
        },
        {
            "location": "/inteligencia-artificial/#psicologia",
            "text": "La  Psicolog\u00eda  trata sobre el estudio y an\u00e1lisis de la conducta y los procesos mentales de los individuos y grupos humanos. La rama que m\u00e1s influencia ha tenido para la  Inteligencia Artificial  es la de la  psicolog\u00eda cognitiva  que se encarga del estudio de la cognici\u00f3n; es decir, de los procesos mentales implicados en el conocimiento. Tiene como objeto de estudio los mecanismos b\u00e1sicos y profundos por los que se elabora el conocimiento, desde la percepci\u00f3n, la memoria y el aprendizaje, hasta la formaci\u00f3n de conceptos y razonamiento l\u00f3gico. Las teor\u00eda descritas por esta rama han sido utilizados para desarrollar varios modelos de  Inteligencia Artificial  y  Machine Learning",
            "title": "Psicolog\u00eda"
        },
        {
            "location": "/inteligencia-artificial/#ramas-de-la-inteligencia-artificial",
            "text": "Dentro de la  Inteligencia Artificial  podemos encontrar distintas ramas, entre las que se destacan:",
            "title": "Ramas de la Inteligencia artificial"
        },
        {
            "location": "/inteligencia-artificial/#machine-learning",
            "text": "El  Machine Learning  es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del  Machine Learning  es  generalizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada . El ejemplo m\u00e1s t\u00edpico donde podemos ver el uso del Machine Learning es en el filtrado de los correo basura o spam. Mediante la observaci\u00f3n de miles de correos electr\u00f3nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos.   El  Machine Learning  tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito, an\u00e1lisis del mercado de valores, clasificaci\u00f3n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob\u00f3tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distinguir los distintos  tipos de problemas de  Machine Learning  con los que nos podemos encontrar.",
            "title": "Machine Learning"
        },
        {
            "location": "/inteligencia-artificial/#aprendizaje-supervisado",
            "text": "En los problemas de  aprendizaje supervisado  se ense\u00f1a o entrena al algoritmo a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos, el algoritmo podr\u00e1 generalizar en una forma m\u00e1s precisa. Una vez concluido el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el algoritmo de aprendizaje utiliza la experiencia pasada que adquiri\u00f3 durante la etapa de entrenamiento para predecir un resultado.",
            "title": "Aprendizaje supervisado"
        },
        {
            "location": "/inteligencia-artificial/#aprendizaje-no-supervisado",
            "text": "En los problemas de  aprendizaje no supervisado , el algoritmo es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al algoritmo lo que representan los datos. La idea es que el algoritmo pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos.",
            "title": "Aprendizaje no supervisado"
        },
        {
            "location": "/inteligencia-artificial/#aprendizaje-por-refuerzo",
            "text": "En los problemas de  aprendizaje por refuerzo , el algoritmo aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m\u00e1s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar\u00e1 creando una estrategia ganadora.",
            "title": "Aprendizaje por refuerzo"
        },
        {
            "location": "/inteligencia-artificial/#deep-learning",
            "text": "El  Deep Learning  constituye un conjunto particular de algoritmos de  Machine Learning  que utilizan estructuras profundas de  redes neuronales  para encontrar patrones en los datos. Estos tipos de algoritmos cuentan actualmente con un gran inter\u00e9s, ya que han demostrado ser sumamente exitosos para resolver determinados tipos de problemas; como por ejemplo, el reconocimiento de im\u00e1genes. Muchos consideran que este tipo de modelos son los que en el futuro nos llevaran a resolver definitivamente el problema de la  Inteligencia Artificial .",
            "title": "Deep Learning"
        },
        {
            "location": "/inteligencia-artificial/#razonamiento-probabilistico",
            "text": "El  razonamiento probabil\u00edstico  se encarga de lidiar con la incertidumbre inherente de todo proceso de aprendizaje. El problema para crear una  Inteligencia Artificial  entonces se convierte en encontrar la forma de trabajar con informaci\u00f3n ruidosa, incompleta e incluso muchas veces contradictoria. Estos algoritmos est\u00e1n sumamente ligados a la  estad\u00edstica bayesiana ; y la principal herramienta en la que se apoyan es en el  teorema de Bayes .",
            "title": "Razonamiento probabil\u00edstico"
        },
        {
            "location": "/inteligencia-artificial/#algoritmos-geneticos",
            "text": "Los  algoritmos gen\u00e9ticos  se basan en la idea de que la madre de todo aprendizaje es la  selecci\u00f3n natural . Si la Naturaleza pudo crearnos, puede crear cualquier cosa; por tal motivo lo \u00fanico que deber\u00edamos hacer para alcanzar una  Inteligencia Artificial  es simular sus mecanismos en una computadora. La idea de estos algoritmos es imitar a la Evoluci\u00f3n; funcionan seleccionando individuos de una poblaci\u00f3n de soluciones candidatas, y luego intentando producir nuevas generaciones de soluciones mejores que las anteriores una y otra vez hasta aproximarse a una soluci\u00f3n perfecta.",
            "title": "Algoritmos gen\u00e9ticos"
        },
        {
            "location": "/inteligencia-artificial/#aplicaciones-de-la-inteligencia-artificial",
            "text": "Las t\u00e9cnicas de la  Inteligencia Artificial  pueden ser aplicadas en una gran variedad de industrias y situaciones, como ser:",
            "title": "Aplicaciones de la Inteligencia artificial"
        },
        {
            "location": "/inteligencia-artificial/#medicina",
            "text": "Apoy\u00e1ndose en las herramientas que proporciona la  Inteligencia Artificial , los doctores podr\u00edan realizar diagn\u00f3sticos m\u00e1s certeros y oportunos, lo que llevar\u00eda a mejores tratamientos y m\u00e1s vidas salvadas.",
            "title": "Medicina"
        },
        {
            "location": "/inteligencia-artificial/#autos-autonomos",
            "text": "Utilizando  Inteligencia Artificial  podr\u00edamos crear autos aut\u00f3nomos que aprendan de los datos y experiencias de millones de otros autos, mejorando el tr\u00e1fico y haciendo mucho m\u00e1s segura la conducci\u00f3n.",
            "title": "Autos aut\u00f3nomos"
        },
        {
            "location": "/inteligencia-artificial/#bancos",
            "text": "Utilizando t\u00e9cnicas de  Machine Learning  los bancos pueden detectar fraudes antes de que ocurran por medio de analizar los patrones de comportamiento de gastos e identificando r\u00e1pidamente actividades sospechosas.",
            "title": "Bancos"
        },
        {
            "location": "/inteligencia-artificial/#agricultura",
            "text": "En Agricultura se podr\u00eda optimizar el rendimiento de los cultivos por medio de la utilizaci\u00f3n de las t\u00e9cnicas de  Inteligencia Artificial  para analizar los datos del suelo y del clima en tiempo real, logrando producir m\u00e1s alimentos incluso con climas perjudiciales.",
            "title": "Agricultura"
        },
        {
            "location": "/inteligencia-artificial/#educacion",
            "text": "En la Educaci\u00f3n se podr\u00edan utilizar las t\u00e9cnicas de la  Inteligencia Artificial  para dise\u00f1ar programas de estudios personalizados basados en datos que mejoren el rendimiento y el ritmo de aprendizaje de los alumnos.",
            "title": "Educaci\u00f3n"
        },
        {
            "location": "/inteligencia-artificial/#la-etica-y-los-riesgos-de-desarrollar-una-inteligencia-artificial",
            "text": "Actualmente tambi\u00e9n ha surgido un debate \u00e9tico alrededor de la  Inteligencia Artificial . Algunos de los pensadores m\u00e1s importantes del planeta han establecido su preocupaci\u00f3n sobre el progreso de la  IA . Entre los problemas que puede traer aparejado el desarrollo de la  Inteligencia Artificial , podemos encontrar los siguientes:   Las personas podr\u00edan perder sus trabajos por la automatizaci\u00f3n.  Las personas podr\u00edan tener demasiado (o muy poco) tiempo de ocio.  Las personas podr\u00edan perder el sentido de ser \u00fanicos.  Las personas podr\u00edan perder algunos de sus derechos privados.  La utilizaci\u00f3n de los sistemas de  IA  podr\u00eda llevar a la p\u00e9rdida de responsabilidad.  El \u00e9xito de la  IA  podr\u00eda significar el fin de la raza humana.   El debate sobre los beneficios y riesgos del desarrollo de la  Inteligencia Artificial  est\u00e1 todav\u00eda abierto.",
            "title": "La \u00e9tica y los riesgos de desarrollar una Inteligencia Artificial"
        },
        {
            "location": "/inteligencia-artificial/#como-iniciarse-en-el-campo-de-la-inteligencia-artificial",
            "text": "Si luego de leer esta introducci\u00f3n, te has quedado fascinado por el campo de la  Inteligencia Artificial  y quieres incursionar en el mismo, aqu\u00ed te dejo algunas recomendaciones para iniciarse.",
            "title": "\u00bfC\u00f3mo iniciarse en el campo de la Inteligencia artificial?"
        },
        {
            "location": "/inteligencia-artificial/#iaar",
            "text": "IAAR  es la comunidad argentina de inteligencia artificial. Agrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo \u00e9tico y humanitario de las tecnolog\u00edas cognitivas. Para comenzar a formar parte de la comunidad pueden inscribirse en los grupos de facebook:  IAAR ,  Debates ,  Proyectos ,  Capacitaci\u00f3n ; y/o en el  meetup .",
            "title": "IAAR"
        },
        {
            "location": "/inteligencia-artificial/#programacion",
            "text": "Para poder trabajar en problemas relacionados al campo de la  Inteligencia Artificial  es necesario saber programar. Los principales lenguajes que se utilizan son  Python  y  R . En los repositorios de  Academia de IAAR  van a poder encontrar material sobre estos lenguajes.",
            "title": "Programaci\u00f3n"
        },
        {
            "location": "/inteligencia-artificial/#frameworks",
            "text": "Existen varios frameworks open source que nos facilitan el trabajar con modelos de  Deep Learning , entre los que se destacan:   TensorFlow :  TensorFlow  es un frameworks desarrollado por Google. Es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos que utiliza el lenguaje  Python .   PyTorch :  PyTorch  es un framework de  Deep Learning  que utiliza el lenguaje  Python  y cuenta con el apoyo de Facebook.  Caffe :  Caffe  es un framework de  Deep Learning  hecho con expresi\u00f3n, velocidad y modularidad en mente, el cual es desarrollado por la universidad de Berkeley.  CNTK :  CNTK  es un conjunto de herramientas, desarrolladas por Microsoft, f\u00e1ciles de usar, de c\u00f3digo abierto que entrena algoritmos de  Deep Learning  para aprender como el cerebro humano.  Theano :  Theano  es una librer\u00eda de  Python  que permite definir, optimizar y evaluar expresiones matem\u00e1ticas que involucran tensores de manera eficiente.   DeepLearning4j :  DeepLearning4j  Es una librer\u00eda open source para trabajar con modelos de  Deep Learning  distribuidos utilizando el lenguaje  Java .",
            "title": "Frameworks"
        },
        {
            "location": "/inteligencia-artificial/#bots",
            "text": "Una de las ramas con mayor crecimiento y que m\u00e1s se ha beneficiado con el boom de la  Inteligencia Artificial  es la de los  Bots . Generar peque\u00f1os  Bots  que puedan tener conversaciones b\u00e1sicas con los usuarios es bastante simple. Pueden encontrar una gu\u00eda con una gran n\u00famero de herramientas en el  blog de IAAR .",
            "title": "Bots"
        },
        {
            "location": "/deeplearning/",
            "text": "Introducci\u00f3n al Deep Learning\n\n\n\n\nEl \nDeep Learning\n es sin duda el \u00e1rea de investigaci\u00f3n m\u00e1s popular dentro del campo de la \nInteligencia Artificial\n. La mayor\u00eda de las nuevas investigaciones que se realizan, trabajan con modelos basados en las t\u00e9cnicas de \nDeep Learning\n; ya que las mismas han logrado resultados sorprendes en campos como \nprocesamiento del lenguaje natural\n y \nVisi\u00f3n por computadora\n. Pero... \u00bfqu\u00e9 es este misterioso concepto que ha ganado tanta popularidad? y... \u00bfc\u00f3mo se relaciona con el campo de la \nInteligencia Artificial\n y el \nMachine Learning\n?. \n\n\nInteligencia artificial, Machine learning y Deep learning \n\n\nEn general se suelen utilizar los t\u00e9rminos de \nInteligencia Artificial\n, \nMachine Learning\n y \nDeep Learning\n en forma intercambiada. Sin embargo, \u00e9stos t\u00e9rminos no son los mismo y abarcan distintas cosas. \n\n\nInteligencia Artificial\n\n\nEl t\u00e9rmino \nInteligencia Artificial\n es el m\u00e1s general y engloba a los campos de \nMachine Learning\n y \nDeep Learning\n junto con otras t\u00e9cnicas como los \nalgoritmos de b\u00fasqueda\n, el \nrazonamiento simb\u00f3lico\n, el \nrazonamiento l\u00f3gico\n y la \nestad\u00edstica\n. Naci\u00f3 en los a\u00f1os 1950s, cuando un grupo de pioneros de la computaci\u00f3n comenzaron a preguntarse si se pod\u00eda hacer que las computadoras \npensaran\n. Una definici\u00f3n concisa de la \nInteligencia Artificial\n ser\u00eda: \nel esfuerzo para automatizar las tareas intelectuales que normalmente realizan los seres humanos\n. \n\n\nMachine Learning\n\n\nEl \nMachine Learning\n o \nAprendizaje autom\u00e1tico\n se refiere a un amplio conjunto de t\u00e9cnicas inform\u00e1ticas que nos permiten dar a las computadoras \nla capacidad de aprender sin ser expl\u00edcitamente programadas\n. Hay muchos tipos diferentes de algoritmos de \nAprendizaje autom\u00e1tico\n, entre los que se encuentran el \naprendizaje por refuerzo\n, los \nalgoritmos gen\u00e9ticos\n, el aprendizaje basado en \nreglas de asociaci\u00f3n\n, los \nalgoritmos de agrupamiento\n, los \n\u00e1rboles de decisi\u00f3n\n, las \nm\u00e1quinas de vectores de soporte\n y las \nredes neuronales\n. Actualmente, los algoritmos m\u00e1s populares dentro de este campo son los de \nDeep Learning\n.\n\n\nDeep Learning\n\n\nEl \nDeep Learning\n o \naprendizaje profundo\n es un subcampo dentro del \nMachine Learning\n, el cu\u00e1l utiliza distintas estructuras de \nredes neuronales\n para lograr el aprendizaje de sucesivas \ncapas de representaciones\n cada vez m\u00e1s significativas de los datos. El \nprofundo\n o \ndeep\n en \nDeep Learning\n hace referencia a la cantidad de \ncapas de representaciones\n que se utilizan en el modelo; en general se suelen utilizar decenas o incluso cientos de \ncapas de representaci\u00f3n\n. las cuales \naprenden\n autom\u00e1ticamente a medida que el modelo es entrenado con los datos.\n\n\n\n\n\u00bfQu\u00e9 es el Deep Learning? \n\n\nAntes de poder entender que es el \nDeep Learning\n, debemos en primer lugar conocer dos conceptos fundamentales: las \nredes neuronales artificiales\n y la \nPropagaci\u00f3n hacia atr\u00e1s\n.\n\n\nRedes Neuronales\n\n\nLas \nredes neuronales\n son un modelo computacional basado en un gran conjunto de unidades neuronales simples (\nneuronas artificiales\n), de forma aproximadamente an\u00e1loga al comportamiento observado en los axones de las neuronas en los cerebros biol\u00f3gicos. \n\n\nCada una de estas neuronas simples, va a tener una forma similar al siguiente diagrama:\n\n\n\n\nEn donde sus componentes son:\n\n\n\n\n\n\n$x_1, x_2, \\dots, x_n$: Los datos de entrada en la neurona, los cuales tambi\u00e9n puede ser que sean producto de la salida de otra neurona de la red.\n\n\n\n\n\n\n$x_0$: La unidad de sesgo; un valor constante que se le suma a la entrada de la funci\u00f3n de activaci\u00f3n de la neurona. Generalmente tiene el valor 1. Este valor va a permitir cambiar la funci\u00f3n de activaci\u00f3n hacia la derecha o izquierda, otorg\u00e1ndole m\u00e1s flexibilidad para aprender a la neurona.\n\n\n\n\n\n\n$w_0, w_1, w_2, \\dots, w_n$: Los pesos relativos de cada entrada. Tener en cuenta que incluso la unidad de sesgo tiene un peso.\n\n\n\n\n\n\na: La salida de la neurona. Que va a ser calculada de la siguiente forma:\n\n\n\n\n\n\n$$a = f\\left(\\sum_{i=0}^n w_i \\cdot x_i \\right)$$\n\n\nAqu\u00ed $f$ es la \nfunci\u00f3n de activaci\u00f3n\n de la neurona. Esta funci\u00f3n es la que le otorga tanta flexibilidad a las \nredes neuronales\n y le permite estimar complejas relaciones no lineales en los datos. Puede ser tanto una \nfunci\u00f3n lineal\n, una \nfunci\u00f3n log\u00edstica\n, \nhiperb\u00f3lica\n, etc.\n\n\nCada unidad neuronal est\u00e1 conectada con muchas otras y los enlaces entre ellas pueden incrementar o inhibir el estado de activaci\u00f3n de las neuronas adyacentes. Estos sistemas aprenden y se forman a s\u00ed mismos, en lugar de ser programados de forma expl\u00edcita, y sobresalen en \u00e1reas donde la detecci\u00f3n de soluciones o caracter\u00edsticas es dif\u00edcil de expresar con la programaci\u00f3n convencional.\n\n\n\n\nPropagaci\u00f3n hacia atr\u00e1s\n\n\nLa \npropagaci\u00f3n hacia atr\u00e1s\n o \nbackpropagation\n es un algoritmo que funciona mediante la determinaci\u00f3n de la p\u00e9rdida (o error) en la salida y luego propag\u00e1ndolo de nuevo hacia atr\u00e1s en la red. De esta forma los pesos se van actualizando para minimizar el error resultante de cada neurona. Este algoritmo es lo que les permite a las \nredes neuronales\n aprender.\n\n\n\n\n\u00bfC\u00f3mo funciona el Deep Learning?  \n\n\nEn general, cualquier t\u00e9cnica de \nMachine Learning\n trata de realizar la asignaci\u00f3n de entradas (por ejemplo, im\u00e1genes) a salidas objetivo (Por ejemplo, la etiqueta \"gato\"), mediante la observaci\u00f3n de un gran n\u00famero de ejemplos de entradas y salidas. El \nDeep Learning\n realiza este mapeo de entrada-a-objetivo por medio de una \nred neuronal artificial\n que est\u00e1 compuesta de un n\u00famero grande de \ncapas\n dispuestas en forma de jerarqu\u00eda. La \nred\n aprende algo simple en la capa inicial de la jerarqu\u00eda y luego env\u00eda esta informaci\u00f3n a la siguiente capa. La siguiente capa toma esta informaci\u00f3n simple, lo combina en algo que es un poco m\u00e1s complejo, y lo pasa a la tercer capa. Este proceso contin\u00faa de forma tal que cada capa de la jerarqu\u00eda construye algo m\u00e1s complejo de la entrada que recibi\u00f3 de la capa anterior. De esta forma, la \nred\n ir\u00e1 \naprendiendo\n por medio de la exposici\u00f3n a los datos de ejemplo.\n\n\nLa especificaci\u00f3n de lo que cada \ncapa\n hace a la entrada que recibe es almacenada en los \npesos\n de la capa, que en esencia, no son m\u00e1s que n\u00fameros. Utilizando terminolog\u00eda m\u00e1s t\u00e9cnica podemos decir que la transformaci\u00f3n de datos que se produce en la \ncapa\n es \nparametrizada\n por sus \npesos\n. Para que la \nred\n aprenda debemos encontrar los \npesos\n de todas las \ncapas\n de forma tal que la \nred\n realice un mapeo perfecto entre los ejemplos de entrada con sus respectivas salidas objetivo. Pero el problema reside en que una \nred\n de \nDeep Learning\n puede tener millones de \npar\u00e1metros\n, por lo que encontrar el valor correcto de todos ellos puede ser una tarea realmente muy dif\u00edcil, especialmente si la modificaci\u00f3n del valor de uno de ellos afecta a todos los dem\u00e1s.\n\n\n\n\nPara poder controlar algo, en primer lugar debemos poder observarlo. En este sentido, para controlar la salida de la \nred neuronal\n, deber\u00edamos poder medir cuan lejos esta la salida que obtuvimos de la que se esperaba obtener. Este es el trabajo de la \nfunci\u00f3n de p\u00e9rdida\n de la \nred\n. Esta funci\u00f3n toma las predicciones que realiza el modelo y los valores objetivos (lo que realmente esperamos que la \nred\n produzca), y calcula cu\u00e1n lejos estamos de ese valor, de esta manera, podemos capturar que tan bien esta funcionando el modelo para el ejemplo especificado. El truco fundamental del \nDeep Learning\n es utilizar el valor que nos devuelve esta  \nfunci\u00f3n de p\u00e9rdida\n para retroalimentar la \nred\n y ajustar los \npesos\n en la direcci\u00f3n que vayan reduciendo la \np\u00e9rdida\n del modelo para cada ejemplo. Este ajuste, es el trabajo del \noptimizador\n, el cu\u00e1l implementa la \npropagaci\u00f3n hacia atr\u00e1s\n. \n\n\n\n\nResumiendo, el funcionamiento ser\u00eda el siguiente: inicialmente, los \npesos\n de cada \ncapa\n son asignados en forma aleatoria, por lo que la \nred\n simplemente implementa una serie de transformaciones aleatorias. En este primer paso, obviamente la salida del modelo dista bastante del ideal que deseamos obtener, por lo que el valor de la \nfunci\u00f3n de p\u00e9rdida\n va a ser bastante alto. Pero a medida que la \nred\n va procesando nuevos casos, los \npesos\n se van ajustando de forma tal de ir reduciendo cada vez m\u00e1s el valor de la \nfunci\u00f3n de p\u00e9rdida\n. Este proceso es el que se conoce como \nentrenamiento\n de la \nred\n, el cual repetido una suficiente cantidad de veces, generalmente 10 iteraciones de miles de ejemplos, logra que los \npesos\n se ajusten a los que minimizan la \nfunci\u00f3n de p\u00e9rdida\n. Una \nred\n que ha minimizado la \np\u00e9rdida\n es la que logra los resultados que mejor se ajustan a las salidas objetivo, es decir, que el modelo se encuentra \nentrenado\n. \n\n\nArquitecturas de Deep Learning \n\n\nLa estructura de datos fundamental de una \nred neuronal\n est\u00e1 vagamente inspirada en el cerebro humano. Cada una de nuestras c\u00e9lulas cerebrales (neuronas) est\u00e1 conectada a muchas otras neuronas por sinapsis. A medida que experimentamos e interactuamos con el mundo, nuestro cerebro crea nuevas conexiones, refuerza algunas conexiones y debilita a los dem\u00e1s. De esta forma, en nuestro cerebro se desarrollan ciertas regiones que se especializan en el procesamiento de determinadas \nentradas\n. As\u00ed vamos a tener un \u00e1rea especializada en la visi\u00f3n, otra que se especializa en la audici\u00f3n, otra para el lenguaje, etc. De forma similar, dependiendo del tipo de \nentradas\n con las que trabajemos, van a existir distintas \narquitecturas\n de \nredes neuronales\n que mejor se adaptan para procesar esa informaci\u00f3n. Algunas de las arquitecturas m\u00e1s populares son:\n\n\nRedes neuronales prealimentadas\n\n\nLas \nRedes neuronales prealimentadas\n fueron las primeras que se desarrollaron y son el modelo m\u00e1s sencillo. En estas redes la informaci\u00f3n se mueve en una sola direcci\u00f3n: hacia adelante. Los principales exponentes de este tipo de arquitectura son el \nperceptr\u00f3n\n y el \nperceptr\u00f3n multicapa\n. Se suelen utilizar en problemas de clasificaci\u00f3n simples. \n\n\n\n\nRedes neuronales convolucionales\n\n\nLas \nredes neuronales convolucionales\n son muy similares a las \nredes neuronales\n ordinarias como el \nperceptron multicapa\n; se componen de \nneuronas\n que tienen \npesos\n y \nsesgos\n que pueden aprender. Cada \nneurona\n recibe algunas entradas, realiza un \nproducto escalar\n y luego aplica una funci\u00f3n de activaci\u00f3n. Al igual que en el \nperceptron multicapa\n tambi\u00e9n vamos a tener una \nfunci\u00f3n de p\u00e9rdida o costo\n sobre la \u00faltima capa, la cual estar\u00e1 totalmente conectada. Lo que diferencia a las \nredes neuronales convolucionales\n es que suponen expl\u00edcitamente que las entradas son im\u00e1genes, lo que nos permite codificar ciertas propiedades en la arquitectura; permitiendo ganar en eficiencia y reducir la cantidad de par\u00e1metros en la red. \n\n\nEn general, las \nredes neuronales convolucionales\n van a estar construidas con una estructura que contendr\u00e1 3 tipos distintos de capas:\n\n\n\n\nUna capa \nconvolucional\n, que es la que le da le nombre a la red.\n\n\nUna capa de reducci\u00f3n o de \npooling\n, la cual va a reducir la cantidad de par\u00e1metros al quedarse con las caracter\u00edsticas m\u00e1s comunes.\n\n\nUna capa clasificadora totalmente conectada, la cual nos va dar el resultado final de la red.\n\n\n\n\nAlgunas implementaciones espec\u00edficas que podemos encontrar sobre este tipo de redes son: \ninception v3\n, \nResNet\n, \nVGG16\n y \nxception\n, entre otras. Todas ellas han logrado excelentes resultados.\n\n\n\n\nRedes neuronales recurrentes\n\n\nLos seres humanos no comenzamos nuestro pensamiento desde cero cada segundo, sino que los mismos tienen una persistencia. Las \nRedes neuronales prealimentadas\n tradicionales no cuentan con esta persistencia, y esto parece una deficiencia importante. Las \nRedes neuronales recurrentes\n abordan este problema. Son redes con bucles de retroalimentaci\u00f3n, que permiten que la informaci\u00f3n persista.\n\n\nUna \nRed neural recurrente\n puede ser pensada como una red con m\u00faltiples copias de ella misma, en las que cada una de ellas pasa un mensaje a su sucesor. Esta naturaleza en forma de cadena revela que las \nRedes neurales recurrentes\n est\u00e1n \u00edntimamente relacionadas con las secuencias y listas; por lo que son ideales para trabajar con este tipo de datos. En los \u00faltimos a\u00f1os, ha habido un \u00e9xito incre\u00edble aplicando \nRedes neurales recurrentes\n  a una variedad de problemas como: reconocimiento de voz, modelado de lenguaje, traducci\u00f3n, subt\u00edtulos de im\u00e1genes y la lista contin\u00faa.\n\n\nLas \nredes de memoria de largo plazo a corto plazo\n - generalmente llamadas \nLSTMs\n - son un tipo especial de \nRedes neurales recurrentes\n, capaces de aprender dependencias a largo plazo. Ellas tambi\u00e9n tienen una estructura como cadena, pero el m\u00f3dulo de repetici\u00f3n tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, tiene cuatro, que interact\u00faan de una manera especial permitiendo tener una memoria a m\u00e1s largo plazo.\n\n\n\n\nPara m\u00e1s informaci\u00f3n sobre diferentes arquitecturas de \nredes neuronales\n pueden visitar el siguiente art\u00edculo de \nwikipedia\n.\n\n\nLogros del Deep Learning \n\n\nEn los \u00faltimos a\u00f1os el \nDeep Learning\n ha producido toda una revoluci\u00f3n en el campo del \nMachine Learning\n, con resultados notables en todos los problemas de \npercepci\u00f3n\n, como \nver\n y \nescuchar\n, problemas que implican habilidades que parecen muy naturales e intuitivas para los seres humanos, pero que desde hace tiempo se han mostrado dif\u00edciles para las m\u00e1quinas. En particular, el \nDeep Learning\n ha logrado los siguientes avances, todos ellos en \u00e1reas hist\u00f3ricamente dif\u00edciles del \nMachine Learning\n.\n\n\n\n\nUn nivel casi humano para la clasificaci\u00f3n de im\u00e1genes.\n\n\nUn nivel casi humano para el reconocimiento del lenguaje hablado.\n\n\nUn nivel casi humano en el reconocimiento de escritura.\n\n\nGrandes mejoras en traducciones de lenguas.\n\n\nGrandes mejoras en conversaciones \ntext-to-speech\n.\n\n\nAsistentes digitales como Google Now o Siri.\n\n\nUn nivel casi humano en autos aut\u00f3nomos.\n\n\nMejores resultados de b\u00fasqueda en la web.\n\n\nGrandes mejoras para responder preguntas en lenguaje natural.\n\n\nAlcanzado Nivel maestro (superior al humano) en varios juegos.\n\n\n\n\nEn muchos sentidos, el \nDeep Learning\n todav\u00eda sigue siendo un campo misterioso para explorar, por lo que seguramente veremos nuevos avances en nuevas \u00e1reas utilizando estas t\u00e9cnicas. Tal vez alg\u00fan d\u00eda el \nDeep Learning\n ayuda a los seres humanos a hacer ciencia, desarrollar software y mucho m\u00e1s.\n\n\n\u00bfPor qu\u00e9 estos sorprendentes resultados surgen ahora?  \n\n\nMuchos de los conceptos del \nDeep Learning\n se desarrollaron en los a\u00f1os 80s y 90s, algunos incluso mucho antes. Sin embargo, los primeros resultados exitosos del \nDeep Learning\n surgieron en los \u00faltimos 5 a\u00f1os. \u00bfqu\u00e9 fue lo que cambio para lograr la popularidad y \u00e9xito de los modelos basados en \nDeep Learning\n en estos \u00faltimos a\u00f1os? \n\n\nSi bien existen m\u00faltiples factores para explicar esta \nrevoluci\u00f3n\n del \nDeep Learning\n, los dos principales componentes parecen ser la \ndisponibilidad de masivos vol\u00famenes de datos\n, lo que actualmente se conoce bajo el nombre de \nBig Data\n; y el \nprogreso en el poder de computo\n, especialmente gracias a los \nGPUs\n. Entonces, dentro de los factores que explican esta popularidad de los modelos de \nDeep Learning\n podemos encontrar:\n\n\n\n\n\n\nLa disponibilidad de conjuntos de datos enormes y de buena calidad\n. Gracias a la  revoluci\u00f3n digital en que nos encontramos, podemos generar conjuntos de datos enormes con los cuales alimentar a los algoritmos de \nDeep Learning\n, los cuales necesitan de muchos datos para poder \ngeneralizar\n.\n\n\n\n\n\n\nComputaci\u00f3n paralela masiva con \nGPUs\n. En l\u00edneas generales, los modelos de \nredes neuronales\n no son m\u00e1s que complicados c\u00e1lculos num\u00e9ricos que se realizan en paralelo. Gracias al desarrollo de los \nGPUs\n estos c\u00e1lculos ahora se pueden realizar en forma mucho m\u00e1s r\u00e1pida, permitiendo que podamos entrenar modelos m\u00e1s profundos y grandes. \n\n\n\n\n\n\nFunciones de activaci\u00f3n amigables para \nBackpropagation\n. La \nprogaci\u00f3n hacia atr\u00e1s\n o \nBackpropagation\n es el algoritmo fundamental que hace funcionar a las \nredes neuronales\n; pero la forma en que trabaja implica c\u00e1lculos realmente complicados. La transici\u00f3n desde funciones de activaci\u00f3n como \ntanh\n o \nsigmoid\n a funciones como \nReLU\n o \nSELU\n han simplificado estos problemas. \n\n\n\n\n\n\nNuevas arquitecturas\n. Arquitecturas como \nResnets\n, \ninception\n y \nGAN\n mantienen el campo actualizado y contin\u00faan aumentando las flexibilidad de los modelos. \n\n\n\n\n\n\nNuevas t\u00e9cnicas de \nregularizaci\u00f3n\n. T\u00e9cnicas como \ndropout\n, \nbatch normalization\n y \ndata-augmentation\n nos permiten entrenar redes m\u00e1s grandes con menos peligro de \nsobreajuste\n.\n\n\n\n\n\n\nOptimizadores m\u00e1s robustos\n. La \noptimizaci\u00f3n\n es fundamental para el funcionamiento de las \nredes neuronales\n. Mejoras sobre el tradicional procedimiento de \nSGD\n, como \nADAM\n han ayudado a mejorar el rendimiento de los modelos.\n\n\n\n\n\n\nPlataformas de software\n. Herramientas como \nTensorFlow\n, \nTheano\n, \nKeras\n, \nCNTK\n, \nPyTorch\n, \nChainer\n, y \nmxnet\n nos permiten crear prototipos en forma m\u00e1s r\u00e1pida y trabajar con \nGPUs\n sin tantas complicaciones. Nos permiten enfocarnos en la estructura del modelo sin tener que preocuparnos por los detalles de m\u00e1s bajo nivel.\n\n\n\n\n\n\nOtra raz\u00f3n por la que el \nDeep Learning\n ha tenido tanta repercusi\u00f3n \u00faltimamente adem\u00e1s de ofrecer un mejor rendimiento en muchos problemas; es que el \nDeep Learning\n esta haciendo la resoluci\u00f3n de problemas mucho m\u00e1s f\u00e1cil, ya que automatiza completamente lo que sol\u00eda ser uno de los pasos m\u00e1s dif\u00edciles y cruciales en el flujo de trabajo de \nMachine Learning\n: la \ningenier\u00eda de atributos\n. Antes del \nDeep Learning\n, para poder entrenar un modelo, primero deb\u00edamos refinar las \nentradas\n para adaptarlas al tipo de transformaci\u00f3n del modelo; ten\u00edamos que cuidadosamente \nseleccionar los atributos\n m\u00e1s representativos y desechar los poco informativos. El \nDeep Learning\n, en cambio, automatiza este proceso; aprendemos todos los atributos de una sola pasada y el mismo modelo se encarga de adaptarse y quedarse con lo m\u00e1s representativo.  \n\n\n\u00bfC\u00f3mo mantenerse actualizado en el campo de Deep Learning? \n\n\nEl campo del \nDeep Learning\n se mueve muy r\u00e1pidamente, con varios \npapers\n que se publican por mes; por tal motivo, mantenerse actualizado con las \u00faltimas tendencias del campo puede ser bastante complicado. Algunos consejos pueden ser:\n\n\n\n\n\n\nEstarse atento a las publicaciones en \narxiv\n, especialmente a la secci\u00f3n de \nmachine learning\n. La mayor\u00eda de los \npapers\n m\u00e1s relevantes, los vamos a poder encontrar en esa plataforma.\n\n\n\n\n\n\nSeguir el blog de \nkeras\n en el cual podemos encontrar como implementar varios modelos utilizando esta genial librer\u00eda.\n\n\n\n\n\n\nSeguir el blog de \nopenai\n en d\u00f3nde detallan las investigaciones que van realizando, especialmente trabajando con \nGANs\n.\n\n\n\n\n\n\nSeguir el blog de \nGoogle research\n; en d\u00f3nde se viene haciendo bastante foco en los modelos de \nDeep Learning\n.\n\n\n\n\n\n\nUtilizar la secci\u00f3n de Machine Learning de \nreddit\n.\n\n\n\n\n\n\nSuscribirse al podcast \nTalking machines\n; en d\u00f3nde se entrevista a los principales exponentes del campo de la \nInteligencia Artificial\n.\n\n\n\n\n\n\nPor \u00faltimo, obviamente estar atentos a las \npublicaciones que se realizan en \nIAAR\n.",
            "title": "Deep Learning"
        },
        {
            "location": "/deeplearning/#introduccion-al-deep-learning",
            "text": "El  Deep Learning  es sin duda el \u00e1rea de investigaci\u00f3n m\u00e1s popular dentro del campo de la  Inteligencia Artificial . La mayor\u00eda de las nuevas investigaciones que se realizan, trabajan con modelos basados en las t\u00e9cnicas de  Deep Learning ; ya que las mismas han logrado resultados sorprendes en campos como  procesamiento del lenguaje natural  y  Visi\u00f3n por computadora . Pero... \u00bfqu\u00e9 es este misterioso concepto que ha ganado tanta popularidad? y... \u00bfc\u00f3mo se relaciona con el campo de la  Inteligencia Artificial  y el  Machine Learning ?.",
            "title": "Introducci\u00f3n al Deep Learning"
        },
        {
            "location": "/deeplearning/#inteligencia-artificial-machine-learning-y-deep-learning",
            "text": "En general se suelen utilizar los t\u00e9rminos de  Inteligencia Artificial ,  Machine Learning  y  Deep Learning  en forma intercambiada. Sin embargo, \u00e9stos t\u00e9rminos no son los mismo y abarcan distintas cosas.",
            "title": "Inteligencia artificial, Machine learning y Deep learning "
        },
        {
            "location": "/deeplearning/#inteligencia-artificial",
            "text": "El t\u00e9rmino  Inteligencia Artificial  es el m\u00e1s general y engloba a los campos de  Machine Learning  y  Deep Learning  junto con otras t\u00e9cnicas como los  algoritmos de b\u00fasqueda , el  razonamiento simb\u00f3lico , el  razonamiento l\u00f3gico  y la  estad\u00edstica . Naci\u00f3 en los a\u00f1os 1950s, cuando un grupo de pioneros de la computaci\u00f3n comenzaron a preguntarse si se pod\u00eda hacer que las computadoras  pensaran . Una definici\u00f3n concisa de la  Inteligencia Artificial  ser\u00eda:  el esfuerzo para automatizar las tareas intelectuales que normalmente realizan los seres humanos .",
            "title": "Inteligencia Artificial"
        },
        {
            "location": "/deeplearning/#machine-learning",
            "text": "El  Machine Learning  o  Aprendizaje autom\u00e1tico  se refiere a un amplio conjunto de t\u00e9cnicas inform\u00e1ticas que nos permiten dar a las computadoras  la capacidad de aprender sin ser expl\u00edcitamente programadas . Hay muchos tipos diferentes de algoritmos de  Aprendizaje autom\u00e1tico , entre los que se encuentran el  aprendizaje por refuerzo , los  algoritmos gen\u00e9ticos , el aprendizaje basado en  reglas de asociaci\u00f3n , los  algoritmos de agrupamiento , los  \u00e1rboles de decisi\u00f3n , las  m\u00e1quinas de vectores de soporte  y las  redes neuronales . Actualmente, los algoritmos m\u00e1s populares dentro de este campo son los de  Deep Learning .",
            "title": "Machine Learning"
        },
        {
            "location": "/deeplearning/#deep-learning",
            "text": "El  Deep Learning  o  aprendizaje profundo  es un subcampo dentro del  Machine Learning , el cu\u00e1l utiliza distintas estructuras de  redes neuronales  para lograr el aprendizaje de sucesivas  capas de representaciones  cada vez m\u00e1s significativas de los datos. El  profundo  o  deep  en  Deep Learning  hace referencia a la cantidad de  capas de representaciones  que se utilizan en el modelo; en general se suelen utilizar decenas o incluso cientos de  capas de representaci\u00f3n . las cuales  aprenden  autom\u00e1ticamente a medida que el modelo es entrenado con los datos.",
            "title": "Deep Learning"
        },
        {
            "location": "/deeplearning/#que-es-el-deep-learning",
            "text": "Antes de poder entender que es el  Deep Learning , debemos en primer lugar conocer dos conceptos fundamentales: las  redes neuronales artificiales  y la  Propagaci\u00f3n hacia atr\u00e1s .",
            "title": "\u00bfQu\u00e9 es el Deep Learning? "
        },
        {
            "location": "/deeplearning/#redes-neuronales",
            "text": "Las  redes neuronales  son un modelo computacional basado en un gran conjunto de unidades neuronales simples ( neuronas artificiales ), de forma aproximadamente an\u00e1loga al comportamiento observado en los axones de las neuronas en los cerebros biol\u00f3gicos.   Cada una de estas neuronas simples, va a tener una forma similar al siguiente diagrama:   En donde sus componentes son:    $x_1, x_2, \\dots, x_n$: Los datos de entrada en la neurona, los cuales tambi\u00e9n puede ser que sean producto de la salida de otra neurona de la red.    $x_0$: La unidad de sesgo; un valor constante que se le suma a la entrada de la funci\u00f3n de activaci\u00f3n de la neurona. Generalmente tiene el valor 1. Este valor va a permitir cambiar la funci\u00f3n de activaci\u00f3n hacia la derecha o izquierda, otorg\u00e1ndole m\u00e1s flexibilidad para aprender a la neurona.    $w_0, w_1, w_2, \\dots, w_n$: Los pesos relativos de cada entrada. Tener en cuenta que incluso la unidad de sesgo tiene un peso.    a: La salida de la neurona. Que va a ser calculada de la siguiente forma:    $$a = f\\left(\\sum_{i=0}^n w_i \\cdot x_i \\right)$$  Aqu\u00ed $f$ es la  funci\u00f3n de activaci\u00f3n  de la neurona. Esta funci\u00f3n es la que le otorga tanta flexibilidad a las  redes neuronales  y le permite estimar complejas relaciones no lineales en los datos. Puede ser tanto una  funci\u00f3n lineal , una  funci\u00f3n log\u00edstica ,  hiperb\u00f3lica , etc.  Cada unidad neuronal est\u00e1 conectada con muchas otras y los enlaces entre ellas pueden incrementar o inhibir el estado de activaci\u00f3n de las neuronas adyacentes. Estos sistemas aprenden y se forman a s\u00ed mismos, en lugar de ser programados de forma expl\u00edcita, y sobresalen en \u00e1reas donde la detecci\u00f3n de soluciones o caracter\u00edsticas es dif\u00edcil de expresar con la programaci\u00f3n convencional.",
            "title": "Redes Neuronales"
        },
        {
            "location": "/deeplearning/#propagacion-hacia-atras",
            "text": "La  propagaci\u00f3n hacia atr\u00e1s  o  backpropagation  es un algoritmo que funciona mediante la determinaci\u00f3n de la p\u00e9rdida (o error) en la salida y luego propag\u00e1ndolo de nuevo hacia atr\u00e1s en la red. De esta forma los pesos se van actualizando para minimizar el error resultante de cada neurona. Este algoritmo es lo que les permite a las  redes neuronales  aprender.",
            "title": "Propagaci\u00f3n hacia atr\u00e1s"
        },
        {
            "location": "/deeplearning/#como-funciona-el-deep-learning",
            "text": "En general, cualquier t\u00e9cnica de  Machine Learning  trata de realizar la asignaci\u00f3n de entradas (por ejemplo, im\u00e1genes) a salidas objetivo (Por ejemplo, la etiqueta \"gato\"), mediante la observaci\u00f3n de un gran n\u00famero de ejemplos de entradas y salidas. El  Deep Learning  realiza este mapeo de entrada-a-objetivo por medio de una  red neuronal artificial  que est\u00e1 compuesta de un n\u00famero grande de  capas  dispuestas en forma de jerarqu\u00eda. La  red  aprende algo simple en la capa inicial de la jerarqu\u00eda y luego env\u00eda esta informaci\u00f3n a la siguiente capa. La siguiente capa toma esta informaci\u00f3n simple, lo combina en algo que es un poco m\u00e1s complejo, y lo pasa a la tercer capa. Este proceso contin\u00faa de forma tal que cada capa de la jerarqu\u00eda construye algo m\u00e1s complejo de la entrada que recibi\u00f3 de la capa anterior. De esta forma, la  red  ir\u00e1  aprendiendo  por medio de la exposici\u00f3n a los datos de ejemplo.  La especificaci\u00f3n de lo que cada  capa  hace a la entrada que recibe es almacenada en los  pesos  de la capa, que en esencia, no son m\u00e1s que n\u00fameros. Utilizando terminolog\u00eda m\u00e1s t\u00e9cnica podemos decir que la transformaci\u00f3n de datos que se produce en la  capa  es  parametrizada  por sus  pesos . Para que la  red  aprenda debemos encontrar los  pesos  de todas las  capas  de forma tal que la  red  realice un mapeo perfecto entre los ejemplos de entrada con sus respectivas salidas objetivo. Pero el problema reside en que una  red  de  Deep Learning  puede tener millones de  par\u00e1metros , por lo que encontrar el valor correcto de todos ellos puede ser una tarea realmente muy dif\u00edcil, especialmente si la modificaci\u00f3n del valor de uno de ellos afecta a todos los dem\u00e1s.   Para poder controlar algo, en primer lugar debemos poder observarlo. En este sentido, para controlar la salida de la  red neuronal , deber\u00edamos poder medir cuan lejos esta la salida que obtuvimos de la que se esperaba obtener. Este es el trabajo de la  funci\u00f3n de p\u00e9rdida  de la  red . Esta funci\u00f3n toma las predicciones que realiza el modelo y los valores objetivos (lo que realmente esperamos que la  red  produzca), y calcula cu\u00e1n lejos estamos de ese valor, de esta manera, podemos capturar que tan bien esta funcionando el modelo para el ejemplo especificado. El truco fundamental del  Deep Learning  es utilizar el valor que nos devuelve esta   funci\u00f3n de p\u00e9rdida  para retroalimentar la  red  y ajustar los  pesos  en la direcci\u00f3n que vayan reduciendo la  p\u00e9rdida  del modelo para cada ejemplo. Este ajuste, es el trabajo del  optimizador , el cu\u00e1l implementa la  propagaci\u00f3n hacia atr\u00e1s .    Resumiendo, el funcionamiento ser\u00eda el siguiente: inicialmente, los  pesos  de cada  capa  son asignados en forma aleatoria, por lo que la  red  simplemente implementa una serie de transformaciones aleatorias. En este primer paso, obviamente la salida del modelo dista bastante del ideal que deseamos obtener, por lo que el valor de la  funci\u00f3n de p\u00e9rdida  va a ser bastante alto. Pero a medida que la  red  va procesando nuevos casos, los  pesos  se van ajustando de forma tal de ir reduciendo cada vez m\u00e1s el valor de la  funci\u00f3n de p\u00e9rdida . Este proceso es el que se conoce como  entrenamiento  de la  red , el cual repetido una suficiente cantidad de veces, generalmente 10 iteraciones de miles de ejemplos, logra que los  pesos  se ajusten a los que minimizan la  funci\u00f3n de p\u00e9rdida . Una  red  que ha minimizado la  p\u00e9rdida  es la que logra los resultados que mejor se ajustan a las salidas objetivo, es decir, que el modelo se encuentra  entrenado .",
            "title": "\u00bfC\u00f3mo funciona el Deep Learning?  "
        },
        {
            "location": "/deeplearning/#arquitecturas-de-deep-learning",
            "text": "La estructura de datos fundamental de una  red neuronal  est\u00e1 vagamente inspirada en el cerebro humano. Cada una de nuestras c\u00e9lulas cerebrales (neuronas) est\u00e1 conectada a muchas otras neuronas por sinapsis. A medida que experimentamos e interactuamos con el mundo, nuestro cerebro crea nuevas conexiones, refuerza algunas conexiones y debilita a los dem\u00e1s. De esta forma, en nuestro cerebro se desarrollan ciertas regiones que se especializan en el procesamiento de determinadas  entradas . As\u00ed vamos a tener un \u00e1rea especializada en la visi\u00f3n, otra que se especializa en la audici\u00f3n, otra para el lenguaje, etc. De forma similar, dependiendo del tipo de  entradas  con las que trabajemos, van a existir distintas  arquitecturas  de  redes neuronales  que mejor se adaptan para procesar esa informaci\u00f3n. Algunas de las arquitecturas m\u00e1s populares son:",
            "title": "Arquitecturas de Deep Learning "
        },
        {
            "location": "/deeplearning/#redes-neuronales-prealimentadas",
            "text": "Las  Redes neuronales prealimentadas  fueron las primeras que se desarrollaron y son el modelo m\u00e1s sencillo. En estas redes la informaci\u00f3n se mueve en una sola direcci\u00f3n: hacia adelante. Los principales exponentes de este tipo de arquitectura son el  perceptr\u00f3n  y el  perceptr\u00f3n multicapa . Se suelen utilizar en problemas de clasificaci\u00f3n simples.",
            "title": "Redes neuronales prealimentadas"
        },
        {
            "location": "/deeplearning/#redes-neuronales-convolucionales",
            "text": "Las  redes neuronales convolucionales  son muy similares a las  redes neuronales  ordinarias como el  perceptron multicapa ; se componen de  neuronas  que tienen  pesos  y  sesgos  que pueden aprender. Cada  neurona  recibe algunas entradas, realiza un  producto escalar  y luego aplica una funci\u00f3n de activaci\u00f3n. Al igual que en el  perceptron multicapa  tambi\u00e9n vamos a tener una  funci\u00f3n de p\u00e9rdida o costo  sobre la \u00faltima capa, la cual estar\u00e1 totalmente conectada. Lo que diferencia a las  redes neuronales convolucionales  es que suponen expl\u00edcitamente que las entradas son im\u00e1genes, lo que nos permite codificar ciertas propiedades en la arquitectura; permitiendo ganar en eficiencia y reducir la cantidad de par\u00e1metros en la red.   En general, las  redes neuronales convolucionales  van a estar construidas con una estructura que contendr\u00e1 3 tipos distintos de capas:   Una capa  convolucional , que es la que le da le nombre a la red.  Una capa de reducci\u00f3n o de  pooling , la cual va a reducir la cantidad de par\u00e1metros al quedarse con las caracter\u00edsticas m\u00e1s comunes.  Una capa clasificadora totalmente conectada, la cual nos va dar el resultado final de la red.   Algunas implementaciones espec\u00edficas que podemos encontrar sobre este tipo de redes son:  inception v3 ,  ResNet ,  VGG16  y  xception , entre otras. Todas ellas han logrado excelentes resultados.",
            "title": "Redes neuronales convolucionales"
        },
        {
            "location": "/deeplearning/#redes-neuronales-recurrentes",
            "text": "Los seres humanos no comenzamos nuestro pensamiento desde cero cada segundo, sino que los mismos tienen una persistencia. Las  Redes neuronales prealimentadas  tradicionales no cuentan con esta persistencia, y esto parece una deficiencia importante. Las  Redes neuronales recurrentes  abordan este problema. Son redes con bucles de retroalimentaci\u00f3n, que permiten que la informaci\u00f3n persista.  Una  Red neural recurrente  puede ser pensada como una red con m\u00faltiples copias de ella misma, en las que cada una de ellas pasa un mensaje a su sucesor. Esta naturaleza en forma de cadena revela que las  Redes neurales recurrentes  est\u00e1n \u00edntimamente relacionadas con las secuencias y listas; por lo que son ideales para trabajar con este tipo de datos. En los \u00faltimos a\u00f1os, ha habido un \u00e9xito incre\u00edble aplicando  Redes neurales recurrentes   a una variedad de problemas como: reconocimiento de voz, modelado de lenguaje, traducci\u00f3n, subt\u00edtulos de im\u00e1genes y la lista contin\u00faa.  Las  redes de memoria de largo plazo a corto plazo  - generalmente llamadas  LSTMs  - son un tipo especial de  Redes neurales recurrentes , capaces de aprender dependencias a largo plazo. Ellas tambi\u00e9n tienen una estructura como cadena, pero el m\u00f3dulo de repetici\u00f3n tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, tiene cuatro, que interact\u00faan de una manera especial permitiendo tener una memoria a m\u00e1s largo plazo.   Para m\u00e1s informaci\u00f3n sobre diferentes arquitecturas de  redes neuronales  pueden visitar el siguiente art\u00edculo de  wikipedia .",
            "title": "Redes neuronales recurrentes"
        },
        {
            "location": "/deeplearning/#logros-del-deep-learning",
            "text": "En los \u00faltimos a\u00f1os el  Deep Learning  ha producido toda una revoluci\u00f3n en el campo del  Machine Learning , con resultados notables en todos los problemas de  percepci\u00f3n , como  ver  y  escuchar , problemas que implican habilidades que parecen muy naturales e intuitivas para los seres humanos, pero que desde hace tiempo se han mostrado dif\u00edciles para las m\u00e1quinas. En particular, el  Deep Learning  ha logrado los siguientes avances, todos ellos en \u00e1reas hist\u00f3ricamente dif\u00edciles del  Machine Learning .   Un nivel casi humano para la clasificaci\u00f3n de im\u00e1genes.  Un nivel casi humano para el reconocimiento del lenguaje hablado.  Un nivel casi humano en el reconocimiento de escritura.  Grandes mejoras en traducciones de lenguas.  Grandes mejoras en conversaciones  text-to-speech .  Asistentes digitales como Google Now o Siri.  Un nivel casi humano en autos aut\u00f3nomos.  Mejores resultados de b\u00fasqueda en la web.  Grandes mejoras para responder preguntas en lenguaje natural.  Alcanzado Nivel maestro (superior al humano) en varios juegos.   En muchos sentidos, el  Deep Learning  todav\u00eda sigue siendo un campo misterioso para explorar, por lo que seguramente veremos nuevos avances en nuevas \u00e1reas utilizando estas t\u00e9cnicas. Tal vez alg\u00fan d\u00eda el  Deep Learning  ayuda a los seres humanos a hacer ciencia, desarrollar software y mucho m\u00e1s.",
            "title": "Logros del Deep Learning "
        },
        {
            "location": "/deeplearning/#por-que-estos-sorprendentes-resultados-surgen-ahora",
            "text": "Muchos de los conceptos del  Deep Learning  se desarrollaron en los a\u00f1os 80s y 90s, algunos incluso mucho antes. Sin embargo, los primeros resultados exitosos del  Deep Learning  surgieron en los \u00faltimos 5 a\u00f1os. \u00bfqu\u00e9 fue lo que cambio para lograr la popularidad y \u00e9xito de los modelos basados en  Deep Learning  en estos \u00faltimos a\u00f1os?   Si bien existen m\u00faltiples factores para explicar esta  revoluci\u00f3n  del  Deep Learning , los dos principales componentes parecen ser la  disponibilidad de masivos vol\u00famenes de datos , lo que actualmente se conoce bajo el nombre de  Big Data ; y el  progreso en el poder de computo , especialmente gracias a los  GPUs . Entonces, dentro de los factores que explican esta popularidad de los modelos de  Deep Learning  podemos encontrar:    La disponibilidad de conjuntos de datos enormes y de buena calidad . Gracias a la  revoluci\u00f3n digital en que nos encontramos, podemos generar conjuntos de datos enormes con los cuales alimentar a los algoritmos de  Deep Learning , los cuales necesitan de muchos datos para poder  generalizar .    Computaci\u00f3n paralela masiva con  GPUs . En l\u00edneas generales, los modelos de  redes neuronales  no son m\u00e1s que complicados c\u00e1lculos num\u00e9ricos que se realizan en paralelo. Gracias al desarrollo de los  GPUs  estos c\u00e1lculos ahora se pueden realizar en forma mucho m\u00e1s r\u00e1pida, permitiendo que podamos entrenar modelos m\u00e1s profundos y grandes.     Funciones de activaci\u00f3n amigables para  Backpropagation . La  progaci\u00f3n hacia atr\u00e1s  o  Backpropagation  es el algoritmo fundamental que hace funcionar a las  redes neuronales ; pero la forma en que trabaja implica c\u00e1lculos realmente complicados. La transici\u00f3n desde funciones de activaci\u00f3n como  tanh  o  sigmoid  a funciones como  ReLU  o  SELU  han simplificado estos problemas.     Nuevas arquitecturas . Arquitecturas como  Resnets ,  inception  y  GAN  mantienen el campo actualizado y contin\u00faan aumentando las flexibilidad de los modelos.     Nuevas t\u00e9cnicas de  regularizaci\u00f3n . T\u00e9cnicas como  dropout ,  batch normalization  y  data-augmentation  nos permiten entrenar redes m\u00e1s grandes con menos peligro de  sobreajuste .    Optimizadores m\u00e1s robustos . La  optimizaci\u00f3n  es fundamental para el funcionamiento de las  redes neuronales . Mejoras sobre el tradicional procedimiento de  SGD , como  ADAM  han ayudado a mejorar el rendimiento de los modelos.    Plataformas de software . Herramientas como  TensorFlow ,  Theano ,  Keras ,  CNTK ,  PyTorch ,  Chainer , y  mxnet  nos permiten crear prototipos en forma m\u00e1s r\u00e1pida y trabajar con  GPUs  sin tantas complicaciones. Nos permiten enfocarnos en la estructura del modelo sin tener que preocuparnos por los detalles de m\u00e1s bajo nivel.    Otra raz\u00f3n por la que el  Deep Learning  ha tenido tanta repercusi\u00f3n \u00faltimamente adem\u00e1s de ofrecer un mejor rendimiento en muchos problemas; es que el  Deep Learning  esta haciendo la resoluci\u00f3n de problemas mucho m\u00e1s f\u00e1cil, ya que automatiza completamente lo que sol\u00eda ser uno de los pasos m\u00e1s dif\u00edciles y cruciales en el flujo de trabajo de  Machine Learning : la  ingenier\u00eda de atributos . Antes del  Deep Learning , para poder entrenar un modelo, primero deb\u00edamos refinar las  entradas  para adaptarlas al tipo de transformaci\u00f3n del modelo; ten\u00edamos que cuidadosamente  seleccionar los atributos  m\u00e1s representativos y desechar los poco informativos. El  Deep Learning , en cambio, automatiza este proceso; aprendemos todos los atributos de una sola pasada y el mismo modelo se encarga de adaptarse y quedarse con lo m\u00e1s representativo.",
            "title": "\u00bfPor qu\u00e9 estos sorprendentes resultados surgen ahora?  "
        },
        {
            "location": "/deeplearning/#como-mantenerse-actualizado-en-el-campo-de-deep-learning",
            "text": "El campo del  Deep Learning  se mueve muy r\u00e1pidamente, con varios  papers  que se publican por mes; por tal motivo, mantenerse actualizado con las \u00faltimas tendencias del campo puede ser bastante complicado. Algunos consejos pueden ser:    Estarse atento a las publicaciones en  arxiv , especialmente a la secci\u00f3n de  machine learning . La mayor\u00eda de los  papers  m\u00e1s relevantes, los vamos a poder encontrar en esa plataforma.    Seguir el blog de  keras  en el cual podemos encontrar como implementar varios modelos utilizando esta genial librer\u00eda.    Seguir el blog de  openai  en d\u00f3nde detallan las investigaciones que van realizando, especialmente trabajando con  GANs .    Seguir el blog de  Google research ; en d\u00f3nde se viene haciendo bastante foco en los modelos de  Deep Learning .    Utilizar la secci\u00f3n de Machine Learning de  reddit .    Suscribirse al podcast  Talking machines ; en d\u00f3nde se entrevista a los principales exponentes del campo de la  Inteligencia Artificial .    Por \u00faltimo, obviamente estar atentos a las  publicaciones que se realizan en  IAAR .",
            "title": "\u00bfC\u00f3mo mantenerse actualizado en el campo de Deep Learning? "
        },
        {
            "location": "/machine-learning/",
            "text": "Introducci\u00f3n al Machine Learning\n\n\n\n\nUna de las ramas de estudio que cada vez esta ganando m\u00e1s popularidad dentro de las \nciencias de la computaci\u00f3n\n es el \naprendizaje autom\u00e1tico\n o \nMachine Learning\n. Muchos de los servicios que utilizamos en nuestro d\u00eda a d\u00eda como google, gmail, netflix, spotify o amazon se valen de las herramientas que les brinda el \nMachine Learning\n para alcanzar un servicio cada vez m\u00e1s personalizado y lograr as\u00ed ventajas competitivas sobre sus rivales. \n\n\n\u00bfQu\u00e9 es Machine Learning?\n\n\nPero, \u00bfqu\u00e9 es exactamente \nMachine Learning\n?. El \nMachine Learning\n es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprender de los datos. El objetivo fundamental del \nMachine Learning\n es \ngeneralizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada\n. El ejemplo m\u00e1s t\u00edpico donde podemos ver el uso del \nMachine Learning\n es en el filtrado de los correo basura o spam. Mediante la observaci\u00f3n de miles de correos electr\u00f3nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos. El \nMachine Learning\n combina conceptos y t\u00e9cnicas de diferentes \u00e1reas del conocimiento, como las \nmatem\u00e1ticas\n, \nestad\u00edsticas\n y las \nciencias de la computaci\u00f3n\n; por tal motivo, hay muchas maneras de aprender la disciplina.\n\n\nTipos de Machine Learning\n\n\nEl \nMachine Learning\n tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito, an\u00e1lisis del mercado de valores, clasificaci\u00f3n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob\u00f3tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distinguir los distintos tipos de problemas de \nMachine Learning\n con los que nos podemos encontrar.\n\n\nAprendizaje supervisado\n\n\nEn los problemas de \naprendizaje supervisado\n se ense\u00f1a o entrena al \nalgoritmo\n a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos m\u00e1s el \nalgoritmo\n puede aprender sobre el tema. Una vez concluido el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el \nalgoritmo\n de aprendizaje utiliza la experiencia pasada que adquiri\u00f3 durante la etapa de entrenamiento para predecir un resultado. Esto es similar al m\u00e9todo de aprendizaje que se utiliza en las escuelas, donde se nos ense\u00f1an problemas y las formas de resolverlos, para que luego podamos aplicar los mismos m\u00e9todos en situaciones similares.\n\n\nAprendizaje no supervisado\n\n\nEn los problemas de \naprendizaje no supervisado\n el \nalgoritmo\n es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al \nalgoritmo\n lo que representan los datos. La idea es que el \nalgoritmo\n pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos. El \naprendizaje no supervisado\n es similar al m\u00e9todo que utilizamos para aprender a hablar cuando somos bebes, en un principio escuchamos hablar a nuestros padres y no entendemos nada; pero a medida que vamos escuchando miles de conversaciones, nuestro cerebro comenzar\u00e1 a formar un modelo sobre c\u00f3mo funciona el lenguaje y comenzaremos a reconocer patrones y a esperar ciertos sonidos. \n\n\nAprendizaje por refuerzo\n\n\nEn los problemas de aprendizaje por refuerzo, el \nalgoritmo\n aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m\u00e1s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar\u00e1 creando una estrategia ganadora.\n\n\nSobreajuste\n\n\nComo mencionamos cuando definimos al \nMachine Learning\n, la idea fundamental es encontrar patrones que podamos generalizar para luego poder aplicar esta generalizaci\u00f3n sobre los casos que todav\u00eda no hemos observado y realizar predicciones. Pero tambi\u00e9n puede ocurrir que durante el entrenamiento solo descubramos casualidades en los datos que se parecen a patrones interesantes, pero que no generalicen. Esto es lo que se conoce con el nombre de \nsobreajuste\n o sobreentrenamiento\n.\n\n\nEl \nsobreajuste\n es la tendencia que tienen la mayor\u00eda de los \nalgoritmos\n de \nMachine Learning\n a ajustarse a unas caracter\u00edsticas muy espec\u00edficas de los datos de entrenamiento que no tienen relaci\u00f3n causal con la \nfunci\u00f3n objetivo\n que estamos buscando para generalizar. El ejemplo m\u00e1s extremo de un modelo \nsobreajustado\n es un modelo que solo memoriza las respuestas correctas; este modelo al ser utilizado con datos que nunca antes ha visto va a tener un rendimiento azaroso, ya que nunca logr\u00f3 generalizar un patr\u00f3n para predecir.\n\n\nComo evitar el sobreajuste\n\n\nComo mencionamos anteriormente, todos los modelos de \nMachine Learning\n tienen tendencia al \nsobreajuste\n; es por esto que debemos aprender a convivir con el mismo y tratar de tomar medidas preventivas para reducirlo lo m\u00e1s posible. Las dos principales estrategias para lidiar son el \nsobreajuste\n son: la \nretenci\u00f3n de datos\n y la \nvalidaci\u00f3n cruzada\n.\n\n\nEn el primer caso, la idea es dividir nuestro \nconjunto de datos\n, en uno o varios conjuntos de entrenamiento y otro/s conjuntos de evaluaci\u00f3n. Es decir, que no le vamos a pasar todos nuestros datos al \nalgoritmo\n durante el entrenamiento, sino que vamos a \nretener\n una parte de los datos de entrenamiento para realizar una evaluaci\u00f3n de la efectividad del modelo. Con esto lo que buscamos es evitar que los mismos datos que usamos para entrenar sean los mismos que utilizamos para evaluar. De esta forma vamos a poder analizar con m\u00e1s precisi\u00f3n como el modelo se va comportando a medida que m\u00e1s lo vamos entrenando y poder detectar el punto cr\u00edtico en el que el modelo deja de generalizar y comienza a \nsobreajustarse\n a los datos de entrenamiento.\n\n\nLa \nvalidaci\u00f3n cruzada\n es un procedimiento m\u00e1s sofisticado que el anterior. En lugar de solo obtener una simple estimaci\u00f3n de la efectividad de la \ngeneralizaci\u00f3n\n; la idea es realizar un an\u00e1lisis estad\u00edstico para obtener otras medidas del rendimiento estimado, como la media y la varianza, y as\u00ed poder entender c\u00f3mo se espera que el rendimiento var\u00ede a trav\u00e9s de los distintos conjuntos de datos. Esta variaci\u00f3n es fundamental para la evaluaci\u00f3n de la confianza en la estimaci\u00f3n del rendimiento.\nLa \nvalidaci\u00f3n cruzada\n tambi\u00e9n hace un mejor uso de un conjunto de datos limitado; ya que a diferencia de la simple divisi\u00f3n de los datos en uno el entrenamiento y otro de evaluaci\u00f3n; la \nvalidaci\u00f3n cruzada\n calcula sus estimaciones sobre todo el \nconjunto de datos\n mediante la realizaci\u00f3n de m\u00faltiples divisiones e intercambios sistem\u00e1ticos entre datos de entrenamiento y datos de evaluaci\u00f3n.\n\n\nPasos para construir un modelo de machine learning\n\n\nConstruir un modelo de \nMachine Learning\n, no se reduce solo a utilizar un \nalgoritmo\n de aprendizaje o utilizar una librer\u00eda de \nMachine Learning\n; sino que es todo un proceso que suele involucrar los siguientes pasos:\n\n\n\n\n\n\nRecolectar los datos\n. Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una \nAPI\n o desde una base de datos. Podemos tambi\u00e9n utilizar otros dispositivos que recolectan los datos por nosotros; o utilizar datos que son de dominio p\u00fablico. El n\u00famero de opciones que tenemos para recolectar datos no tiene fin!. Este paso parece obvio, pero es uno de los que m\u00e1s complicaciones trae y m\u00e1s tiempo consume. \n\n\n\n\n\n\nPreprocesar los datos\n. Una vez que tenemos los datos, tenemos que asegurarnos que tiene el formato correcto para nutrir nuestro \nalgoritmo\n de aprendizaje. Es pr\u00e1cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m\u00e1s sencillo que el paso anterior.\n\n\n\n\n\n\nExplorar los datos\n. Una vez que ya tenemos los datos y est\u00e1n con el formato correcto, podemos realizar un pre an\u00e1lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg\u00fan patr\u00f3n en los mismos que nos facilite la construcci\u00f3n del modelo. En esta etapa suelen ser de mucha utilidad las medidas estad\u00edsticas y los gr\u00e1ficos en 2 y 3 dimensiones para tener una idea visual de como se comportan nuestros datos. En este punto podemos detectar \nvalores at\u00edpicos\n que debamos descartar; o encontrar las caracter\u00edsticas que m\u00e1s influencia tienen para realizar una predicci\u00f3n.\n\n\n\n\n\n\nEntrenar el \nalgoritmo\n. Aqu\u00ed es donde comenzamos a utilizar las t\u00e9cnicas de \nMachine Learning\n realmente. En esta etapa nutrimos al o los \nalgoritmos\n de aprendizaje con los datos que venimos procesando en las etapas anteriores. La idea es que los \nalgoritmos\n puedan extraer informaci\u00f3n \u00fatil de los datos que le pasamos para luego poder hacer predicciones. \n\n\n\n\n\n\nEvaluar el \nalgoritmo\n. En esta etapa ponemos a prueba la informaci\u00f3n o conocimiento que el \nalgoritmo\n obtuvo del entrenamiento del paso anterior. Evaluamos que tan preciso es el algoritmo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el \nalgoritmo\n cambiando algunos par\u00e1metros hasta lograr un rendimiento aceptable.  \n\n\n\n\n\n\nUtilizar el modelo\n. En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu\u00ed tambi\u00e9n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores. \n\n\n\n\n\n\nLibrer\u00edas de Python para machine learning\n\n\nComo siempre me gusta comentar, una de las grandes ventajas que ofrece \nPython\n sobre otros lenguajes de programaci\u00f3n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer\u00edas de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de \nMachine Learning\n, las principales librer\u00edas que podemos utilizar son: \n\n\nScikit-Learn\n\n\nScikit-learn\n es la principal librer\u00eda que existe para trabajar con \nMachine Learning\n, incluye la implementaci\u00f3n de un gran n\u00famero de \nalgoritmos\n de aprendizaje. La podemos utilizar para \nclasificaciones\n, \nextraccion de caracter\u00edsticas\n, \nregresiones\n, \nagrupaciones\n, \nreducci\u00f3n de dimensiones\n, \nselecci\u00f3n de modelos\n, o \npreprocesamiento\n. Posee una \nAPI\n que es consistente en todos los modelos y se integra muy bien con el resto de los paquetes cient\u00edficos que ofrece \nPython\n. Esta librer\u00eda tambi\u00e9n nos facilita las tareas de evaluaci\u00f3n, diagnostico y \nvalidaciones cruzadas\n ya que nos proporciona varios m\u00e9todos de f\u00e1brica para poder realizar estas tareas en forma muy simple. \n\n\nStatsmodels\n\n\nStatsmodels\n es otra gran librer\u00eda que hace foco en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios. Al igual que \nScikit-learn\n, tambi\u00e9n se integra muy bien con el resto de los paquetes cient\u00edficos de \nPython\n. Si deseamos ajustar modelos lineales, hacer una an\u00e1lisis estad\u00edstico, o tal vez un poco de modelado predictivo, entonces \nStatsmodels\n es la librer\u00eda ideal. Las pruebas estad\u00edsticas que ofrece son bastante amplias y abarcan tareas de validaci\u00f3n para la mayor\u00eda de los casos. \n\n\nPyMC\n\n\npyMC\n es un m\u00f3dulo de \nPython\n que implementa modelos estad\u00edsticos bayesianos, incluyendo la \ncadena de Markov Monte Carlo(MCMC)\n. \npyMC\n  ofrece funcionalidades para hacer el an\u00e1lisis bayesiano lo mas simple posible. Incluye los modelos \nbayesianos\n,  distribuciones estad\u00edsticas y herramientas de diagnostico  para la covarianza de los modelos. Si queremos realizar un an\u00e1lisis \nbayesiano\n esta es sin duda la librer\u00eda a utilizar.\n\n\nNTLK\n\n\nNLTK\n es la librer\u00eda l\u00edder para el procesamiento del lenguaje natural o \nNLP\n por sus siglas en ingl\u00e9s. Proporciona interfaces f\u00e1ciles de usar a m\u00e1s de 50 cuerpos y recursos l\u00e9xicos, como \nWordNet\n, junto con un conjunto de bibliotecas de procesamiento de texto para la clasificaci\u00f3n, tokenizaci\u00f3n, el etiquetado, el an\u00e1lisis y el razonamiento sem\u00e1ntico. \n\n\nObviamente, aqu\u00ed solo estoy listando unas pocas de las muchas librer\u00edas que existen en \nPython\n para trabajar con problemas de \nMachine Learning\n, los invito a realizar su propia investigaci\u00f3n sobre el tema.\n\n\nAlgoritmos m\u00e1s utilizados\n\n\nLos \nalgoritmos\n  que m\u00e1s se suelen utilizar en los problemas de \nMachine Learning\n son los siguientes:\n\n\n\n\nRegresi\u00f3n Lineal\n\n\nRegresi\u00f3n Log\u00edstica\n\n\nArboles de Decision\n\n\nRandom Forest\n\n\nSVM\n o M\u00e1quinas de vectores de soporte.\n\n\nKNN\n o K vecinos m\u00e1s cercanos.\n\n\nK-means\n\n\n\n\nTodos ellos se pueden aplicar a casi cualquier problema de datos y obviamente est\u00e1n todos implementados por la excelente librer\u00eda de \nPython\n, \nScikit-learn\n. Veamos algunos ejemplos de ellos.\n\n\nRegresi\u00f3n Lineal\n\n\nSe utiliza para estimar los valores reales (costo de las viviendas, el n\u00famero de llamadas, ventas totales, etc.) basados en variables continuas. La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar una mejor l\u00ednea recta con respecto a los puntos. Esta l\u00ednea de mejor ajuste se conoce como l\u00ednea de regresi\u00f3n y esta representada por la siguiente ecuaci\u00f3n lineal:\n\n\n$$Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{n}X_{n}$$\n\n\nVeamos un peque\u00f1o ejemplo de como se implementa en \nPython\n. En este ejemplo voy a utilizar el dataset Boston que ya viene junto con \nScikit-learn\n y es ideal para practicar con \nRegresiones Lineales\n; el mismo contiene precios de casas de varias \u00e1reas de la ciudad de Boston. \n\n\n# importando pandas, numpy y matplotlib\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# importando los datasets de sklearn\nfrom sklearn import datasets\n\nboston = datasets.load_boston()\nboston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\nboston_df['TARGET'] = boston.target\nboston_df.head() # estructura de nuestro dataset.\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \nTARGET\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0.00632\n\n      \n18.0\n\n      \n2.31\n\n      \n0.0\n\n      \n0.538\n\n      \n6.575\n\n      \n65.2\n\n      \n4.0900\n\n      \n1.0\n\n      \n296.0\n\n      \n15.3\n\n      \n396.90\n\n      \n4.98\n\n      \n24.0\n\n    \n\n    \n\n      \n1\n\n      \n0.02731\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n6.421\n\n      \n78.9\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n396.90\n\n      \n9.14\n\n      \n21.6\n\n    \n\n    \n\n      \n2\n\n      \n0.02729\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n7.185\n\n      \n61.1\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n392.83\n\n      \n4.03\n\n      \n34.7\n\n    \n\n    \n\n      \n3\n\n      \n0.03237\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n6.998\n\n      \n45.8\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n394.63\n\n      \n2.94\n\n      \n33.4\n\n    \n\n    \n\n      \n4\n\n      \n0.06905\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n7.147\n\n      \n54.2\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n396.90\n\n      \n5.33\n\n      \n36.2\n\n    \n\n  \n\n\n\n\n\n# importando el modelo de regresi\u00f3n lineal\nfrom sklearn.linear_model import LinearRegression\n\nrl = LinearRegression() # Creando el modelo.\nrl.fit(boston.data, boston.target) # ajustando el modelo\n\n# haciendo las predicciones\npredicciones = rl.predict(boston.data)\npredicciones_df = pd.DataFrame(predicciones, columns=['Pred'])\npredicciones_df.head() # predicciones de las primeras 5 lineas\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPred\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n30.008213\n\n    \n\n    \n\n      \n1\n\n      \n25.029861\n\n    \n\n    \n\n      \n2\n\n      \n30.570232\n\n    \n\n    \n\n      \n3\n\n      \n28.608141\n\n    \n\n    \n\n      \n4\n\n      \n27.942882\n\n    \n\n  \n\n\n\n\n\n# Calculando el desvio\nnp.mean(boston.target - predicciones)\n\n\n\n\nComo podemos ver, el desv\u00edo del modelo es peque\u00f1o, por lo que sus resultados para este ejemplo son bastante confiables.\n\n\nRegresi\u00f3n Log\u00edstica\n\n\nLos modelos lineales, tambi\u00e9n pueden ser utilizados para clasificaciones; es decir, que primero ajustamos el modelo lineal a la probabilidad de que una cierta clase o categor\u00eda ocurra y, a luego, utilizamos una funci\u00f3n para crear un umbral en el cual especificamos el resultado de una de estas clases o categor\u00edas. La funci\u00f3n que utiliza este modelo, no es ni m\u00e1s ni menos que la funci\u00f3n log\u00edstica.\n\n\n$$f(x) = \\frac{1}{1 + e^{-1}}$$\n\n\nVeamos, aqu\u00ed tambi\u00e9n un peque\u00f1o ejemplo en \nPython\n.\n\n\n# Creando un dataset de ejemplo \nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1000, n_features=4)\n\n# Importando el modelo\nfrom sklearn.linear_model import LogisticRegression\n\nrlog = LogisticRegression() # Creando el modelo\n\n# Dividiendo el dataset en entrenamiento y evaluacion\nX_entrenamiento = X[:-200]\nX_evaluacion = X[-200:]\ny_entrenamiento = y[:-200]\ny_evaluacion = y[-200:]\n\nrlog.fit(X_entrenamiento, y_entrenamiento) #ajustando el modelo\n\n# Realizando las predicciones\ny_predic_entrenamiento = rlog.predict(X_entrenamiento) \ny_predic_evaluacion = rlog.predict(X_evaluacion)\n\n# Verificando la exactitud del modelo\nentrenamiento = (y_predic_entrenamiento == y_entrenamiento).sum().astype(float) / y_entrenamiento.shape[0]\nprint(\"sobre datos de entrenamiento: {0:.2f}\".format(entrenamiento))\nevaluacion = (y_predic_evaluacion == y_evaluacion).sum().astype(float) / y_evaluacion.shape[0]\nprint(\"sobre datos de evaluaci\u00f3n: {0:.2f}\".format(evaluacion))\n\nsobre datos de entrenamiento: 0.95\nsobre datos de evaluaci\u00f3n: 0.94\n\n\n\n\nComo podemos ver en este ejemplo tambi\u00e9n nuestro modelo tiene bastante precisi\u00f3n clasificando las categor\u00edas de nuestro dataset.\n\n\nArboles de decisi\u00f3n\n\n\nLos \nArboles de Decision\n son diagramas con construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema.\nLos \nArboles de Decision\n est\u00e1n compuestos por nodos interiores, nodos terminales y ramas que emanan de los nodos interiores. Cada nodo interior en el \u00e1rbol contiene una prueba de un atributo, y cada rama representa un valor distinto del atributo. Siguiendo las ramas desde el nodo ra\u00edz hacia abajo, cada ruta finalmente termina en un nodo terminal creando una segmentaci\u00f3n de los datos. Veamos aqu\u00ed tambi\u00e9n un peque\u00f1o ejemplo en \nPython\n.\n\n\n# Creando un dataset de ejemplo\nX, y = datasets.make_classification(1000, 20, n_informative=3)\n\n# Importando el arbol de decisi\u00f3n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nad = DecisionTreeClassifier(criterion='entropy', max_depth=5) # Creando el modelo\nad.fit(X, y) # Ajustando el modelo\n\n#generando archivo para graficar el arbol\nwith open(\"mi_arbol.dot\", 'w') as archivo_dot:\n    tree.export_graphviz(ad, out_file = archivo_dot)\n\n# utilizando el lenguaje dot para graficar el arbol.\n!dot -Tjpeg mi_arbol.dot -o arbol_decision.jpeg\n\n\n\n\nLuego de usar el lenguaje \ndot\n para convertir nuestro arbol a formato jpeg, ya podemos ver la imagen del mismo.\n\n\n\n\n# verificando la precisi\u00f3n\nprint(\"precisi\u00f3n del modelo: {0: .2f}\".format((y == ad.predict(X)).mean()))\n\nprecisi\u00f3n del modelo:  0.94\n\n\n\n\nEn este ejemplo, nuestro \u00e1rbol tiene una precisi\u00f3n del 89%. Tener en cuenta que los \nArboles de Decision\n tienen tendencia al \nsobreajuste\n.\n\n\nRandom Forest\n\n\nEn lugar de utilizar solo un arbol para decidir, \u00bfpor qu\u00e9 no utilizar todo un bosque?!!. Esta es la idea central detr\u00e1s del \nalgoritmo\n de \nRandom Forest\n. Trabaja construyendo una gran cantidad de \narboles de decision\n muy poco profundos, y luego toma la clase que\ncada \u00e1rbol eligi\u00f3. Esta idea es muy poderosa en \nMachine Learning\n. Si tenemos en cuenta que un sencillo clasificador entrenado podr\u00eda tener s\u00f3lo el 60 por ciento de precisi\u00f3n, podemos entrenar un mont\u00f3n de clasificadores que sean por lo general acertados y luego podemos utilizar la sabidur\u00eda de todos los aprendices juntos.\nCon \nPython\n los podemos utilizar de la siguiente manera:\n\n\n# Creando un dataset de ejemplo\nX, y = datasets.make_classification(1000)\n\n# Importando el random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier() # Creando el modelo\nrf.fit(X, y) # Ajustando el modelo\n\n# verificando la precisi\u00f3n\nprint(\"precisi\u00f3n del modelo: {0: .2f}\".format((y == rf.predict(X)).mean()))\n\nprecisi\u00f3n del modelo:  1.00\n\n\n\n\nSVM o M\u00e1quinas de vectores de soporte\n\n\nLa idea detr\u00e1s de \nSVM\n es encontrar un plano que separe los grupos dentro de los datos de la mejor forma posible. Aqu\u00ed, la separaci\u00f3n significa que la elecci\u00f3n\ndel plano maximiza el margen entre los puntos m\u00e1s cercanos en el plano; \u00e9stos puntos se denominan vectores de soporte. Pasemos al ejemplo.\n\n\n# importando SVM\nfrom sklearn import svm\n\n# importando el dataset iris\niris = datasets.load_iris()\nX = iris.data[:, :2]  # solo tomamos las primeras 2 caracter\u00edsticas\ny = iris.target\n\nh = .02  # tama\u00f1o de la malla del grafico\n\n# Creando el SVM con sus diferentes m\u00e9todos\nC = 1.0  # parametro de regulacion SVM \nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\nlin_svc = svm.LinearSVC(C=C).fit(X, y)\n\n# crear el area para graficar\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# titulos de los graficos\ntitles = ['SVC con el motor lineal',\n          'LinearSVC',\n          'SVC con el motor RBF',\n          'SVC con el motor polinomial']\n\n\nfor i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n    # Realizando el gr\u00e1fico, se le asigna un color a cada punto\n    plt.subplot(2, 2, i + 1)\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n    # Graficando tambien los puntos de datos\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('largo del petalo')\n    plt.ylabel('ancho del petalo')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(titles[i])\n\nplt.show()\n\n\n\n\n\n\nKNN o k vecinos m\u00e1s cercanos\n\n\nEste es un m\u00e9todo de clasificaci\u00f3n no param\u00e9trico, que estima el valor de la probabilidad a posteriori de que un elemento $x$ pertenezca a una clase en particular a partir de la informaci\u00f3n proporcionada por el conjunto de prototipos.\nLa regresi\u00f3n \nKNN\n se calcula simplemente tomando el promedio del punto k m\u00e1s cercano al punto que se est\u00e1 probando. \n\n\n\n# Creando el dataset iris\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# importando KNN \nfrom sklearn.neighbors import KNeighborsRegressor\n\nknnr = KNeighborsRegressor(n_neighbors=10) # Creando el modelo con 10 vecinos\nknnr.fit(X, y) # Ajustando el modelo\n\n# Verificando el error medio del modelo\nprint(\"El error medio del modelo es: {:.2f}\".format(np.power(y - knnr.predict(X),\n2).mean()))\n\nEl error medio del modelo es: 0.02\n\n\n\n\nK-means\n\n\nK-means\n es probablemente uno de los algoritmos de agrupamiento m\u00e1s conocidos y, en un sentido m\u00e1s amplio, una de las t\u00e9cnicas de aprendizaje no supervisado m\u00e1s conocidas.\n\nK-means\n es en realidad un \nalgoritmo\n muy simple que funciona para reducir al m\u00ednimo la suma de las distancias cuadradas desde la media dentro del agrupamiento. Para hacer esto establece primero un n\u00famero previamente especificado de conglomerados, K, y luego va asignando cada observaci\u00f3n a la agrupaci\u00f3n m\u00e1s cercana de acuerdo a su media. Veamos el ejemplo\n\n\n# Creando el dataset\ngrupos, pos_correcta = datasets.make_blobs(1000, centers=3,\ncluster_std=1.75)\n\n# Graficando los grupos de datos\nf, ax = plt.subplots(figsize=(7, 5))\ncolores = ['r', 'g', 'b']\n\nfor i in range(3):\n    p = grupos[pos_correcta == i]\n    ax.scatter(p[:,0], p[:,1], c=colores[i],\n               label=\"Grupo {}\".format(i))\n\nax.set_title(\"Agrupamiento perfecto\")\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n# importando KMeans\nfrom sklearn.cluster import KMeans\n\n# Creando el modelo\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(grupos) # Ajustando el modelo\n\n# verificando los centros de los grupos\nkmeans.cluster_centers_\n\n# Graficando segun modelo\nf, ax = plt.subplots(figsize=(7, 5))\ncolores = ['r', 'g', 'b']\n\nfor i in range(3):\n    p = grupos[pos_correcta == i]\n    ax.scatter(p[:,0], p[:,1], c=colores[i],\n               label=\"Grupo {}\".format(i))\n\nax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n           s=100, color='black', label='Centros')\n\nax.set_title(\"Agrupamiento s/modelo\")\nax.legend()\n\nplt.show()",
            "title": "Machine Learning"
        },
        {
            "location": "/machine-learning/#introduccion-al-machine-learning",
            "text": "Una de las ramas de estudio que cada vez esta ganando m\u00e1s popularidad dentro de las  ciencias de la computaci\u00f3n  es el  aprendizaje autom\u00e1tico  o  Machine Learning . Muchos de los servicios que utilizamos en nuestro d\u00eda a d\u00eda como google, gmail, netflix, spotify o amazon se valen de las herramientas que les brinda el  Machine Learning  para alcanzar un servicio cada vez m\u00e1s personalizado y lograr as\u00ed ventajas competitivas sobre sus rivales.",
            "title": "Introducci\u00f3n al Machine Learning"
        },
        {
            "location": "/machine-learning/#que-es-machine-learning",
            "text": "Pero, \u00bfqu\u00e9 es exactamente  Machine Learning ?. El  Machine Learning  es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprender de los datos. El objetivo fundamental del  Machine Learning  es  generalizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada . El ejemplo m\u00e1s t\u00edpico donde podemos ver el uso del  Machine Learning  es en el filtrado de los correo basura o spam. Mediante la observaci\u00f3n de miles de correos electr\u00f3nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos. El  Machine Learning  combina conceptos y t\u00e9cnicas de diferentes \u00e1reas del conocimiento, como las  matem\u00e1ticas ,  estad\u00edsticas  y las  ciencias de la computaci\u00f3n ; por tal motivo, hay muchas maneras de aprender la disciplina.",
            "title": "\u00bfQu\u00e9 es Machine Learning?"
        },
        {
            "location": "/machine-learning/#tipos-de-machine-learning",
            "text": "El  Machine Learning  tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito, an\u00e1lisis del mercado de valores, clasificaci\u00f3n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob\u00f3tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distinguir los distintos tipos de problemas de  Machine Learning  con los que nos podemos encontrar.",
            "title": "Tipos de Machine Learning"
        },
        {
            "location": "/machine-learning/#aprendizaje-supervisado",
            "text": "En los problemas de  aprendizaje supervisado  se ense\u00f1a o entrena al  algoritmo  a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos m\u00e1s el  algoritmo  puede aprender sobre el tema. Una vez concluido el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el  algoritmo  de aprendizaje utiliza la experiencia pasada que adquiri\u00f3 durante la etapa de entrenamiento para predecir un resultado. Esto es similar al m\u00e9todo de aprendizaje que se utiliza en las escuelas, donde se nos ense\u00f1an problemas y las formas de resolverlos, para que luego podamos aplicar los mismos m\u00e9todos en situaciones similares.",
            "title": "Aprendizaje supervisado"
        },
        {
            "location": "/machine-learning/#aprendizaje-no-supervisado",
            "text": "En los problemas de  aprendizaje no supervisado  el  algoritmo  es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al  algoritmo  lo que representan los datos. La idea es que el  algoritmo  pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos. El  aprendizaje no supervisado  es similar al m\u00e9todo que utilizamos para aprender a hablar cuando somos bebes, en un principio escuchamos hablar a nuestros padres y no entendemos nada; pero a medida que vamos escuchando miles de conversaciones, nuestro cerebro comenzar\u00e1 a formar un modelo sobre c\u00f3mo funciona el lenguaje y comenzaremos a reconocer patrones y a esperar ciertos sonidos.",
            "title": "Aprendizaje no supervisado"
        },
        {
            "location": "/machine-learning/#aprendizaje-por-refuerzo",
            "text": "En los problemas de aprendizaje por refuerzo, el  algoritmo  aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m\u00e1s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar\u00e1 creando una estrategia ganadora.",
            "title": "Aprendizaje por refuerzo"
        },
        {
            "location": "/machine-learning/#sobreajuste",
            "text": "Como mencionamos cuando definimos al  Machine Learning , la idea fundamental es encontrar patrones que podamos generalizar para luego poder aplicar esta generalizaci\u00f3n sobre los casos que todav\u00eda no hemos observado y realizar predicciones. Pero tambi\u00e9n puede ocurrir que durante el entrenamiento solo descubramos casualidades en los datos que se parecen a patrones interesantes, pero que no generalicen. Esto es lo que se conoce con el nombre de  sobreajuste  o sobreentrenamiento .  El  sobreajuste  es la tendencia que tienen la mayor\u00eda de los  algoritmos  de  Machine Learning  a ajustarse a unas caracter\u00edsticas muy espec\u00edficas de los datos de entrenamiento que no tienen relaci\u00f3n causal con la  funci\u00f3n objetivo  que estamos buscando para generalizar. El ejemplo m\u00e1s extremo de un modelo  sobreajustado  es un modelo que solo memoriza las respuestas correctas; este modelo al ser utilizado con datos que nunca antes ha visto va a tener un rendimiento azaroso, ya que nunca logr\u00f3 generalizar un patr\u00f3n para predecir.",
            "title": "Sobreajuste"
        },
        {
            "location": "/machine-learning/#como-evitar-el-sobreajuste",
            "text": "Como mencionamos anteriormente, todos los modelos de  Machine Learning  tienen tendencia al  sobreajuste ; es por esto que debemos aprender a convivir con el mismo y tratar de tomar medidas preventivas para reducirlo lo m\u00e1s posible. Las dos principales estrategias para lidiar son el  sobreajuste  son: la  retenci\u00f3n de datos  y la  validaci\u00f3n cruzada .  En el primer caso, la idea es dividir nuestro  conjunto de datos , en uno o varios conjuntos de entrenamiento y otro/s conjuntos de evaluaci\u00f3n. Es decir, que no le vamos a pasar todos nuestros datos al  algoritmo  durante el entrenamiento, sino que vamos a  retener  una parte de los datos de entrenamiento para realizar una evaluaci\u00f3n de la efectividad del modelo. Con esto lo que buscamos es evitar que los mismos datos que usamos para entrenar sean los mismos que utilizamos para evaluar. De esta forma vamos a poder analizar con m\u00e1s precisi\u00f3n como el modelo se va comportando a medida que m\u00e1s lo vamos entrenando y poder detectar el punto cr\u00edtico en el que el modelo deja de generalizar y comienza a  sobreajustarse  a los datos de entrenamiento.  La  validaci\u00f3n cruzada  es un procedimiento m\u00e1s sofisticado que el anterior. En lugar de solo obtener una simple estimaci\u00f3n de la efectividad de la  generalizaci\u00f3n ; la idea es realizar un an\u00e1lisis estad\u00edstico para obtener otras medidas del rendimiento estimado, como la media y la varianza, y as\u00ed poder entender c\u00f3mo se espera que el rendimiento var\u00ede a trav\u00e9s de los distintos conjuntos de datos. Esta variaci\u00f3n es fundamental para la evaluaci\u00f3n de la confianza en la estimaci\u00f3n del rendimiento.\nLa  validaci\u00f3n cruzada  tambi\u00e9n hace un mejor uso de un conjunto de datos limitado; ya que a diferencia de la simple divisi\u00f3n de los datos en uno el entrenamiento y otro de evaluaci\u00f3n; la  validaci\u00f3n cruzada  calcula sus estimaciones sobre todo el  conjunto de datos  mediante la realizaci\u00f3n de m\u00faltiples divisiones e intercambios sistem\u00e1ticos entre datos de entrenamiento y datos de evaluaci\u00f3n.",
            "title": "Como evitar el sobreajuste"
        },
        {
            "location": "/machine-learning/#pasos-para-construir-un-modelo-de-machine-learning",
            "text": "Construir un modelo de  Machine Learning , no se reduce solo a utilizar un  algoritmo  de aprendizaje o utilizar una librer\u00eda de  Machine Learning ; sino que es todo un proceso que suele involucrar los siguientes pasos:    Recolectar los datos . Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una  API  o desde una base de datos. Podemos tambi\u00e9n utilizar otros dispositivos que recolectan los datos por nosotros; o utilizar datos que son de dominio p\u00fablico. El n\u00famero de opciones que tenemos para recolectar datos no tiene fin!. Este paso parece obvio, pero es uno de los que m\u00e1s complicaciones trae y m\u00e1s tiempo consume.     Preprocesar los datos . Una vez que tenemos los datos, tenemos que asegurarnos que tiene el formato correcto para nutrir nuestro  algoritmo  de aprendizaje. Es pr\u00e1cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m\u00e1s sencillo que el paso anterior.    Explorar los datos . Una vez que ya tenemos los datos y est\u00e1n con el formato correcto, podemos realizar un pre an\u00e1lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg\u00fan patr\u00f3n en los mismos que nos facilite la construcci\u00f3n del modelo. En esta etapa suelen ser de mucha utilidad las medidas estad\u00edsticas y los gr\u00e1ficos en 2 y 3 dimensiones para tener una idea visual de como se comportan nuestros datos. En este punto podemos detectar  valores at\u00edpicos  que debamos descartar; o encontrar las caracter\u00edsticas que m\u00e1s influencia tienen para realizar una predicci\u00f3n.    Entrenar el  algoritmo . Aqu\u00ed es donde comenzamos a utilizar las t\u00e9cnicas de  Machine Learning  realmente. En esta etapa nutrimos al o los  algoritmos  de aprendizaje con los datos que venimos procesando en las etapas anteriores. La idea es que los  algoritmos  puedan extraer informaci\u00f3n \u00fatil de los datos que le pasamos para luego poder hacer predicciones.     Evaluar el  algoritmo . En esta etapa ponemos a prueba la informaci\u00f3n o conocimiento que el  algoritmo  obtuvo del entrenamiento del paso anterior. Evaluamos que tan preciso es el algoritmo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el  algoritmo  cambiando algunos par\u00e1metros hasta lograr un rendimiento aceptable.      Utilizar el modelo . En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu\u00ed tambi\u00e9n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores.",
            "title": "Pasos para construir un modelo de machine learning"
        },
        {
            "location": "/machine-learning/#librerias-de-python-para-machine-learning",
            "text": "Como siempre me gusta comentar, una de las grandes ventajas que ofrece  Python  sobre otros lenguajes de programaci\u00f3n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer\u00edas de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de  Machine Learning , las principales librer\u00edas que podemos utilizar son:",
            "title": "Librer\u00edas de Python para machine learning"
        },
        {
            "location": "/machine-learning/#scikit-learn",
            "text": "Scikit-learn  es la principal librer\u00eda que existe para trabajar con  Machine Learning , incluye la implementaci\u00f3n de un gran n\u00famero de  algoritmos  de aprendizaje. La podemos utilizar para  clasificaciones ,  extraccion de caracter\u00edsticas ,  regresiones ,  agrupaciones ,  reducci\u00f3n de dimensiones ,  selecci\u00f3n de modelos , o  preprocesamiento . Posee una  API  que es consistente en todos los modelos y se integra muy bien con el resto de los paquetes cient\u00edficos que ofrece  Python . Esta librer\u00eda tambi\u00e9n nos facilita las tareas de evaluaci\u00f3n, diagnostico y  validaciones cruzadas  ya que nos proporciona varios m\u00e9todos de f\u00e1brica para poder realizar estas tareas en forma muy simple.",
            "title": "Scikit-Learn"
        },
        {
            "location": "/machine-learning/#statsmodels",
            "text": "Statsmodels  es otra gran librer\u00eda que hace foco en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios. Al igual que  Scikit-learn , tambi\u00e9n se integra muy bien con el resto de los paquetes cient\u00edficos de  Python . Si deseamos ajustar modelos lineales, hacer una an\u00e1lisis estad\u00edstico, o tal vez un poco de modelado predictivo, entonces  Statsmodels  es la librer\u00eda ideal. Las pruebas estad\u00edsticas que ofrece son bastante amplias y abarcan tareas de validaci\u00f3n para la mayor\u00eda de los casos.",
            "title": "Statsmodels"
        },
        {
            "location": "/machine-learning/#pymc",
            "text": "pyMC  es un m\u00f3dulo de  Python  que implementa modelos estad\u00edsticos bayesianos, incluyendo la  cadena de Markov Monte Carlo(MCMC) .  pyMC   ofrece funcionalidades para hacer el an\u00e1lisis bayesiano lo mas simple posible. Incluye los modelos  bayesianos ,  distribuciones estad\u00edsticas y herramientas de diagnostico  para la covarianza de los modelos. Si queremos realizar un an\u00e1lisis  bayesiano  esta es sin duda la librer\u00eda a utilizar.",
            "title": "PyMC"
        },
        {
            "location": "/machine-learning/#ntlk",
            "text": "NLTK  es la librer\u00eda l\u00edder para el procesamiento del lenguaje natural o  NLP  por sus siglas en ingl\u00e9s. Proporciona interfaces f\u00e1ciles de usar a m\u00e1s de 50 cuerpos y recursos l\u00e9xicos, como  WordNet , junto con un conjunto de bibliotecas de procesamiento de texto para la clasificaci\u00f3n, tokenizaci\u00f3n, el etiquetado, el an\u00e1lisis y el razonamiento sem\u00e1ntico.   Obviamente, aqu\u00ed solo estoy listando unas pocas de las muchas librer\u00edas que existen en  Python  para trabajar con problemas de  Machine Learning , los invito a realizar su propia investigaci\u00f3n sobre el tema.",
            "title": "NTLK"
        },
        {
            "location": "/machine-learning/#algoritmos-mas-utilizados",
            "text": "Los  algoritmos   que m\u00e1s se suelen utilizar en los problemas de  Machine Learning  son los siguientes:   Regresi\u00f3n Lineal  Regresi\u00f3n Log\u00edstica  Arboles de Decision  Random Forest  SVM  o M\u00e1quinas de vectores de soporte.  KNN  o K vecinos m\u00e1s cercanos.  K-means   Todos ellos se pueden aplicar a casi cualquier problema de datos y obviamente est\u00e1n todos implementados por la excelente librer\u00eda de  Python ,  Scikit-learn . Veamos algunos ejemplos de ellos.",
            "title": "Algoritmos m\u00e1s utilizados"
        },
        {
            "location": "/machine-learning/#regresion-lineal",
            "text": "Se utiliza para estimar los valores reales (costo de las viviendas, el n\u00famero de llamadas, ventas totales, etc.) basados en variables continuas. La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar una mejor l\u00ednea recta con respecto a los puntos. Esta l\u00ednea de mejor ajuste se conoce como l\u00ednea de regresi\u00f3n y esta representada por la siguiente ecuaci\u00f3n lineal:  $$Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{n}X_{n}$$  Veamos un peque\u00f1o ejemplo de como se implementa en  Python . En este ejemplo voy a utilizar el dataset Boston que ya viene junto con  Scikit-learn  y es ideal para practicar con  Regresiones Lineales ; el mismo contiene precios de casas de varias \u00e1reas de la ciudad de Boston.   # importando pandas, numpy y matplotlib\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# importando los datasets de sklearn\nfrom sklearn import datasets\n\nboston = datasets.load_boston()\nboston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\nboston_df['TARGET'] = boston.target\nboston_df.head() # estructura de nuestro dataset.  \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       TARGET \n     \n   \n   \n     \n       0 \n       0.00632 \n       18.0 \n       2.31 \n       0.0 \n       0.538 \n       6.575 \n       65.2 \n       4.0900 \n       1.0 \n       296.0 \n       15.3 \n       396.90 \n       4.98 \n       24.0 \n     \n     \n       1 \n       0.02731 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       6.421 \n       78.9 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       396.90 \n       9.14 \n       21.6 \n     \n     \n       2 \n       0.02729 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       7.185 \n       61.1 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       392.83 \n       4.03 \n       34.7 \n     \n     \n       3 \n       0.03237 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       6.998 \n       45.8 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       394.63 \n       2.94 \n       33.4 \n     \n     \n       4 \n       0.06905 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       7.147 \n       54.2 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       396.90 \n       5.33 \n       36.2 \n     \n     # importando el modelo de regresi\u00f3n lineal\nfrom sklearn.linear_model import LinearRegression\n\nrl = LinearRegression() # Creando el modelo.\nrl.fit(boston.data, boston.target) # ajustando el modelo\n\n# haciendo las predicciones\npredicciones = rl.predict(boston.data)\npredicciones_df = pd.DataFrame(predicciones, columns=['Pred'])\npredicciones_df.head() # predicciones de las primeras 5 lineas  \n   \n     \n       \n       Pred \n     \n   \n   \n     \n       0 \n       30.008213 \n     \n     \n       1 \n       25.029861 \n     \n     \n       2 \n       30.570232 \n     \n     \n       3 \n       28.608141 \n     \n     \n       4 \n       27.942882 \n     \n     # Calculando el desvio\nnp.mean(boston.target - predicciones)  Como podemos ver, el desv\u00edo del modelo es peque\u00f1o, por lo que sus resultados para este ejemplo son bastante confiables.",
            "title": "Regresi\u00f3n Lineal"
        },
        {
            "location": "/machine-learning/#regresion-logistica",
            "text": "Los modelos lineales, tambi\u00e9n pueden ser utilizados para clasificaciones; es decir, que primero ajustamos el modelo lineal a la probabilidad de que una cierta clase o categor\u00eda ocurra y, a luego, utilizamos una funci\u00f3n para crear un umbral en el cual especificamos el resultado de una de estas clases o categor\u00edas. La funci\u00f3n que utiliza este modelo, no es ni m\u00e1s ni menos que la funci\u00f3n log\u00edstica.  $$f(x) = \\frac{1}{1 + e^{-1}}$$  Veamos, aqu\u00ed tambi\u00e9n un peque\u00f1o ejemplo en  Python .  # Creando un dataset de ejemplo \nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1000, n_features=4)\n\n# Importando el modelo\nfrom sklearn.linear_model import LogisticRegression\n\nrlog = LogisticRegression() # Creando el modelo\n\n# Dividiendo el dataset en entrenamiento y evaluacion\nX_entrenamiento = X[:-200]\nX_evaluacion = X[-200:]\ny_entrenamiento = y[:-200]\ny_evaluacion = y[-200:]\n\nrlog.fit(X_entrenamiento, y_entrenamiento) #ajustando el modelo\n\n# Realizando las predicciones\ny_predic_entrenamiento = rlog.predict(X_entrenamiento) \ny_predic_evaluacion = rlog.predict(X_evaluacion)\n\n# Verificando la exactitud del modelo\nentrenamiento = (y_predic_entrenamiento == y_entrenamiento).sum().astype(float) / y_entrenamiento.shape[0]\nprint(\"sobre datos de entrenamiento: {0:.2f}\".format(entrenamiento))\nevaluacion = (y_predic_evaluacion == y_evaluacion).sum().astype(float) / y_evaluacion.shape[0]\nprint(\"sobre datos de evaluaci\u00f3n: {0:.2f}\".format(evaluacion))\n\nsobre datos de entrenamiento: 0.95\nsobre datos de evaluaci\u00f3n: 0.94  Como podemos ver en este ejemplo tambi\u00e9n nuestro modelo tiene bastante precisi\u00f3n clasificando las categor\u00edas de nuestro dataset.",
            "title": "Regresi\u00f3n Log\u00edstica"
        },
        {
            "location": "/machine-learning/#arboles-de-decision",
            "text": "Los  Arboles de Decision  son diagramas con construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema.\nLos  Arboles de Decision  est\u00e1n compuestos por nodos interiores, nodos terminales y ramas que emanan de los nodos interiores. Cada nodo interior en el \u00e1rbol contiene una prueba de un atributo, y cada rama representa un valor distinto del atributo. Siguiendo las ramas desde el nodo ra\u00edz hacia abajo, cada ruta finalmente termina en un nodo terminal creando una segmentaci\u00f3n de los datos. Veamos aqu\u00ed tambi\u00e9n un peque\u00f1o ejemplo en  Python .  # Creando un dataset de ejemplo\nX, y = datasets.make_classification(1000, 20, n_informative=3)\n\n# Importando el arbol de decisi\u00f3n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nad = DecisionTreeClassifier(criterion='entropy', max_depth=5) # Creando el modelo\nad.fit(X, y) # Ajustando el modelo\n\n#generando archivo para graficar el arbol\nwith open(\"mi_arbol.dot\", 'w') as archivo_dot:\n    tree.export_graphviz(ad, out_file = archivo_dot)\n\n# utilizando el lenguaje dot para graficar el arbol.\n!dot -Tjpeg mi_arbol.dot -o arbol_decision.jpeg  Luego de usar el lenguaje  dot  para convertir nuestro arbol a formato jpeg, ya podemos ver la imagen del mismo.   # verificando la precisi\u00f3n\nprint(\"precisi\u00f3n del modelo: {0: .2f}\".format((y == ad.predict(X)).mean()))\n\nprecisi\u00f3n del modelo:  0.94  En este ejemplo, nuestro \u00e1rbol tiene una precisi\u00f3n del 89%. Tener en cuenta que los  Arboles de Decision  tienen tendencia al  sobreajuste .",
            "title": "Arboles de decisi\u00f3n"
        },
        {
            "location": "/machine-learning/#random-forest",
            "text": "En lugar de utilizar solo un arbol para decidir, \u00bfpor qu\u00e9 no utilizar todo un bosque?!!. Esta es la idea central detr\u00e1s del  algoritmo  de  Random Forest . Trabaja construyendo una gran cantidad de  arboles de decision  muy poco profundos, y luego toma la clase que\ncada \u00e1rbol eligi\u00f3. Esta idea es muy poderosa en  Machine Learning . Si tenemos en cuenta que un sencillo clasificador entrenado podr\u00eda tener s\u00f3lo el 60 por ciento de precisi\u00f3n, podemos entrenar un mont\u00f3n de clasificadores que sean por lo general acertados y luego podemos utilizar la sabidur\u00eda de todos los aprendices juntos.\nCon  Python  los podemos utilizar de la siguiente manera:  # Creando un dataset de ejemplo\nX, y = datasets.make_classification(1000)\n\n# Importando el random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier() # Creando el modelo\nrf.fit(X, y) # Ajustando el modelo\n\n# verificando la precisi\u00f3n\nprint(\"precisi\u00f3n del modelo: {0: .2f}\".format((y == rf.predict(X)).mean()))\n\nprecisi\u00f3n del modelo:  1.00",
            "title": "Random Forest"
        },
        {
            "location": "/machine-learning/#svm-o-maquinas-de-vectores-de-soporte",
            "text": "La idea detr\u00e1s de  SVM  es encontrar un plano que separe los grupos dentro de los datos de la mejor forma posible. Aqu\u00ed, la separaci\u00f3n significa que la elecci\u00f3n\ndel plano maximiza el margen entre los puntos m\u00e1s cercanos en el plano; \u00e9stos puntos se denominan vectores de soporte. Pasemos al ejemplo.  # importando SVM\nfrom sklearn import svm\n\n# importando el dataset iris\niris = datasets.load_iris()\nX = iris.data[:, :2]  # solo tomamos las primeras 2 caracter\u00edsticas\ny = iris.target\n\nh = .02  # tama\u00f1o de la malla del grafico\n\n# Creando el SVM con sus diferentes m\u00e9todos\nC = 1.0  # parametro de regulacion SVM \nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\nlin_svc = svm.LinearSVC(C=C).fit(X, y)\n\n# crear el area para graficar\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# titulos de los graficos\ntitles = ['SVC con el motor lineal',\n          'LinearSVC',\n          'SVC con el motor RBF',\n          'SVC con el motor polinomial']\n\n\nfor i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n    # Realizando el gr\u00e1fico, se le asigna un color a cada punto\n    plt.subplot(2, 2, i + 1)\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n    # Graficando tambien los puntos de datos\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('largo del petalo')\n    plt.ylabel('ancho del petalo')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(titles[i])\n\nplt.show()",
            "title": "SVM o M\u00e1quinas de vectores de soporte"
        },
        {
            "location": "/machine-learning/#knn-o-k-vecinos-mas-cercanos",
            "text": "Este es un m\u00e9todo de clasificaci\u00f3n no param\u00e9trico, que estima el valor de la probabilidad a posteriori de que un elemento $x$ pertenezca a una clase en particular a partir de la informaci\u00f3n proporcionada por el conjunto de prototipos.\nLa regresi\u00f3n  KNN  se calcula simplemente tomando el promedio del punto k m\u00e1s cercano al punto que se est\u00e1 probando.   \n# Creando el dataset iris\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# importando KNN \nfrom sklearn.neighbors import KNeighborsRegressor\n\nknnr = KNeighborsRegressor(n_neighbors=10) # Creando el modelo con 10 vecinos\nknnr.fit(X, y) # Ajustando el modelo\n\n# Verificando el error medio del modelo\nprint(\"El error medio del modelo es: {:.2f}\".format(np.power(y - knnr.predict(X),\n2).mean()))\n\nEl error medio del modelo es: 0.02",
            "title": "KNN o k vecinos m\u00e1s cercanos"
        },
        {
            "location": "/machine-learning/#k-means",
            "text": "K-means  es probablemente uno de los algoritmos de agrupamiento m\u00e1s conocidos y, en un sentido m\u00e1s amplio, una de las t\u00e9cnicas de aprendizaje no supervisado m\u00e1s conocidas. K-means  es en realidad un  algoritmo  muy simple que funciona para reducir al m\u00ednimo la suma de las distancias cuadradas desde la media dentro del agrupamiento. Para hacer esto establece primero un n\u00famero previamente especificado de conglomerados, K, y luego va asignando cada observaci\u00f3n a la agrupaci\u00f3n m\u00e1s cercana de acuerdo a su media. Veamos el ejemplo  # Creando el dataset\ngrupos, pos_correcta = datasets.make_blobs(1000, centers=3,\ncluster_std=1.75)\n\n# Graficando los grupos de datos\nf, ax = plt.subplots(figsize=(7, 5))\ncolores = ['r', 'g', 'b']\n\nfor i in range(3):\n    p = grupos[pos_correcta == i]\n    ax.scatter(p[:,0], p[:,1], c=colores[i],\n               label=\"Grupo {}\".format(i))\n\nax.set_title(\"Agrupamiento perfecto\")\nax.legend()\n\nplt.show()   # importando KMeans\nfrom sklearn.cluster import KMeans\n\n# Creando el modelo\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(grupos) # Ajustando el modelo\n\n# verificando los centros de los grupos\nkmeans.cluster_centers_\n\n# Graficando segun modelo\nf, ax = plt.subplots(figsize=(7, 5))\ncolores = ['r', 'g', 'b']\n\nfor i in range(3):\n    p = grupos[pos_correcta == i]\n    ax.scatter(p[:,0], p[:,1], c=colores[i],\n               label=\"Grupo {}\".format(i))\n\nax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n           s=100, color='black', label='Centros')\n\nax.set_title(\"Agrupamiento s/modelo\")\nax.legend()\n\nplt.show()",
            "title": "K-means"
        },
        {
            "location": "/datascience/",
            "text": "Ciencia de datos\n\n\n\n\nEn el mundo actual de la tecnolog\u00eda que nos rodea, donde la \ncomputaci\u00f3n en la nube\n se va haciendo parte de nuestro d\u00eda a d\u00eda (quien no usa los servicios de \nGoogle\n, \nFacebook\n, \nTwitter\n, \nDropbox\n, o \nEvernote\n); d\u00f3nde hay cada vez una mayor cantidad de dispositivos que est\u00e1n las 24 horas del d\u00eda conectadas a internet (desde tel\u00e9fonos, tabletas y TVs hasta autom\u00f3viles), acerc\u00e1ndonos a\u00fan m\u00e1s al concepto de la \nInternet de las cosas\n. En este mundo d\u00f3nde estamos generando datos constantemente, en el mundo de la \nBig Data\n; se esta haciendo cada vez m\u00e1s necesario un nuevo perfil de profesionales de la informaci\u00f3n que puedan aplicar las t\u00e9cnicas de la \nCiencia de Datos\n.\n\n\n\u00bfQu\u00e9 es la Ciencia de Datos?\n\n\nLa \nCiencia de Datos\n es un campo interdisciplinario que involucra m\u00e9todos cient\u00edficos, procesos y sistemas para extraer conocimiento o un mejor entendimiento de datos en sus diferentes formas, ya sea estructurados o no estructurados. Es una continuaci\u00f3n de algunos campos de \nan\u00e1lisis de datos\n como la \nestad\u00edstica\n, la \nminer\u00eda de datos\n, el \naprendizaje autom\u00e1tico\n y el \nan\u00e1lisis predictivo\n. Comprende tres \u00e1reas distintas y superpuestas: las habilidades de un estad\u00edstico que sabe c\u00f3mo modelar y resumir conjuntos de datos (los cuales cada vez tienen mayor tama\u00f1o); las habilidades de un inform\u00e1tico que pueda dise\u00f1ar y utilizar algoritmos para almacenar, procesar y visualizar eficientemente estos datos; Y la experiencia sobre el \ncampo\n o \ndominio\n, lo que podr\u00edamos pensar como una formaci\u00f3n \ncl\u00e1sica\n en un tema; la cual es necesaria tanto para formular las preguntas correctas como para poner sus respuestas en contexto. \n\n\nEl proceso de la Ciencia de Datos\n\n\nEn general, el proceso que utiliza la \nCiencia de Datos\n para  explorar el mundo usando datos es el siguiente:\n\n\n\n\n\n\nEl primer paso consiste en establecer un \nobjetivo de investigaci\u00f3n\n. El prop\u00f3sito principal aqu\u00ed es\nasegurarse de que todos los interesados comprendan el \nqu\u00e9, c\u00f3mo y por qu\u00e9\n del proyecto. Siempre debemos tener bien en claro cual es la pregunta que queremos responder con la ayuda de los datos.\n\n\n\n\n\n\nEl segundo paso consiste en la \nobtenci\u00f3n de los datos\n. Los datos deben estar disponibles para poder ser analizados. Este paso incluye encontrar los datos adecuados y obtener acceso a los mismos. El resultado de esta etapa suelen ser los datos en su forma cruda, que probablemente necesitar\u00e1n ser pulidos y transformados antes de que puedan ser utilizados.\n\n\n\n\n\n\nAhora que ya tenemos los datos sin procesar, el siguiente paso es \nprepararlos\n. Esto incluye la transformaci\u00f3n de los datos de una forma cruda a una forma en la que puedan ser utilizados directamente en los modelos. Para poder lograr esto, debemos detectar y corregir diferentes tipos de errores en los datos, combinar datos de diferentes fuentes y transformarlos. Una vez completado este paso, podemos avanzar hacia la visualizaci\u00f3n de datos y el modelado.\n\n\n\n\n\n\nEl cuarto paso es la \nexploraci\u00f3n de datos\n. El objetivo de este etapa es obtener una comprensi\u00f3n profunda de los datos. Buscaremos patrones, correlaciones y desv\u00edos basados en t\u00e9cnicas visuales y descriptivas. Los conocimientos adquiridos en esta fase nos permitir\u00e1n comenzar con el armado del modelo.\n\n\n\n\n\n\nFinalmente llegamos al paso principal y m\u00e1s importante: la \nconstrucci\u00f3n de modelos\n. En esta etapa intentamos obtener los conocimiento o hacer las predicciones de acuerdo a los lineamientos establecidos en la primer etapa. Aqu\u00ed podemos utilizar todas las t\u00e9cnicas y herramientas que nos proporciona el \nMachine Learning\n. El objetivo es obtener el modelo o la combinaci\u00f3n de modelos que mejor resultados nos proporcionen.\n\n\n\n\n\n\nEl \u00faltimo paso del proceso de la \nCiencia de Datos\n es \npresentar los resultados y automatizar an\u00e1lisis\n. Un buen modelo no sirve de nada si no es utilizado para mejorar la eficiencia y obtener mejores resultados. En esta \u00faltima etapa debemos presentarle los resultados del an\u00e1lisis a las personas responsables de tomar las decisiones en las organizaciones para que los modelos puedan ser adoptados.\n\n\n\n\n\n\nEn general, estas etapas no siguen una progresi\u00f3n lineal desde el paso 1 al 6. Si no que, a menudo, debemos regresar e iterar entre las diferentes etapas de acuerdo a los resultados que vayamos obteniendo. Actualmente, a los profesionales que se dedican a esta disciplina, se los conoce como \nCient\u00edficos de datos\n\n\nCient\u00edfico de datos\n\n\nLos \nData Scientists\n o \nCient\u00edficos de datos\n  son profesionales, generalmente con conocimientos multidisciplinarios, que poseen el entrenamiento y la curiosidad necesarias para realizar descubrimientos en el intrincado mundo de la \nBig Data\n. Ellos son capaces de darle forma a la enorme cantidad de datos desestructurados que generamos d\u00eda a d\u00eda y hacer su an\u00e1lisis posible. Se encargan de identificar potenciales fuentes de informaci\u00f3n, unirlas y depurar el conjunto de resultados; los \nCient\u00edficos de datos\n ayudan a los encargados de tomar las decisiones a moverse de un an\u00e1lisis \nad hoc\n de los datos hacia una constante conversaci\u00f3n con ellos.\n\n\nLos \nCient\u00edficos de datos\n se encargan de encontrar patrones en los datos, hacer descubrimientos en base a ellos, y comunicar las implicaciones de lo que han aprendido a trav\u00e9s de su an\u00e1lisis, para indicar nuevas oportunidades de negocios. Ellos aconsejan a los ejecutivos y gerentes de productos sobre las implicaciones de los datos para los productos, procesos y decisiones.\n\n\nSi bien, una primera impresi\u00f3n, se imaginar\u00eda a los \nCient\u00edficos de datos\n como personas con un fuerte perfil anal\u00edtico y mucho conocimiento estad\u00edstico y matem\u00e1tico, esta impresi\u00f3n estar\u00eda por dem\u00e1s errada. Ellos se caracterizan m\u00e1s por su parte cient\u00edfica; una de las facetas dominantes de su personalidad es su intensa curiosidad, el deseo por ir m\u00e1s all\u00e1 de la superficie de los problemas, encontrar las preguntas en lo m\u00e1s profundo de ellos, e ir depur\u00e1ndolas hasta crear un claro conjunto de hip\u00f3tesis que puedan ser probadas con datos concretos. Es por esto, que algunos de los m\u00e1s renombrados \nCient\u00edficos de datos\n en las principales empresas de tecnolog\u00eda del mundo, vienen de campos poco convencionales como la \nF\u00edsica\n y las \nCiencias Sociales\n.\n\n\nLo que motiva a los \nCient\u00edficos de datos\n no es armar hermosos reportes con informaci\u00f3n estructurada, para eso ya existen los analistas financieros; lo que realmente motiva a los \nCient\u00edficos de datos\n es crear nuevas cosas, no solo dar consejo; ellos quieren crear soluciones que funcionen y generen un impacto innovador para el negocio y los consumidores.\n\n\nUna podr\u00eda pensar a los \nCient\u00edficos de datos\n como un h\u00edbrido entre hacker, analista, comunicador y consejero; personas que tengan el conocimiento t\u00e9cnico necesario para manejar y analizar grandes cantidades de datos, pero que a su vez tengan la suficiente noci\u00f3n y entendimiento de los negocios y la habilidad para comunicar los datos de una forma efectiva. Una combinaci\u00f3n realmente rara de darse, pero sumamente efectiva!.\n\n\nEn lo que hace al apartado t\u00e9cnico, una de las habilidades b\u00e1sicas que todo buen \nCient\u00edficos de datos\n deber\u00eda tener, es sin duda la habilidad de escribir c\u00f3digo, programar. Un buen \nCient\u00edficos de datos\n deber\u00eda ser eficiente con al menos un lenguaje de programaci\u00f3n de alto rendimiento (como \nC\n, \nC++\n o \nJava\n) y tener nociones sobre los principales lenguajes que se manejan en internet (\nHTML\n, \nCSS3\n, \nJavascript\n, \nPHP\n).\n\n\nTambi\u00e9n deber\u00eda poseer buenos conocimientos sobre \nprobabilidad y estad\u00edstica\n, aqu\u00ed lenguajes de programaci\u00f3n con \nR\n y \nPython\n, pueden resultar realmente \u00fatiles.\n\n\nY finalmente, deber\u00eda poseer conocimientos sobre los principales  frameworks para el manejo de la \nBig Data\n, como por ejemplo \nHadoop\n; conocimientos sobre la infraestructura de la  \ncomputaci\u00f3n en la nube\n; y sobre las principales \nbases de datos\n, tanto \nSQL\n como \nNoSQL\n.\n\n\nLos siguientes son ejemplos del trabajo realizado por los \nCient\u00edficos de datos\n:\n\n\n\n\nEvaluaci\u00f3n de modelos estad\u00edsticos para determinar la validez de los an\u00e1lisis.\n\n\nUtilizar el \naprendizaje autom\u00e1tico\n para construir mejores algoritmos predictivos.\n\n\nPruebas y mejora continua de la precisi\u00f3n de los modelos de \naprendizaje autom\u00e1tico\n.\n\n\nConstruir visualizaciones de datos para resumir la conclusi\u00f3n de un an\u00e1lisis avanzado.\n\n\n\n\nLos \nCient\u00edficos de datos\n aportan un enfoque y una perspectiva totalmente nuevos a la comprensi\u00f3n de los datos.\n\n\nOtros roles relacionados con datos\n\n\nAdem\u00e1s del rol de cient\u00edfico de datos existen otros roles relacionados con el manejo de datos, los cuales muchas veces se confunden pero no son exactamente lo mismo. Estos roles son:\n\n\nAnalista de datos\n\n\nLos \nAnalistas de datos\n aportan valor a sus empresas mediante la obtenci\u00f3n de datos, su utilizaci\u00f3n para responder preguntas y la comunicaci\u00f3n de los resultados para ayudar a tomar decisiones. Las tareas m\u00e1s comunes realizadas por los analistas de datos incluyen la limpieza de datos, la realizaci\u00f3n de an\u00e1lisis y la creaci\u00f3n de visualizaciones. Dependiendo de la industria, el \nAnalista de datos\n puede tener varios t\u00edtulos diferentes (por ejemplo, analista de negocios, analista de inteligencia de negocios, analista de operaciones, analista de bases de datos). Independientemente del t\u00edtulo, el \nAnalista de datos\n es un generalista que puede encajar en muchos roles y equipos para ayudar a otros a tomar mejores decisiones basadas en datos.\n\n\nLa naturaleza de las habilidades requeridas depender\u00e1 de las necesidades espec\u00edficas de la empresa, pero estas son algunas de ellas:\n\n\n\n\nLimpieza y organizaci\u00f3n de datos en bruto.\n\n\nUso de estad\u00edsticas descriptivas para obtener una vista panor\u00e1mica de sus datos.\n\n\nAn\u00e1lisis de tendencias interesantes encontradas en los datos.\n\n\nCreaci\u00f3n de visualizaciones y cuadros de mando para ayudar a la empresa a interpretar y tomar decisiones con los datos.\n\n\nPresentaci\u00f3n de los resultados de un an\u00e1lisis t\u00e9cnico a clientes empresariales o equipos internos.\n\n\n\n\nEl \nAnalista de datos\n aporta un valor significativo tanto a los aspectos t\u00e9cnicos como no t\u00e9cnicos de una organizaci\u00f3n.\n\n\nIngeniero de datos\n\n\nLos \nIngenieros de datos\n construyen y optimizan los sistemas que permiten a los cient\u00edficos y analistas de datos realizar su trabajo. Cada empresa depende de los datos sean exactos y accesibles, para que las personas puedan trabajar con ellos. El \nIngeniero de datos\n se asegura de que cualquier dato sea recibido, transformado, almacenado y hecho accesible para otros usuarios.\n\n\nLos \nIngenieros de datos\n son responsables de construir las herramientas para trabajar con datos y, a menudo, tienen que usar t\u00e9cnicas complejas para manejar los datos a escala. A diferencia de los cient\u00edficos y analistas de datos, la ingenier\u00eda de datos se inclina mucho m\u00e1s hacia un conjunto de habilidades de desarrollo de software.\n\n\nUn buen \nIngeniero de datos\n debe permitir que los cient\u00edficos o analistas de datos puedan concentrarse en resolver problemas, en lugar de tener que preocuparse por aspectos m\u00e1s t\u00e9cnicos de la disciplina, como por ejemplo mover los datos de una fuente a otra.\n\n\nLa mentalidad del \nIngeniero de datos\n suele estar m\u00e1s centrada en la construcci\u00f3n y la optimizaci\u00f3n. Los siguientes son ejemplos de tareas en las que un ingeniero de datos podr\u00eda estar trabajando:\n\n\n\n\nCreaci\u00f3n de APIs para el consumo de datos.\n\n\nIntegraci\u00f3n de conjuntos de datos externos o nuevos en los procesos de datos existentes.\n\n\nAplicaci\u00f3n de transformaciones de atributos para los modelos de \naprendizaje autom\u00e1tico\n.\n\n\nSupervisar y probar continuamente los sistemas para asegurar un rendimiento optimizado.",
            "title": "Ciencia de datos"
        },
        {
            "location": "/datascience/#ciencia-de-datos",
            "text": "En el mundo actual de la tecnolog\u00eda que nos rodea, donde la  computaci\u00f3n en la nube  se va haciendo parte de nuestro d\u00eda a d\u00eda (quien no usa los servicios de  Google ,  Facebook ,  Twitter ,  Dropbox , o  Evernote ); d\u00f3nde hay cada vez una mayor cantidad de dispositivos que est\u00e1n las 24 horas del d\u00eda conectadas a internet (desde tel\u00e9fonos, tabletas y TVs hasta autom\u00f3viles), acerc\u00e1ndonos a\u00fan m\u00e1s al concepto de la  Internet de las cosas . En este mundo d\u00f3nde estamos generando datos constantemente, en el mundo de la  Big Data ; se esta haciendo cada vez m\u00e1s necesario un nuevo perfil de profesionales de la informaci\u00f3n que puedan aplicar las t\u00e9cnicas de la  Ciencia de Datos .",
            "title": "Ciencia de datos"
        },
        {
            "location": "/datascience/#que-es-la-ciencia-de-datos",
            "text": "La  Ciencia de Datos  es un campo interdisciplinario que involucra m\u00e9todos cient\u00edficos, procesos y sistemas para extraer conocimiento o un mejor entendimiento de datos en sus diferentes formas, ya sea estructurados o no estructurados. Es una continuaci\u00f3n de algunos campos de  an\u00e1lisis de datos  como la  estad\u00edstica , la  miner\u00eda de datos , el  aprendizaje autom\u00e1tico  y el  an\u00e1lisis predictivo . Comprende tres \u00e1reas distintas y superpuestas: las habilidades de un estad\u00edstico que sabe c\u00f3mo modelar y resumir conjuntos de datos (los cuales cada vez tienen mayor tama\u00f1o); las habilidades de un inform\u00e1tico que pueda dise\u00f1ar y utilizar algoritmos para almacenar, procesar y visualizar eficientemente estos datos; Y la experiencia sobre el  campo  o  dominio , lo que podr\u00edamos pensar como una formaci\u00f3n  cl\u00e1sica  en un tema; la cual es necesaria tanto para formular las preguntas correctas como para poner sus respuestas en contexto.",
            "title": "\u00bfQu\u00e9 es la Ciencia de Datos?"
        },
        {
            "location": "/datascience/#el-proceso-de-la-ciencia-de-datos",
            "text": "En general, el proceso que utiliza la  Ciencia de Datos  para  explorar el mundo usando datos es el siguiente:    El primer paso consiste en establecer un  objetivo de investigaci\u00f3n . El prop\u00f3sito principal aqu\u00ed es\nasegurarse de que todos los interesados comprendan el  qu\u00e9, c\u00f3mo y por qu\u00e9  del proyecto. Siempre debemos tener bien en claro cual es la pregunta que queremos responder con la ayuda de los datos.    El segundo paso consiste en la  obtenci\u00f3n de los datos . Los datos deben estar disponibles para poder ser analizados. Este paso incluye encontrar los datos adecuados y obtener acceso a los mismos. El resultado de esta etapa suelen ser los datos en su forma cruda, que probablemente necesitar\u00e1n ser pulidos y transformados antes de que puedan ser utilizados.    Ahora que ya tenemos los datos sin procesar, el siguiente paso es  prepararlos . Esto incluye la transformaci\u00f3n de los datos de una forma cruda a una forma en la que puedan ser utilizados directamente en los modelos. Para poder lograr esto, debemos detectar y corregir diferentes tipos de errores en los datos, combinar datos de diferentes fuentes y transformarlos. Una vez completado este paso, podemos avanzar hacia la visualizaci\u00f3n de datos y el modelado.    El cuarto paso es la  exploraci\u00f3n de datos . El objetivo de este etapa es obtener una comprensi\u00f3n profunda de los datos. Buscaremos patrones, correlaciones y desv\u00edos basados en t\u00e9cnicas visuales y descriptivas. Los conocimientos adquiridos en esta fase nos permitir\u00e1n comenzar con el armado del modelo.    Finalmente llegamos al paso principal y m\u00e1s importante: la  construcci\u00f3n de modelos . En esta etapa intentamos obtener los conocimiento o hacer las predicciones de acuerdo a los lineamientos establecidos en la primer etapa. Aqu\u00ed podemos utilizar todas las t\u00e9cnicas y herramientas que nos proporciona el  Machine Learning . El objetivo es obtener el modelo o la combinaci\u00f3n de modelos que mejor resultados nos proporcionen.    El \u00faltimo paso del proceso de la  Ciencia de Datos  es  presentar los resultados y automatizar an\u00e1lisis . Un buen modelo no sirve de nada si no es utilizado para mejorar la eficiencia y obtener mejores resultados. En esta \u00faltima etapa debemos presentarle los resultados del an\u00e1lisis a las personas responsables de tomar las decisiones en las organizaciones para que los modelos puedan ser adoptados.    En general, estas etapas no siguen una progresi\u00f3n lineal desde el paso 1 al 6. Si no que, a menudo, debemos regresar e iterar entre las diferentes etapas de acuerdo a los resultados que vayamos obteniendo. Actualmente, a los profesionales que se dedican a esta disciplina, se los conoce como  Cient\u00edficos de datos",
            "title": "El proceso de la Ciencia de Datos"
        },
        {
            "location": "/datascience/#cientifico-de-datos",
            "text": "Los  Data Scientists  o  Cient\u00edficos de datos   son profesionales, generalmente con conocimientos multidisciplinarios, que poseen el entrenamiento y la curiosidad necesarias para realizar descubrimientos en el intrincado mundo de la  Big Data . Ellos son capaces de darle forma a la enorme cantidad de datos desestructurados que generamos d\u00eda a d\u00eda y hacer su an\u00e1lisis posible. Se encargan de identificar potenciales fuentes de informaci\u00f3n, unirlas y depurar el conjunto de resultados; los  Cient\u00edficos de datos  ayudan a los encargados de tomar las decisiones a moverse de un an\u00e1lisis  ad hoc  de los datos hacia una constante conversaci\u00f3n con ellos.  Los  Cient\u00edficos de datos  se encargan de encontrar patrones en los datos, hacer descubrimientos en base a ellos, y comunicar las implicaciones de lo que han aprendido a trav\u00e9s de su an\u00e1lisis, para indicar nuevas oportunidades de negocios. Ellos aconsejan a los ejecutivos y gerentes de productos sobre las implicaciones de los datos para los productos, procesos y decisiones.  Si bien, una primera impresi\u00f3n, se imaginar\u00eda a los  Cient\u00edficos de datos  como personas con un fuerte perfil anal\u00edtico y mucho conocimiento estad\u00edstico y matem\u00e1tico, esta impresi\u00f3n estar\u00eda por dem\u00e1s errada. Ellos se caracterizan m\u00e1s por su parte cient\u00edfica; una de las facetas dominantes de su personalidad es su intensa curiosidad, el deseo por ir m\u00e1s all\u00e1 de la superficie de los problemas, encontrar las preguntas en lo m\u00e1s profundo de ellos, e ir depur\u00e1ndolas hasta crear un claro conjunto de hip\u00f3tesis que puedan ser probadas con datos concretos. Es por esto, que algunos de los m\u00e1s renombrados  Cient\u00edficos de datos  en las principales empresas de tecnolog\u00eda del mundo, vienen de campos poco convencionales como la  F\u00edsica  y las  Ciencias Sociales .  Lo que motiva a los  Cient\u00edficos de datos  no es armar hermosos reportes con informaci\u00f3n estructurada, para eso ya existen los analistas financieros; lo que realmente motiva a los  Cient\u00edficos de datos  es crear nuevas cosas, no solo dar consejo; ellos quieren crear soluciones que funcionen y generen un impacto innovador para el negocio y los consumidores.  Una podr\u00eda pensar a los  Cient\u00edficos de datos  como un h\u00edbrido entre hacker, analista, comunicador y consejero; personas que tengan el conocimiento t\u00e9cnico necesario para manejar y analizar grandes cantidades de datos, pero que a su vez tengan la suficiente noci\u00f3n y entendimiento de los negocios y la habilidad para comunicar los datos de una forma efectiva. Una combinaci\u00f3n realmente rara de darse, pero sumamente efectiva!.  En lo que hace al apartado t\u00e9cnico, una de las habilidades b\u00e1sicas que todo buen  Cient\u00edficos de datos  deber\u00eda tener, es sin duda la habilidad de escribir c\u00f3digo, programar. Un buen  Cient\u00edficos de datos  deber\u00eda ser eficiente con al menos un lenguaje de programaci\u00f3n de alto rendimiento (como  C ,  C++  o  Java ) y tener nociones sobre los principales lenguajes que se manejan en internet ( HTML ,  CSS3 ,  Javascript ,  PHP ).  Tambi\u00e9n deber\u00eda poseer buenos conocimientos sobre  probabilidad y estad\u00edstica , aqu\u00ed lenguajes de programaci\u00f3n con  R  y  Python , pueden resultar realmente \u00fatiles.  Y finalmente, deber\u00eda poseer conocimientos sobre los principales  frameworks para el manejo de la  Big Data , como por ejemplo  Hadoop ; conocimientos sobre la infraestructura de la   computaci\u00f3n en la nube ; y sobre las principales  bases de datos , tanto  SQL  como  NoSQL .  Los siguientes son ejemplos del trabajo realizado por los  Cient\u00edficos de datos :   Evaluaci\u00f3n de modelos estad\u00edsticos para determinar la validez de los an\u00e1lisis.  Utilizar el  aprendizaje autom\u00e1tico  para construir mejores algoritmos predictivos.  Pruebas y mejora continua de la precisi\u00f3n de los modelos de  aprendizaje autom\u00e1tico .  Construir visualizaciones de datos para resumir la conclusi\u00f3n de un an\u00e1lisis avanzado.   Los  Cient\u00edficos de datos  aportan un enfoque y una perspectiva totalmente nuevos a la comprensi\u00f3n de los datos.",
            "title": "Cient\u00edfico de datos"
        },
        {
            "location": "/datascience/#otros-roles-relacionados-con-datos",
            "text": "Adem\u00e1s del rol de cient\u00edfico de datos existen otros roles relacionados con el manejo de datos, los cuales muchas veces se confunden pero no son exactamente lo mismo. Estos roles son:",
            "title": "Otros roles relacionados con datos"
        },
        {
            "location": "/datascience/#analista-de-datos",
            "text": "Los  Analistas de datos  aportan valor a sus empresas mediante la obtenci\u00f3n de datos, su utilizaci\u00f3n para responder preguntas y la comunicaci\u00f3n de los resultados para ayudar a tomar decisiones. Las tareas m\u00e1s comunes realizadas por los analistas de datos incluyen la limpieza de datos, la realizaci\u00f3n de an\u00e1lisis y la creaci\u00f3n de visualizaciones. Dependiendo de la industria, el  Analista de datos  puede tener varios t\u00edtulos diferentes (por ejemplo, analista de negocios, analista de inteligencia de negocios, analista de operaciones, analista de bases de datos). Independientemente del t\u00edtulo, el  Analista de datos  es un generalista que puede encajar en muchos roles y equipos para ayudar a otros a tomar mejores decisiones basadas en datos.  La naturaleza de las habilidades requeridas depender\u00e1 de las necesidades espec\u00edficas de la empresa, pero estas son algunas de ellas:   Limpieza y organizaci\u00f3n de datos en bruto.  Uso de estad\u00edsticas descriptivas para obtener una vista panor\u00e1mica de sus datos.  An\u00e1lisis de tendencias interesantes encontradas en los datos.  Creaci\u00f3n de visualizaciones y cuadros de mando para ayudar a la empresa a interpretar y tomar decisiones con los datos.  Presentaci\u00f3n de los resultados de un an\u00e1lisis t\u00e9cnico a clientes empresariales o equipos internos.   El  Analista de datos  aporta un valor significativo tanto a los aspectos t\u00e9cnicos como no t\u00e9cnicos de una organizaci\u00f3n.",
            "title": "Analista de datos"
        },
        {
            "location": "/datascience/#ingeniero-de-datos",
            "text": "Los  Ingenieros de datos  construyen y optimizan los sistemas que permiten a los cient\u00edficos y analistas de datos realizar su trabajo. Cada empresa depende de los datos sean exactos y accesibles, para que las personas puedan trabajar con ellos. El  Ingeniero de datos  se asegura de que cualquier dato sea recibido, transformado, almacenado y hecho accesible para otros usuarios.  Los  Ingenieros de datos  son responsables de construir las herramientas para trabajar con datos y, a menudo, tienen que usar t\u00e9cnicas complejas para manejar los datos a escala. A diferencia de los cient\u00edficos y analistas de datos, la ingenier\u00eda de datos se inclina mucho m\u00e1s hacia un conjunto de habilidades de desarrollo de software.  Un buen  Ingeniero de datos  debe permitir que los cient\u00edficos o analistas de datos puedan concentrarse en resolver problemas, en lugar de tener que preocuparse por aspectos m\u00e1s t\u00e9cnicos de la disciplina, como por ejemplo mover los datos de una fuente a otra.  La mentalidad del  Ingeniero de datos  suele estar m\u00e1s centrada en la construcci\u00f3n y la optimizaci\u00f3n. Los siguientes son ejemplos de tareas en las que un ingeniero de datos podr\u00eda estar trabajando:   Creaci\u00f3n de APIs para el consumo de datos.  Integraci\u00f3n de conjuntos de datos externos o nuevos en los procesos de datos existentes.  Aplicaci\u00f3n de transformaciones de atributos para los modelos de  aprendizaje autom\u00e1tico .  Supervisar y probar continuamente los sistemas para asegurar un rendimiento optimizado.",
            "title": "Ingeniero de datos"
        },
        {
            "location": "/bigdata/",
            "text": "Introducci\u00f3n a la Big Data\n\n\n\n\nActualmente estamos viviendo la \nEra de los datos\n, con el enorme crecimiento en el uso de herramientas digitales como internet y los dispositivos m\u00f3viles, estamos generando datos constantemente, con cada movimiento que damos navegando en internet o interactuando en las redes sociales como Facebook o Twitter, estamos dejando una huella permanente que queda almacenada en alg\u00fan \nData Center\n. El volumen de informaci\u00f3n que estamos generando a cada segundo es enorme.\n\n\nEste gran crecimiento en el volumen de datos, ha obligado a rever la forma en la que dise\u00f1amos nuestros sistemas, y es as\u00ed, como el concepto de \nBig Data\n y la nueva disciplina de los \ncientificos de datos\n ha surgido.\n\n\n\u00bfQu\u00e9 es la Big Data?\n\n\nLa \nBig Data\n es la rama de las \nTeconlog\u00edas de la informaci\u00f3n\n que estudia las dificultades inherentes a la manipulaci\u00f3n de grandes \nconjuntos de datos\n. El verdadero poder de la \nBig Data\n reside en que se trata sobre el comportamiento de la gente, y no sobre sus datos, consiste en encontrar los patrones de relaciones y comportamientos en el caos de los grandes vol\u00famenes de datos que producimos. Los datos son tantos, que todos se vuelven estad\u00edstacamente significativos, lo que significa, que los m\u00e9todos tradicionales que utilizamos para analizarlos se han vuelto obsoletos.\n\n\nPara hacer frente a los nuevos desaf\u00edos que provoca la \nBig Data\n, nuevas herramientas inform\u00e1ticas han aparecido, sobre todo en el mundo del software \nopen source\n; es as\u00ed, como el movimiento de \nNoSQL\n y proyectos como el de \nApache Hadoop\n, han ganando popularidad.\n\n\nBases de datos NoSQL\n\n\nEl movimiento \nNoSQL\n hace referencia a una serie de nuevos sistemas de gesti\u00f3n de datos, que se alejan del tradicional modelo relacional que tienen las populares bases de datos actuales (\nOracle\n, \nDB2\n, \nMsSQL\n, \nMySQL\n, \nPostgreSQL\n, etc); la principal diferencia que caracteriza a estas nuevas bases de datos es que no poseen un esquema estructurado (tablas y sus relaciones) y no utilizan el lenguaje \nSQL\n para realizar sus consultas. Las bases de datos \nNoSQL\n surgieron principalmente para hacer frente al tratamiento de datos que los sistemas tradicionales de \nRDBMS\n no pod\u00edan manejar del todo bien (como ser toda la nueva informaci\u00f3n que surge del uso de las redes sociales). Se suelen agrupar en tres grandes grupos:\n\n\n\n\nBases de Datos Documentales:\n Donde la informaci\u00f3n se va almacenando en diferentes documentos, estos documentos son un conjunto ordenado de claves con valores asociados. El principal exponente de este tipo de base de datos \nNoSQL\n es \nMongoDB\n.\n\n\nBases de Datos clave-valor:\n  Donde los datos se van almacenando en pares (clave-valor). Un ejemplo de este tipo de bases es \nRedis\n.\n\n\nBig Tables:\n Grandes tablas de estructura tabular que se caracterizan por ser \ndistribuidas\n y de alta eficiencia. Las principales exponentes de este grupo son \nCassandra\n y \nHBase\n.\n\n\n\n\nHadoop\n\n\nEl proyecto \nApache Hadoop\n es un \nframework de software\n, desarrollado en el lenguaje de programaci\u00f3n \nJava\n, que permite el procesamiento distribuido de grandes \nconjuntos de datos\n a trav\u00e9s de \nclusters\n de computadoras utilizando simples modelos de programaci\u00f3n. La verdadera utilidad de \nHadoop\n radica en su capacidad para escalar f\u00e1cilmente de uno a miles de \nservidores distribuidos\n, cada uno aportando su poder de computaci\u00f3n y su capacidad de almacenamiento.\n\n\nLos m\u00f3dulos que forman parte del framework \nHadoop\n son:\n\n\n\n\nHadoop common:\n que incluye un conjunto de componentes e interfaces para el \nsistema de archivos\n distribuido que implementa \nHadoop\n. Son las herramientas comunes que se utilizan en todos los m\u00f3dulos.\n\n\nHadoop Distributed File System (HDFS):\n Una de las herramientas principales del framework, el \nsistema de archivos\n distribuido que corre entre los \nclusters\n de computadoras y que brinda el acceso a los datos para las aplicaciones.\n\n\nHadoop YARN(Yet Another Resource Negotiator):\n Es un sistema general de gesti\u00f3n de trabajos para correr aplicaciones \ndistribuidas\n.\n\n\nHadoop MapReduce:\n La herramienta principal del framework, y a la que \nHadoop\n debe su gran popularidad. Consiste en un modelo de procesamiento de datos \ndistribuidos\n que corre en forma paralela entre los \nclusters\n de computadoras que constituyen la arquitectura del framework \nHadoop\n. \nMapReduce\n es todo un modelo de programaci\u00f3n para el procesamientos de datos distribuidos. \n\n\n\n\nHadoop\n ha cambiado la econom\u00eda y la din\u00e1mica de la computaci\u00f3n a gran escala. Su impacto puede sintetizarse en cuatro caracter\u00edsticas principales.\n\n\nHadoop\n posibilita una soluci\u00f3n inform\u00e1tica que es:\n\n\n\n\nRedimensionable:\n Pueden agregarse tantos nuevos nodos como sea necesario, y agregarse sin tener que cambiar el formato de los datos, la forma en\n que se cargan los datos, la forma en que se escriben los procesos o las aplicaciones que est\u00e1n encima.\n\n\n\n\nRentable:\n \nHadoop\n incorpora masivamente la computaci\u00f3n paralela a los servidores b\u00e1sicos. El resultado de esto es una marcada reducci\u00f3n del costo por terabyte de almacenamiento, que a su vez abarata el modelado de sus datos.\n\n\n\n\n\n\nFlexible:\n \nHadoop\n funciona sin esquema y puede absorber cualquier tipo de datos, estructurados o no, provenientes de un n\u00famero cualquiera de fuentes. Los datos de diversas fuentes pueden agruparse de manera arbitraria y as\u00ed permitir an\u00e1lisis m\u00e1s profundos que los proporcionados por cualquier otro sistema.\n\n\n\n\n\n\nTolerante a fallas:\n Si usted pierde un nodo, el sistema redirige el trabajo a otra localizaci\u00f3n de los datos y contin\u00faa procesando sin perder el ritmo.\n\n\n\n\n\n\nHadoop\n es la herramienta que grandes empresas como Facebook, Yahoo, Linkedin y Twitter han elegido para almacenar todos los datos de sus usuarios y saberlo todo sobre ellos. \n\n\nSpark\n\n\nApache Spark\n es una de las nuevas estrellas en el \nan\u00e1lisis de datos\n masivos. Desarrollado en \nScala\n, \nApache Spark\n es una plataforma de computaci\u00f3n de c\u00f3digo abierto para el an\u00e1lisis y procesamiento de grandes vol\u00famenes de datos. \n\n\nAlgunas de las ventajas que nos ofrece \nApache Spark\n sobre otros \nframeworks\n, son:\n\n\n\n\n\n\nVelocidad:\n Sin dudas la velocidad es una de las principales fortalezas de \nApache Spark\n, como esta dise\u00f1ado para soportar el \nprocesameinto en memoria\n, puede alcanzar una performance sorprendente en an\u00e1lisis avanzados de datos. Algunos programas escritos utilizando \nApache Spark\n, pueden correr hasta 100x m\u00e1s r\u00e1pido que utilizando \nHadoop\n.\n\n\n\n\n\n\nF\u00e1cil de usar:\n Podemos escribir programas en \nPython\n, \nScala\n o \nJava\n que hagan uso de las herramientas que ofrece \nApache Spark\n; asimismo nos permite trabajar en forma interactiva (con \nPython\n o con \nScala\n) y su \nAPI\n es muy f\u00e1cil de aprender. \n\n\n\n\n\n\nGeneralismo:\n El mundo del an\u00e1lisis de datos incluye muchos subgrupos de distinta \u00edndole, est\u00e1n los que hacen un an\u00e1lisis investigativo, los que que realizan an\u00e1lisis exploratorios, los que construyen sistemas de procesamientos de datos, etc. Los usuarios de cada uno de esos subgrupos, al tener objetivos distintos, suelen utilizar una gran variedad de herramientas totalmente diferentes. \nApache Spark\n nos proporciona un gran n\u00famero de herramientas de alto nivel como \nSpark SQL\n, \nMLlib\n para \nmachine learning\n, \nGraphX\n, y \nSpark Streaming\n; las cuales pueden ser combinadas para crear aplicaciones multiprop\u00f3sito que ataquen los diferentes dominios del an\u00e1lisis de datos.\n\n\n\n\n\n\nOtras herramientas de Big data\n\n\nKafka\n\n\nKafka\n Es una plataforma distribuida de \nstreaming\n. Es decir, que tiene tres capacidades clave:\n\n\n\n\nPermite publicar y suscribirse a flujos de registros. A este respecto, es similar a una cola de mensajes o un sistema de mensajer\u00eda empresarial.\n\n\nPermite almacenar flujos de registros de forma tolerante a fallos.\n\n\nPermite procesar flujos de registros a medida que ocurren.\n\n\n\n\n\u00bfPara qu\u00e9 sirve \nKafka\n?\n\n\nSe utiliza para dos clases de aplicaci\u00f3n:\n\n\n\n\nCreaci\u00f3n de flujos de datos que fluyen en tiempo real entre distintos sistemas o aplicaciones.\n\n\nCreaci\u00f3n de aplicaciones de streaming en tiempo real que transforman o reaccionan a los flujos de datos.\n\n\n\n\nStorm\n\n\nStorm\n es una sistema de computaci\u00f3n distribuida en tiempo real y de c\u00f3digo abierto. Permite el procesamiento sencillo y fiable de grandes vol\u00famenes de datos en anal\u00edtica (por ejemplo para el estudio de informaci\u00f3n de modalidad continua procedente de redes sociales), \nRPC distribuida\n, \nprocesos de ETL\n. \n\n\nMientras que \nHadoop\n se encarga del procesamiento de datos por lotes, \nStorm\n se encarga de hacerlo en tiempo real. \n\n\nSi bien la \nBig Data\n trae muchas oportunidades, tambi\u00e9n abre muchas interrogantes, como ser: \u00bfCu\u00e1n \u00e9tica es esta recolecci\u00f3n de datos silenciosa que realizan las grandes empresas sobre nuestros datos?; \u00bfQuienes son los verdaderos due\u00f1os de esos datos?\u00bfQu\u00e9 pueden hacer las empresas con ellos?, el debate esta abierto\u2026",
            "title": "Big Data"
        },
        {
            "location": "/bigdata/#introduccion-a-la-big-data",
            "text": "Actualmente estamos viviendo la  Era de los datos , con el enorme crecimiento en el uso de herramientas digitales como internet y los dispositivos m\u00f3viles, estamos generando datos constantemente, con cada movimiento que damos navegando en internet o interactuando en las redes sociales como Facebook o Twitter, estamos dejando una huella permanente que queda almacenada en alg\u00fan  Data Center . El volumen de informaci\u00f3n que estamos generando a cada segundo es enorme.  Este gran crecimiento en el volumen de datos, ha obligado a rever la forma en la que dise\u00f1amos nuestros sistemas, y es as\u00ed, como el concepto de  Big Data  y la nueva disciplina de los  cientificos de datos  ha surgido.",
            "title": "Introducci\u00f3n a la Big Data"
        },
        {
            "location": "/bigdata/#que-es-la-big-data",
            "text": "La  Big Data  es la rama de las  Teconlog\u00edas de la informaci\u00f3n  que estudia las dificultades inherentes a la manipulaci\u00f3n de grandes  conjuntos de datos . El verdadero poder de la  Big Data  reside en que se trata sobre el comportamiento de la gente, y no sobre sus datos, consiste en encontrar los patrones de relaciones y comportamientos en el caos de los grandes vol\u00famenes de datos que producimos. Los datos son tantos, que todos se vuelven estad\u00edstacamente significativos, lo que significa, que los m\u00e9todos tradicionales que utilizamos para analizarlos se han vuelto obsoletos.  Para hacer frente a los nuevos desaf\u00edos que provoca la  Big Data , nuevas herramientas inform\u00e1ticas han aparecido, sobre todo en el mundo del software  open source ; es as\u00ed, como el movimiento de  NoSQL  y proyectos como el de  Apache Hadoop , han ganando popularidad.",
            "title": "\u00bfQu\u00e9 es la Big Data?"
        },
        {
            "location": "/bigdata/#bases-de-datos-nosql",
            "text": "El movimiento  NoSQL  hace referencia a una serie de nuevos sistemas de gesti\u00f3n de datos, que se alejan del tradicional modelo relacional que tienen las populares bases de datos actuales ( Oracle ,  DB2 ,  MsSQL ,  MySQL ,  PostgreSQL , etc); la principal diferencia que caracteriza a estas nuevas bases de datos es que no poseen un esquema estructurado (tablas y sus relaciones) y no utilizan el lenguaje  SQL  para realizar sus consultas. Las bases de datos  NoSQL  surgieron principalmente para hacer frente al tratamiento de datos que los sistemas tradicionales de  RDBMS  no pod\u00edan manejar del todo bien (como ser toda la nueva informaci\u00f3n que surge del uso de las redes sociales). Se suelen agrupar en tres grandes grupos:   Bases de Datos Documentales:  Donde la informaci\u00f3n se va almacenando en diferentes documentos, estos documentos son un conjunto ordenado de claves con valores asociados. El principal exponente de este tipo de base de datos  NoSQL  es  MongoDB .  Bases de Datos clave-valor:   Donde los datos se van almacenando en pares (clave-valor). Un ejemplo de este tipo de bases es  Redis .  Big Tables:  Grandes tablas de estructura tabular que se caracterizan por ser  distribuidas  y de alta eficiencia. Las principales exponentes de este grupo son  Cassandra  y  HBase .",
            "title": "Bases de datos NoSQL"
        },
        {
            "location": "/bigdata/#hadoop",
            "text": "El proyecto  Apache Hadoop  es un  framework de software , desarrollado en el lenguaje de programaci\u00f3n  Java , que permite el procesamiento distribuido de grandes  conjuntos de datos  a trav\u00e9s de  clusters  de computadoras utilizando simples modelos de programaci\u00f3n. La verdadera utilidad de  Hadoop  radica en su capacidad para escalar f\u00e1cilmente de uno a miles de  servidores distribuidos , cada uno aportando su poder de computaci\u00f3n y su capacidad de almacenamiento.  Los m\u00f3dulos que forman parte del framework  Hadoop  son:   Hadoop common:  que incluye un conjunto de componentes e interfaces para el  sistema de archivos  distribuido que implementa  Hadoop . Son las herramientas comunes que se utilizan en todos los m\u00f3dulos.  Hadoop Distributed File System (HDFS):  Una de las herramientas principales del framework, el  sistema de archivos  distribuido que corre entre los  clusters  de computadoras y que brinda el acceso a los datos para las aplicaciones.  Hadoop YARN(Yet Another Resource Negotiator):  Es un sistema general de gesti\u00f3n de trabajos para correr aplicaciones  distribuidas .  Hadoop MapReduce:  La herramienta principal del framework, y a la que  Hadoop  debe su gran popularidad. Consiste en un modelo de procesamiento de datos  distribuidos  que corre en forma paralela entre los  clusters  de computadoras que constituyen la arquitectura del framework  Hadoop .  MapReduce  es todo un modelo de programaci\u00f3n para el procesamientos de datos distribuidos.    Hadoop  ha cambiado la econom\u00eda y la din\u00e1mica de la computaci\u00f3n a gran escala. Su impacto puede sintetizarse en cuatro caracter\u00edsticas principales.  Hadoop  posibilita una soluci\u00f3n inform\u00e1tica que es:   Redimensionable:  Pueden agregarse tantos nuevos nodos como sea necesario, y agregarse sin tener que cambiar el formato de los datos, la forma en\n que se cargan los datos, la forma en que se escriben los procesos o las aplicaciones que est\u00e1n encima.   Rentable:   Hadoop  incorpora masivamente la computaci\u00f3n paralela a los servidores b\u00e1sicos. El resultado de esto es una marcada reducci\u00f3n del costo por terabyte de almacenamiento, que a su vez abarata el modelado de sus datos.    Flexible:   Hadoop  funciona sin esquema y puede absorber cualquier tipo de datos, estructurados o no, provenientes de un n\u00famero cualquiera de fuentes. Los datos de diversas fuentes pueden agruparse de manera arbitraria y as\u00ed permitir an\u00e1lisis m\u00e1s profundos que los proporcionados por cualquier otro sistema.    Tolerante a fallas:  Si usted pierde un nodo, el sistema redirige el trabajo a otra localizaci\u00f3n de los datos y contin\u00faa procesando sin perder el ritmo.    Hadoop  es la herramienta que grandes empresas como Facebook, Yahoo, Linkedin y Twitter han elegido para almacenar todos los datos de sus usuarios y saberlo todo sobre ellos.",
            "title": "Hadoop"
        },
        {
            "location": "/bigdata/#spark",
            "text": "Apache Spark  es una de las nuevas estrellas en el  an\u00e1lisis de datos  masivos. Desarrollado en  Scala ,  Apache Spark  es una plataforma de computaci\u00f3n de c\u00f3digo abierto para el an\u00e1lisis y procesamiento de grandes vol\u00famenes de datos.   Algunas de las ventajas que nos ofrece  Apache Spark  sobre otros  frameworks , son:    Velocidad:  Sin dudas la velocidad es una de las principales fortalezas de  Apache Spark , como esta dise\u00f1ado para soportar el  procesameinto en memoria , puede alcanzar una performance sorprendente en an\u00e1lisis avanzados de datos. Algunos programas escritos utilizando  Apache Spark , pueden correr hasta 100x m\u00e1s r\u00e1pido que utilizando  Hadoop .    F\u00e1cil de usar:  Podemos escribir programas en  Python ,  Scala  o  Java  que hagan uso de las herramientas que ofrece  Apache Spark ; asimismo nos permite trabajar en forma interactiva (con  Python  o con  Scala ) y su  API  es muy f\u00e1cil de aprender.     Generalismo:  El mundo del an\u00e1lisis de datos incluye muchos subgrupos de distinta \u00edndole, est\u00e1n los que hacen un an\u00e1lisis investigativo, los que que realizan an\u00e1lisis exploratorios, los que construyen sistemas de procesamientos de datos, etc. Los usuarios de cada uno de esos subgrupos, al tener objetivos distintos, suelen utilizar una gran variedad de herramientas totalmente diferentes.  Apache Spark  nos proporciona un gran n\u00famero de herramientas de alto nivel como  Spark SQL ,  MLlib  para  machine learning ,  GraphX , y  Spark Streaming ; las cuales pueden ser combinadas para crear aplicaciones multiprop\u00f3sito que ataquen los diferentes dominios del an\u00e1lisis de datos.",
            "title": "Spark"
        },
        {
            "location": "/bigdata/#otras-herramientas-de-big-data",
            "text": "",
            "title": "Otras herramientas de Big data"
        },
        {
            "location": "/bigdata/#kafka",
            "text": "Kafka  Es una plataforma distribuida de  streaming . Es decir, que tiene tres capacidades clave:   Permite publicar y suscribirse a flujos de registros. A este respecto, es similar a una cola de mensajes o un sistema de mensajer\u00eda empresarial.  Permite almacenar flujos de registros de forma tolerante a fallos.  Permite procesar flujos de registros a medida que ocurren.   \u00bfPara qu\u00e9 sirve  Kafka ?  Se utiliza para dos clases de aplicaci\u00f3n:   Creaci\u00f3n de flujos de datos que fluyen en tiempo real entre distintos sistemas o aplicaciones.  Creaci\u00f3n de aplicaciones de streaming en tiempo real que transforman o reaccionan a los flujos de datos.",
            "title": "Kafka"
        },
        {
            "location": "/bigdata/#storm",
            "text": "Storm  es una sistema de computaci\u00f3n distribuida en tiempo real y de c\u00f3digo abierto. Permite el procesamiento sencillo y fiable de grandes vol\u00famenes de datos en anal\u00edtica (por ejemplo para el estudio de informaci\u00f3n de modalidad continua procedente de redes sociales),  RPC distribuida ,  procesos de ETL .   Mientras que  Hadoop  se encarga del procesamiento de datos por lotes,  Storm  se encarga de hacerlo en tiempo real.   Si bien la  Big Data  trae muchas oportunidades, tambi\u00e9n abre muchas interrogantes, como ser: \u00bfCu\u00e1n \u00e9tica es esta recolecci\u00f3n de datos silenciosa que realizan las grandes empresas sobre nuestros datos?; \u00bfQuienes son los verdaderos due\u00f1os de esos datos?\u00bfQu\u00e9 pueden hacer las empresas con ellos?, el debate esta abierto\u2026",
            "title": "Storm"
        },
        {
            "location": "/internet-de-las-cosas/",
            "text": "Internet de las cosas\n\n\n\n\nDesde su creaci\u00f3n en 1969, la \ninternet\n, el mayor conjunto de informaci\u00f3n, personas, computadoras y software funcionando de manera cooperativa; lo ha cambiado todo.\nA partir de los avances tecnol\u00f3gicos, el acceso a \ninternet\n de alta velocidad se ha hecho omnipresente; y con la introducci\u00f3n de los \ndispositivos m\u00f3viles\n y la \nbanda ancha m\u00f3vil\n a trav\u00e9s de las redes celulares, ha surgido una cultura siempre alerta y siempre conectada. \n\n\nHoy en d\u00eda, cada dispositivo conectado a la \ninternet\n recibe una \ndirecci\u00f3n IP\n y esta direcci\u00f3n permite que cada dispositivo se conecte a otros dispositivos, incluyendo tel\u00e9fonos inteligentes, tabletas, consolas de juegos, autom\u00f3viles, heladeras, lavadorropas, sistemas de iluminaci\u00f3n, y muchas cosas m\u00e1s. Esto ha permitido que en los \u00faltimos a\u00f1os haya surgido una gran variedad de sistemas y plataformas interconectadas, un fen\u00f3meno que se conoce con el nombre de \nInternet de las cosas\n.\n\n\n\u00bfQu\u00e9 es la Internet de las cosas?\n\n\nPodemos definir a la \nInternet de las cosas\n o \nIoT\n como una colecci\u00f3n de  \ncosas\n u \nobjetos\n que se conectan a \ninternet\n, y entre s\u00ed. Como ya dijimos, estos \nobjetos\n podr\u00edan ser casi cualquier cosa: desde una computadora, una tableta o un smartphone, a un dispositivo de aire acondicionado, una cerradura de una puerta, un libro, un motor de avi\u00f3n, o una heladera. Cada uno de estos dispositivos o \ncosas\n tiene un n\u00famero de \nidentificaci\u00f3n \u00fanico (UID)\n y una \ndirecci\u00f3n IP\n. Estos \nobjetos\n se pueden conectar a trav\u00e9s de cables, fibra \u00f3ptica o tecnolog\u00edas inal\u00e1mbricas, como ser redes celulares, redes satelitales,  \nWi-Fi\n y \nBluetooth\n. Utilizan circuitos electr\u00f3nicos incorporados, as\u00ed como capacidades de \nidentificaci\u00f3n por radiofrecuencia (RFID)\n o de \ncomunicaci\u00f3n de campo cercano (NFC)\n que se a\u00f1aden posteriormente a trav\u00e9s de chips y plaquetas. Independientemente del enfoque exacto, el \nIoT\n implica el movimiento de datos a trav\u00e9s de \ninternet\n para permitir procesos desde una ubicaci\u00f3n en particular hacia alguna parte al otro lado del mundo.\n\n\nLa \nIoT\n est\u00e1 creando una explosi\u00f3n en la diversidad de dispositivos conectados a \ninternet\n. Estamos viendo como objetos familiares ganan conectividad y potencia computacional, as\u00ed como nuevas categor\u00edas de dispositivos que s\u00f3lo pueden existir como resultado de las redes interconectadas. Los \nsensores\n y \nactuadores\n crean nuevas posibilidades para unir informaci\u00f3n y acciones en los mundos real y digital.\n\n\nFactores que hacen posible la Internet de las cosas\n\n\nPara que una tecnolog\u00eda despegue se necesita la combinaci\u00f3n correcta de factores. La \nIoT\n est\u00e1 sucediendo ahora porque una serie de factores est\u00e1n disponibles juntos por primera vez:\n\u00a0\n\n\nSemiconductores de bajo costo y dispositivos inal\u00e1mbricos\n\n\nCada a\u00f1o se venden m\u00e1s de mil millones de tel\u00e9fonos inteligentes en un mercado altamente competitivo, con nuevos dise\u00f1os que se presentan cada 12-18 meses. Este crecimiento continuo y evoluci\u00f3n del dise\u00f1o reduce los costos de los  componentes, especialmente para los mercados adyacentes que pueden utilizar los dispositivos m\u00e1s antiguos. No s\u00f3lo la \nley de Moore\n reduce el costo de los semiconductores y la capacidad de computaci\u00f3n. La \nley de Koomey\n tambi\u00e9n est\u00e1 ah\u00ed, reduciendo constantemente la energ\u00eda necesaria para producir esa capacidad de computaci\u00f3n. Si adem\u00e1s tenemos en cuenta el crecimiento y la disponibilidad de las redes inal\u00e1mbricas y la compatibilidad con \nIPV6\n, est\u00e1 claro que la infraestructura necesaria para proporcionar \ninteligencia conectada\n sobre el mundo que nos rodea est\u00e1 f\u00e1cilmente disponible.\nLos fabricantes ahora pueden agregar peque\u00f1os microcontroladores y chips inal\u00e1mbricos a casi cualquier objeto f\u00edsico a bajo costo, creando de esta forma \nproductos inteligentes\n. Esta computaci\u00f3n y conectividad integradas eventualmente ser\u00e1n tan comunes como un enchufe de alimentaci\u00f3n o una bater\u00eda. \n\u00a0\n\n\nComputaci\u00f3n en la nube de c\u00f3digo abierto y en la web, Big Data y Ciencia de datos\n\n\nLa \ntecnolog\u00eda en la nube\n contin\u00faa avanzando r\u00e1pidamente y puede ser m\u00e1s segura y rentable que sus alternativas. Ahora es posible recopilar, almacenar y analizar de manera r\u00e1pida y eficiente cantidades masivas de datos no estructurados a escala web y tomar medidas sobre las ideas que desarrollamos a partir de ese an\u00e1lisis. Podemos distribuir las tareas inform\u00e1ticas horizontal, vertical y geogr\u00e1ficamente, tanto centralizadas como pr\u00f3ximas a un equipo f\u00edsico, proceso o cualquier otro objeto, permitiendo una capacidad virtualmente ilimitada para implementar los algoritmos inteligentes donde tendr\u00e1n el mayor impacto posible. Utilizando los dispositivos de \nIoT\n podemos obtener datos sobre cualquier cosa de inter\u00e9s y aprovechar esa informaci\u00f3n que obtengamos en tiempo real o casi real. Los beneficios potenciales de estas capacidades son b\u00e1sicamente limitados s\u00f3lo por nuestra creatividad.\n\n\nCiudades inteligentes\n\n\nLa nueva regla para el futuro que se acerca va a ser: \"Cualquier cosa que pueda conectarse, estar\u00e1 conectada\". Lo que nos va a permitir aplicar las tecnolog\u00edas de la \nInternet de las cosas\n para crear \nCiudades inteligentes\n que puedan ayudarnos a reducir el desperdicio y mejorar la eficiencia para cosas como el uso de energ\u00eda o el transporte; y nos permitir\u00e1 entender y mejorar c\u00f3mo trabajamos y vivimos. En este sentido, las \nCiudades inteligentes\n van a ser aquellas que utilicen la informaci\u00f3n obtenida a trav\u00e9s de los dispositivos de \nIoT\n para mejorar la infraestructura, la seguridad, y los bienes y servicios p\u00fablicos con el fin de mejorar la vida de sus ciudadanos. \n\n\nAlgunas de las aplicaciones de los programas de \nCiudades inteligentes\n incluyen: la iluminaci\u00f3n de calles inteligente que se apagar\u00e1 para conservar energ\u00eda cuando no haya nadie alrededor; el an\u00e1lisis del consumo de energ\u00eda de la ciudad para entender mejor la demanda; el trazando de los movimientos de las personas para maximizar el uso de las bicicletas y caminos peatonales; sensores conectados a las luces de la calle y otros muebles urbanos medir\u00e1n los niveles de ruido y la contaminaci\u00f3n del aire; entre otras cosas.\n\n\nRevoluci\u00f3n industrial 4.0\n\n\nSi aplicamos el concepto de \nIoT\n a la industria en lugar de las ciudades, obtenemos lo que se conoce como la \nRevoluci\u00f3n industrial 4.0\n. Aqu\u00ed la idea es crear \nf\u00e1bricas inteligentes\n en las que las m\u00e1quinas son aumentadas con conectividad web y conectadas a un sistema que puede visualizar toda la cadena de producci\u00f3n y tomar decisiones por su cuenta aplicando algoritmos inteligentes.\n\n\nPara que una f\u00e1brica o sistema sea considerado como de \nIndustria 4.0\n, debe incluir:\n\n\n\n\n\n\nInteroperabilidad:\n m\u00e1quinas, dispositivos, sensores y personas que se conectan y se comunican entre s\u00ed.\n\n\n\n\n\n\nTransparencia de la informaci\u00f3n:\n los sistemas crean una copia virtual del mundo f\u00edsico a trav\u00e9s de datos de sensores para contextualizar la informaci\u00f3n.\n\n\n\n\n\n\nAsistencia t\u00e9cnica:\n tanto la capacidad de los sistemas para apoyar a los seres humanos en la toma de decisiones y la soluci\u00f3n de problemas y la capacidad de ayudar a los seres humanos con tareas que son demasiado dif\u00edciles o inseguros para ellos.\n\n\n\n\n\n\nLa toma de decisiones descentralizada:\n la capacidad de los sistemas cibern\u00e9ticos de tomar decisiones simples por s\u00ed mismos y llegar a ser lo m\u00e1s aut\u00f3nomos posible.\n\n\n\n\n\n\nLa \u00c9tica y la Internet de las cosas\n\n\nLa tecnolog\u00eda generalmente crea nuevas posibilidades por un lado y nuevos problemas por el otro. Conectar la \ninternet\n al mundo real permite que nuestras acciones f\u00edsicas se hagan p\u00fablicas y, en la direcci\u00f3n contraria, que los eventos en \ninternet\n afecten nuestro entorno. La aplicaci\u00f3n de las tecnolog\u00edas de la \nInternet de las cosas\n puede conducir a situaciones que involucren problemas \u00e9ticos relacionados con la privacidad y el control.\n\n\nPrivacidad\n\n\nLa \ninternet\n, como plataforma masiva de publicaci\u00f3n abierta, ha sido una fuerza disruptiva en cuanto al concepto de privacidad. La mayor\u00edas de las cosas que escribimos en la web puede ser visible por cualquier persona. A medida que la utilizaci\u00f3n de los dispositivos de \nIoT\n se haga m\u00e1s masiva, vamos a estar abriendo nuevas puertas que afecten a\u00fan m\u00e1s nuestra privacidad. \n\n\nControl\n\n\nOtro tema que invita a debatir el concepto de \nInternet de las cosas\n es el del control de los datos. \u00bfQuienes son los due\u00f1os de los datos que se obtienen con los dispositivos de \nIoT\n? \u00bfLas personas due\u00f1as de los dispositivos o los fabricantes de los mismos o los que brindan los servicios? En la era actual en la que vivimos, controlar la informaci\u00f3n puede ser una ventaja competitiva muy importante. Asimismo, al estar todo tan interconectado y automatizado, \u00bfqu\u00e9 pasar\u00eda si un ciberataque toma el control de nuestras f\u00e1bricas inteligentes?. \n\n\nA medida que se va desarrollando la \nInternet de las cosas\n, veremos m\u00e1s claramente los pr\u00f3ximos pasos a seguir. Tal vez algunos de los futuros que propone no se lleguen a concretar nunca. Ser\u00e1 nuestra responsabilidad estar listos para ayudar a guiar la \nInternet de las cosas\n de tal manera que no se convierta en un instrumento invasor y opresivo, sino en un marco en el que podamos aprender mejor lo que es ser humano.",
            "title": "Internet de las cosas"
        },
        {
            "location": "/internet-de-las-cosas/#internet-de-las-cosas",
            "text": "Desde su creaci\u00f3n en 1969, la  internet , el mayor conjunto de informaci\u00f3n, personas, computadoras y software funcionando de manera cooperativa; lo ha cambiado todo.\nA partir de los avances tecnol\u00f3gicos, el acceso a  internet  de alta velocidad se ha hecho omnipresente; y con la introducci\u00f3n de los  dispositivos m\u00f3viles  y la  banda ancha m\u00f3vil  a trav\u00e9s de las redes celulares, ha surgido una cultura siempre alerta y siempre conectada.   Hoy en d\u00eda, cada dispositivo conectado a la  internet  recibe una  direcci\u00f3n IP  y esta direcci\u00f3n permite que cada dispositivo se conecte a otros dispositivos, incluyendo tel\u00e9fonos inteligentes, tabletas, consolas de juegos, autom\u00f3viles, heladeras, lavadorropas, sistemas de iluminaci\u00f3n, y muchas cosas m\u00e1s. Esto ha permitido que en los \u00faltimos a\u00f1os haya surgido una gran variedad de sistemas y plataformas interconectadas, un fen\u00f3meno que se conoce con el nombre de  Internet de las cosas .",
            "title": "Internet de las cosas"
        },
        {
            "location": "/internet-de-las-cosas/#que-es-la-internet-de-las-cosas",
            "text": "Podemos definir a la  Internet de las cosas  o  IoT  como una colecci\u00f3n de   cosas  u  objetos  que se conectan a  internet , y entre s\u00ed. Como ya dijimos, estos  objetos  podr\u00edan ser casi cualquier cosa: desde una computadora, una tableta o un smartphone, a un dispositivo de aire acondicionado, una cerradura de una puerta, un libro, un motor de avi\u00f3n, o una heladera. Cada uno de estos dispositivos o  cosas  tiene un n\u00famero de  identificaci\u00f3n \u00fanico (UID)  y una  direcci\u00f3n IP . Estos  objetos  se pueden conectar a trav\u00e9s de cables, fibra \u00f3ptica o tecnolog\u00edas inal\u00e1mbricas, como ser redes celulares, redes satelitales,   Wi-Fi  y  Bluetooth . Utilizan circuitos electr\u00f3nicos incorporados, as\u00ed como capacidades de  identificaci\u00f3n por radiofrecuencia (RFID)  o de  comunicaci\u00f3n de campo cercano (NFC)  que se a\u00f1aden posteriormente a trav\u00e9s de chips y plaquetas. Independientemente del enfoque exacto, el  IoT  implica el movimiento de datos a trav\u00e9s de  internet  para permitir procesos desde una ubicaci\u00f3n en particular hacia alguna parte al otro lado del mundo.  La  IoT  est\u00e1 creando una explosi\u00f3n en la diversidad de dispositivos conectados a  internet . Estamos viendo como objetos familiares ganan conectividad y potencia computacional, as\u00ed como nuevas categor\u00edas de dispositivos que s\u00f3lo pueden existir como resultado de las redes interconectadas. Los  sensores  y  actuadores  crean nuevas posibilidades para unir informaci\u00f3n y acciones en los mundos real y digital.",
            "title": "\u00bfQu\u00e9 es la Internet de las cosas?"
        },
        {
            "location": "/internet-de-las-cosas/#factores-que-hacen-posible-la-internet-de-las-cosas",
            "text": "Para que una tecnolog\u00eda despegue se necesita la combinaci\u00f3n correcta de factores. La  IoT  est\u00e1 sucediendo ahora porque una serie de factores est\u00e1n disponibles juntos por primera vez:",
            "title": "Factores que hacen posible la Internet de las cosas"
        },
        {
            "location": "/internet-de-las-cosas/#semiconductores-de-bajo-costo-y-dispositivos-inalambricos",
            "text": "Cada a\u00f1o se venden m\u00e1s de mil millones de tel\u00e9fonos inteligentes en un mercado altamente competitivo, con nuevos dise\u00f1os que se presentan cada 12-18 meses. Este crecimiento continuo y evoluci\u00f3n del dise\u00f1o reduce los costos de los  componentes, especialmente para los mercados adyacentes que pueden utilizar los dispositivos m\u00e1s antiguos. No s\u00f3lo la  ley de Moore  reduce el costo de los semiconductores y la capacidad de computaci\u00f3n. La  ley de Koomey  tambi\u00e9n est\u00e1 ah\u00ed, reduciendo constantemente la energ\u00eda necesaria para producir esa capacidad de computaci\u00f3n. Si adem\u00e1s tenemos en cuenta el crecimiento y la disponibilidad de las redes inal\u00e1mbricas y la compatibilidad con  IPV6 , est\u00e1 claro que la infraestructura necesaria para proporcionar  inteligencia conectada  sobre el mundo que nos rodea est\u00e1 f\u00e1cilmente disponible.\nLos fabricantes ahora pueden agregar peque\u00f1os microcontroladores y chips inal\u00e1mbricos a casi cualquier objeto f\u00edsico a bajo costo, creando de esta forma  productos inteligentes . Esta computaci\u00f3n y conectividad integradas eventualmente ser\u00e1n tan comunes como un enchufe de alimentaci\u00f3n o una bater\u00eda.",
            "title": "Semiconductores de bajo costo y dispositivos inal\u00e1mbricos"
        },
        {
            "location": "/internet-de-las-cosas/#computacion-en-la-nube-de-codigo-abierto-y-en-la-web-big-data-y-ciencia-de-datos",
            "text": "La  tecnolog\u00eda en la nube  contin\u00faa avanzando r\u00e1pidamente y puede ser m\u00e1s segura y rentable que sus alternativas. Ahora es posible recopilar, almacenar y analizar de manera r\u00e1pida y eficiente cantidades masivas de datos no estructurados a escala web y tomar medidas sobre las ideas que desarrollamos a partir de ese an\u00e1lisis. Podemos distribuir las tareas inform\u00e1ticas horizontal, vertical y geogr\u00e1ficamente, tanto centralizadas como pr\u00f3ximas a un equipo f\u00edsico, proceso o cualquier otro objeto, permitiendo una capacidad virtualmente ilimitada para implementar los algoritmos inteligentes donde tendr\u00e1n el mayor impacto posible. Utilizando los dispositivos de  IoT  podemos obtener datos sobre cualquier cosa de inter\u00e9s y aprovechar esa informaci\u00f3n que obtengamos en tiempo real o casi real. Los beneficios potenciales de estas capacidades son b\u00e1sicamente limitados s\u00f3lo por nuestra creatividad.",
            "title": "Computaci\u00f3n en la nube de c\u00f3digo abierto y en la web, Big Data y Ciencia de datos"
        },
        {
            "location": "/internet-de-las-cosas/#ciudades-inteligentes",
            "text": "La nueva regla para el futuro que se acerca va a ser: \"Cualquier cosa que pueda conectarse, estar\u00e1 conectada\". Lo que nos va a permitir aplicar las tecnolog\u00edas de la  Internet de las cosas  para crear  Ciudades inteligentes  que puedan ayudarnos a reducir el desperdicio y mejorar la eficiencia para cosas como el uso de energ\u00eda o el transporte; y nos permitir\u00e1 entender y mejorar c\u00f3mo trabajamos y vivimos. En este sentido, las  Ciudades inteligentes  van a ser aquellas que utilicen la informaci\u00f3n obtenida a trav\u00e9s de los dispositivos de  IoT  para mejorar la infraestructura, la seguridad, y los bienes y servicios p\u00fablicos con el fin de mejorar la vida de sus ciudadanos.   Algunas de las aplicaciones de los programas de  Ciudades inteligentes  incluyen: la iluminaci\u00f3n de calles inteligente que se apagar\u00e1 para conservar energ\u00eda cuando no haya nadie alrededor; el an\u00e1lisis del consumo de energ\u00eda de la ciudad para entender mejor la demanda; el trazando de los movimientos de las personas para maximizar el uso de las bicicletas y caminos peatonales; sensores conectados a las luces de la calle y otros muebles urbanos medir\u00e1n los niveles de ruido y la contaminaci\u00f3n del aire; entre otras cosas.",
            "title": "Ciudades inteligentes"
        },
        {
            "location": "/internet-de-las-cosas/#revolucion-industrial-40",
            "text": "Si aplicamos el concepto de  IoT  a la industria en lugar de las ciudades, obtenemos lo que se conoce como la  Revoluci\u00f3n industrial 4.0 . Aqu\u00ed la idea es crear  f\u00e1bricas inteligentes  en las que las m\u00e1quinas son aumentadas con conectividad web y conectadas a un sistema que puede visualizar toda la cadena de producci\u00f3n y tomar decisiones por su cuenta aplicando algoritmos inteligentes.  Para que una f\u00e1brica o sistema sea considerado como de  Industria 4.0 , debe incluir:    Interoperabilidad:  m\u00e1quinas, dispositivos, sensores y personas que se conectan y se comunican entre s\u00ed.    Transparencia de la informaci\u00f3n:  los sistemas crean una copia virtual del mundo f\u00edsico a trav\u00e9s de datos de sensores para contextualizar la informaci\u00f3n.    Asistencia t\u00e9cnica:  tanto la capacidad de los sistemas para apoyar a los seres humanos en la toma de decisiones y la soluci\u00f3n de problemas y la capacidad de ayudar a los seres humanos con tareas que son demasiado dif\u00edciles o inseguros para ellos.    La toma de decisiones descentralizada:  la capacidad de los sistemas cibern\u00e9ticos de tomar decisiones simples por s\u00ed mismos y llegar a ser lo m\u00e1s aut\u00f3nomos posible.",
            "title": "Revoluci\u00f3n industrial 4.0"
        },
        {
            "location": "/internet-de-las-cosas/#la-etica-y-la-internet-de-las-cosas",
            "text": "La tecnolog\u00eda generalmente crea nuevas posibilidades por un lado y nuevos problemas por el otro. Conectar la  internet  al mundo real permite que nuestras acciones f\u00edsicas se hagan p\u00fablicas y, en la direcci\u00f3n contraria, que los eventos en  internet  afecten nuestro entorno. La aplicaci\u00f3n de las tecnolog\u00edas de la  Internet de las cosas  puede conducir a situaciones que involucren problemas \u00e9ticos relacionados con la privacidad y el control.",
            "title": "La \u00c9tica y la Internet de las cosas"
        },
        {
            "location": "/internet-de-las-cosas/#privacidad",
            "text": "La  internet , como plataforma masiva de publicaci\u00f3n abierta, ha sido una fuerza disruptiva en cuanto al concepto de privacidad. La mayor\u00edas de las cosas que escribimos en la web puede ser visible por cualquier persona. A medida que la utilizaci\u00f3n de los dispositivos de  IoT  se haga m\u00e1s masiva, vamos a estar abriendo nuevas puertas que afecten a\u00fan m\u00e1s nuestra privacidad.",
            "title": "Privacidad"
        },
        {
            "location": "/internet-de-las-cosas/#control",
            "text": "Otro tema que invita a debatir el concepto de  Internet de las cosas  es el del control de los datos. \u00bfQuienes son los due\u00f1os de los datos que se obtienen con los dispositivos de  IoT ? \u00bfLas personas due\u00f1as de los dispositivos o los fabricantes de los mismos o los que brindan los servicios? En la era actual en la que vivimos, controlar la informaci\u00f3n puede ser una ventaja competitiva muy importante. Asimismo, al estar todo tan interconectado y automatizado, \u00bfqu\u00e9 pasar\u00eda si un ciberataque toma el control de nuestras f\u00e1bricas inteligentes?.   A medida que se va desarrollando la  Internet de las cosas , veremos m\u00e1s claramente los pr\u00f3ximos pasos a seguir. Tal vez algunos de los futuros que propone no se lleguen a concretar nunca. Ser\u00e1 nuestra responsabilidad estar listos para ayudar a guiar la  Internet de las cosas  de tal manera que no se convierta en un instrumento invasor y opresivo, sino en un marco en el que podamos aprender mejor lo que es ser humano.",
            "title": "Control"
        },
        {
            "location": "/procesamiento-del-lenguaje-natural/",
            "text": "Introducci\u00f3n al Procesamiento del Lenguaje Natural\n\n\n\n\nEl \nlenguaje\n es una de las herramientas centrales en nuestra vida social y profesional. Entre otras cosas, act\u00faa como un medio para transmitir ideas, informaci\u00f3n, opiniones y sentimientos; as\u00ed como para persuadir, pedir informaci\u00f3n, o dar ordenes. Asimismo, el \nlenguaje\n humano es algo que esta en constante cambio y evoluci\u00f3n; y que puede llegar a ser muy \nambiguo\n y \nvariable\n. Tomemos por ejemplo la frase \n\"com\u00ed una pizza con amigos\"\n comparada con \n\"com\u00ed una pizza con aceitunas\"\n; su estructura es la misma, pero su significado es totalmente distinto. De la misma manera, un mismo mensaje puede ser expresado de formas diferentes; \n\"com\u00ed una pizza con amigos\"\n puede tambi\u00e9n ser expresado como \n\"compart\u00ed una pizza con amigos\"\n. \n\n\nLos seres humanos somos muy buenos a la hora de producir e interpretar el \nlenguaje\n, podemos expresar, percibir e interpretar significados muy elaborados en fracci\u00f3n de segundos casi sin dificultades; pero al mismo tiempo, somos tambi\u00e9n muy malos a la hora de entender y describir formalmente las reglas que lo gobiernan. Por este motivo, entender y producir el \nlenguaje\n por medio de una \ncomputadora\n es un problema muy dif\u00edcil de resolver. \u00c9ste problema, es el campo de estudio de lo que en \ninteligencia artificial\n se conoce como \nProcesamiento del Lenguaje Natural\n o \nNLP\n por sus siglas en ingl\u00e9s. \n\n\n\u00bfQu\u00e9 es el Procesamiento del Lenguaje Natural?\n\n\nEl \nProcesamiento del Lenguaje Natural\n o \nNLP\n  es una disciplina que se encuentra en la intersecci\u00f3n de varias ciencias, tales como las \nCiencias de la Computaci\u00f3n\n, la \ninteligencia artificial\n y \nPsicolog\u00eda Cognitiva\n. Su idea central es la de darle a las m\u00e1quinas la capacidad de leer y comprender los idiomas que hablamos los humanos. La investigaci\u00f3n del \nProcesamiento del Lenguaje Natural\n tiene como objetivo responder a la pregunta de c\u00f3mo las personas son capaces de comprender el significado de una oraci\u00f3n oral / escrita y c\u00f3mo las personas entienden lo que sucedi\u00f3, cu\u00e1ndo y d\u00f3nde sucedi\u00f3; y las diferencias entre una suposici\u00f3n, una creencia o un hecho.\n\n\nLos elementos comunes de cualquier arquitectura est\u00e1ndar de un sistema para el \nProcesamiento del Lenguaje Natural\n son:\n\n\n\n\n\n\nReconocimiento de voz:\n Convertir una palabra hablada en un conjunto de palabras. Las palabras habladas se componen de una serie de par\u00e1metros relacionados con el sentido de la audici\u00f3n.\n\n\n\n\n\n\nComprensi\u00f3n del lenguaje:\n El objetivo de este elemento es generar un significado para las palabras habladas, y ese significado ser\u00e1 utilizado por el siguiente elemento (gesti\u00f3n del di\u00e1logo).\n\n\n\n\n\n\nGesti\u00f3n del di\u00e1logo:\n La tarea principal de este elemento es coordinar y mantener unidas todas las partes del sistema y los usuarios, y conectarse con otros sistemas.\n\n\n\n\n\n\nComunicaci\u00f3n con sistemas externos:\n como sistemas expertos, sistemas de bases de datos u otras aplicaciones inform\u00e1ticas.\n\n\n\n\n\n\nGeneraci\u00f3n de respuesta:\n Establecer el mensaje que el sistema debe entregar.\n\n\n\n\n\n\nSalida de voz:\n Uso de diferentes t\u00e9cnicas para producir el mensaje desde el sistema.\n\n\n\n\n\n\nEn general, en \nProcesamiento del Lenguaje Natural\n se utilizan seis niveles de comprensi\u00f3n con el objetivo de descubrir el significado del discurso. Estos niveles son:\n\n\n\n\n\n\nNivel fon\u00e9tico:\n Aqu\u00ed se presta atenci\u00f3n a la \nfon\u00e9tica\n, la forma en que las palabras son pronunciadas. Este nivel es importante cuando procesamos la palabra hablada, no as\u00ed cuando trabajamos con texto escrito. \n\n\n\n\n\n\nNivel morfol\u00f3gico:\n Aqu\u00ed nos interesa realizar un an\u00e1lisis \nmorfol\u00f3gico\n del discurso; estudiar la estructura de las palabras para delimitarlas y clasificarlas.\n\n\n\n\n\n\nNivel sint\u00e1ctico:\n Aqu\u00ed se realiza un an\u00e1lisis de \nsintaxis\n, el cual  incluye la acci\u00f3n de dividir una oraci\u00f3n en cada uno de sus componentes. \n\n\n\n\n\n\nNivel sem\u00e1ntico:\n Este nivel es un complemente del anterior, en el an\u00e1lisis \nsem\u00e1ntico\n se busca entender el significado de la oraci\u00f3n. Las palabras pueden tener m\u00faltiples significados, la idea es identificar el significado apropiado por medio del contexto de la oraci\u00f3n.\n\n\n\n\n\n\nNivel discursivo:\n El nivel discursivo examina el significado de la oraci\u00f3n en relaci\u00f3n a otra oraci\u00f3n en el texto o p\u00e1rrafo del mismo documento.\n\n\n\n\n\n\nNivel pragm\u00e1tico:\n Este nivel se ocupa del an\u00e1lisis de oraciones y c\u00f3mo se usan en diferentes situaciones. Adem\u00e1s, tambi\u00e9n c\u00f3mo su significado cambia dependiendo de la situaci\u00f3n.\n\n\n\n\n\n\nTodos los niveles descritos aqu\u00ed son inseparables y se complementan entre s\u00ed. El objetivo de los sistemas de \nNLP\n es incluir estas definiciones en una \ncomputadora\n y luego usarlas para crear una oraci\u00f3n estructurada y sin ambig\u00fcedades con un significado bien definido.\n\n\nAplicaciones del Procesamiento del Lenguaje Natural\n\n\nLos algoritmos de \nProcesamiento del Lenguaje Natural\n suelen basarse en algoritmos de \naprendizaje autom\u00e1tico\n. En lugar de codificar manualmente grandes conjuntos de reglas, el \nNLP\n puede confiar en el \naprendizaje autom\u00e1tico\n para aprender estas reglas autom\u00e1ticamente analizando un conjunto de ejemplos y haciendo una \ninferencia estad\u00edstica\n. En general, cuanto m\u00e1s datos analizados, m\u00e1s preciso ser\u00e1 el modelo. Estos algoritmos pueden ser utilizados en algunas de las siguientes aplicaciones:\n\n\n\n\n\n\nResumir texto:\n Podemos utilizar los modelos de \nNLP\n para extraer las ideas m\u00e1s importantes y centrales mientras ignoramos la informaci\u00f3n irrelevante.\n\n\n\n\n\n\nCrear chatbots:\n Podemos utilizar las t\u00e9cnicas de \nNLP\n para crear \nchatbots\n que puedan interactuar con las personas.\n\n\n\n\n\n\nGenerar autom\u00e1ticamente \netiquetas de palabras clave\n: Con \nNLP\n tambi\u00e9n podemos realizar un an\u00e1lisis de contenido aprovechando el algoritmo de \nLDA\n para asignar palabras claves a p\u00e1rrafos del texto.\n\n\n\n\n\n\nReconocer entidades\n: Con \nNLP\n podemos identificar a las distintas entidades del texto como ser una persona, lugar u organizaci\u00f3n.\n\n\n\n\n\n\nAn\u00e1lisis de sentimiento\n: Tambi\u00e9n podemos utilizar \nNLP\n para identificar el \nsentimiento\n de una cadena de texto, desde muy negativo a neutral y a muy positivo.\n\n\n\n\n\n\nCorpus ling\u00fc\u00edstico\n\n\nHoy en d\u00eda, es indispensable el uso de buenos recursos ling\u00fc\u00edsticos para el desarrollo de los sistemas de \nNLP\n. Estos recursos son esenciales para la creaci\u00f3n de gram\u00e1ticas, en el marco de aproximaciones simb\u00f3licas; o para llevar a cabo la formaci\u00f3n de m\u00f3dulos basados en el \naprendizaje autom\u00e1tico\n.\n\n\nUn \ncorpus ling\u00fc\u00edstico\n es un conjunto amplio y estructurado de ejemplos reales de uso de la lengua. Estos ejemplos pueden ser textos (los m\u00e1s comunes), o muestras orales (generalmente transcritas). Un \ncorpus ling\u00fc\u00edstico\n es un conjunto de textos relativamente grande, creado independientemente de sus posibles formas o usos. Es decir, en cuanto a su estructura, variedad y complejidad, un corpus debe reflejar una lengua, o su modalidad, de la forma m\u00e1s exacta posible; en cuanto a su uso, preocuparse de que su representaci\u00f3n sea real. La idea es que representen al lenguaje de la mejor forma posible para que los modelos de \nNLP\n puedan aprender los patrones necesarios para entender el \nlenguaje\n. Encontrar un buen \ncorpus\n sobre el cual trabajar no suele ser una tarea sencilla; uno que se suele utilizar para entrenar modelos es la informaci\u00f3n de \nwikipedia\n.\n\n\nLibrer\u00edas de Python para Procesamiento del Lenguaje Natural\n\n\nActualmente, \nPython\n es uno de los lenguajes m\u00e1s populares para trabajar en el campo la \ninteligencia artificial\n. Para abordar los problemas relacionados con el \nProcesamiento del Lenguaje Natural\n \nPython\n nos proporciona las siguientes librer\u00edas:\n\n\n\n\n\n\nNLTK\n:\n Es la librer\u00eda l\u00edder para el \nProcesamiento del Lenguaje Natural\n. Proporciona interfaces f\u00e1ciles de usar a m\u00e1s de \n50 corpus\n y recursos l\u00e9xicos, junto con un conjunto de bibliotecas de procesamiento de texto para la clasificaci\u00f3n, tokenizaci\u00f3n, el etiquetado, el an\u00e1lisis y el razonamiento sem\u00e1ntico.\n\n\n\n\n\n\nSpacy\n:\n Es una librer\u00eda relativamente nueva que sobresale por su facilidad de uso y su velocidad a la hora de realizar el procesamiento de texto.\n\n\n\n\n\n\nGensim\n:\n Es una librer\u00eda dise\u00f1ada para extraer autom\u00e1ticamente los temas sem\u00e1nticos de los documentos de la forma m\u00e1s eficiente y con menos complicaciones posible.\n\n\n\n\n\n\npyLDAvis\n:\n Esta librer\u00eda est\u00e1 dise\u00f1ado para ayudar a los usuarios a interpretar los temas que surgen de un an\u00e1lisis de t\u00f3picos. Nos permite visualizar en forma muy sencilla cada uno de los temas incluidos en el texto.\n\n\n\n\n\n\nActualmente el \nNLP\n se esta sumando a la popularidad del \nDeep Learning\n, por lo que muchos de los \nframeworks\n que se utilizan en \nDeep Learning\n pueden ser aplicados para realizar modelos de \nNLP\n.\n\n\nDeep Learning y Procesamiento del Lenguaje Natural\n\n\nDurante mucho tiempo, las t\u00e9cnicas principales de \nProcesamiento del Lenguaje Natural\n fueron dominadas por m\u00e9todos de \naprendizaje autom\u00e1tico\n que utilizaron \nmodelos lineales\n como las \nm\u00e1quinas de vectores de soporte\n o la \nregresi\u00f3n log\u00edstica\n, entrenados sobre \nvectores\n de caracter\u00edsticas de muy alta dimensional pero muy escasos.\nRecientemente, el campo ha tenido cierto \u00e9xito en el cambio hacia modelos de \ndeep learning\n sobre entradas densas.\n\n\nLas \nredes neuronales\n proporcionan una poderosa maquina de aprendizaje que es muy atractiva para su uso en problemas de \nlenguaje natural\n. Un componente importante en las \nredes neuronales\n para el \nlenguaje\n es el uso de una capa de \nword embedding\n, una asignaci\u00f3n de s\u00edmbolos discretos a vectores continuos en un espacio dimensional relativamente bajo. Cuando se utiliza \nword embedding\n, se transforman los distintos s\u00edmbolos en objetos matem\u00e1ticos sobre los que se pueden realizar operaciones. En particular, la distancia entre vectores puede equipararse a la distancia entre palabras, facilitando la generalizaci\u00f3n del comportamiento de una palabra sobre otra.\nEsta representaci\u00f3n de palabras como vectores es aprendida por la red como parte del proceso de entrenamiento.\nSubiendo en la jerarqu\u00eda, la red tambi\u00e9n aprende a combinar los vectores de palabras de una manera que es \u00fatil para la predicci\u00f3n. Esta capacidad alivia en cierta medida los problemas de dispersi\u00f3n de los datos.\nHay dos tipos principales de \narquitecturas de redes neuronales\n que resultan muy \u00fatiles en los problemas de \nProcesamiento del Lenguaje Natural\n: las \nRedes neuronales prealimentadas\n y las \nRedes neuronales recurrentes\n.",
            "title": "Procesamiento de lenguaje"
        },
        {
            "location": "/procesamiento-del-lenguaje-natural/#introduccion-al-procesamiento-del-lenguaje-natural",
            "text": "El  lenguaje  es una de las herramientas centrales en nuestra vida social y profesional. Entre otras cosas, act\u00faa como un medio para transmitir ideas, informaci\u00f3n, opiniones y sentimientos; as\u00ed como para persuadir, pedir informaci\u00f3n, o dar ordenes. Asimismo, el  lenguaje  humano es algo que esta en constante cambio y evoluci\u00f3n; y que puede llegar a ser muy  ambiguo  y  variable . Tomemos por ejemplo la frase  \"com\u00ed una pizza con amigos\"  comparada con  \"com\u00ed una pizza con aceitunas\" ; su estructura es la misma, pero su significado es totalmente distinto. De la misma manera, un mismo mensaje puede ser expresado de formas diferentes;  \"com\u00ed una pizza con amigos\"  puede tambi\u00e9n ser expresado como  \"compart\u00ed una pizza con amigos\" .   Los seres humanos somos muy buenos a la hora de producir e interpretar el  lenguaje , podemos expresar, percibir e interpretar significados muy elaborados en fracci\u00f3n de segundos casi sin dificultades; pero al mismo tiempo, somos tambi\u00e9n muy malos a la hora de entender y describir formalmente las reglas que lo gobiernan. Por este motivo, entender y producir el  lenguaje  por medio de una  computadora  es un problema muy dif\u00edcil de resolver. \u00c9ste problema, es el campo de estudio de lo que en  inteligencia artificial  se conoce como  Procesamiento del Lenguaje Natural  o  NLP  por sus siglas en ingl\u00e9s.",
            "title": "Introducci\u00f3n al Procesamiento del Lenguaje Natural"
        },
        {
            "location": "/procesamiento-del-lenguaje-natural/#que-es-el-procesamiento-del-lenguaje-natural",
            "text": "El  Procesamiento del Lenguaje Natural  o  NLP   es una disciplina que se encuentra en la intersecci\u00f3n de varias ciencias, tales como las  Ciencias de la Computaci\u00f3n , la  inteligencia artificial  y  Psicolog\u00eda Cognitiva . Su idea central es la de darle a las m\u00e1quinas la capacidad de leer y comprender los idiomas que hablamos los humanos. La investigaci\u00f3n del  Procesamiento del Lenguaje Natural  tiene como objetivo responder a la pregunta de c\u00f3mo las personas son capaces de comprender el significado de una oraci\u00f3n oral / escrita y c\u00f3mo las personas entienden lo que sucedi\u00f3, cu\u00e1ndo y d\u00f3nde sucedi\u00f3; y las diferencias entre una suposici\u00f3n, una creencia o un hecho.  Los elementos comunes de cualquier arquitectura est\u00e1ndar de un sistema para el  Procesamiento del Lenguaje Natural  son:    Reconocimiento de voz:  Convertir una palabra hablada en un conjunto de palabras. Las palabras habladas se componen de una serie de par\u00e1metros relacionados con el sentido de la audici\u00f3n.    Comprensi\u00f3n del lenguaje:  El objetivo de este elemento es generar un significado para las palabras habladas, y ese significado ser\u00e1 utilizado por el siguiente elemento (gesti\u00f3n del di\u00e1logo).    Gesti\u00f3n del di\u00e1logo:  La tarea principal de este elemento es coordinar y mantener unidas todas las partes del sistema y los usuarios, y conectarse con otros sistemas.    Comunicaci\u00f3n con sistemas externos:  como sistemas expertos, sistemas de bases de datos u otras aplicaciones inform\u00e1ticas.    Generaci\u00f3n de respuesta:  Establecer el mensaje que el sistema debe entregar.    Salida de voz:  Uso de diferentes t\u00e9cnicas para producir el mensaje desde el sistema.    En general, en  Procesamiento del Lenguaje Natural  se utilizan seis niveles de comprensi\u00f3n con el objetivo de descubrir el significado del discurso. Estos niveles son:    Nivel fon\u00e9tico:  Aqu\u00ed se presta atenci\u00f3n a la  fon\u00e9tica , la forma en que las palabras son pronunciadas. Este nivel es importante cuando procesamos la palabra hablada, no as\u00ed cuando trabajamos con texto escrito.     Nivel morfol\u00f3gico:  Aqu\u00ed nos interesa realizar un an\u00e1lisis  morfol\u00f3gico  del discurso; estudiar la estructura de las palabras para delimitarlas y clasificarlas.    Nivel sint\u00e1ctico:  Aqu\u00ed se realiza un an\u00e1lisis de  sintaxis , el cual  incluye la acci\u00f3n de dividir una oraci\u00f3n en cada uno de sus componentes.     Nivel sem\u00e1ntico:  Este nivel es un complemente del anterior, en el an\u00e1lisis  sem\u00e1ntico  se busca entender el significado de la oraci\u00f3n. Las palabras pueden tener m\u00faltiples significados, la idea es identificar el significado apropiado por medio del contexto de la oraci\u00f3n.    Nivel discursivo:  El nivel discursivo examina el significado de la oraci\u00f3n en relaci\u00f3n a otra oraci\u00f3n en el texto o p\u00e1rrafo del mismo documento.    Nivel pragm\u00e1tico:  Este nivel se ocupa del an\u00e1lisis de oraciones y c\u00f3mo se usan en diferentes situaciones. Adem\u00e1s, tambi\u00e9n c\u00f3mo su significado cambia dependiendo de la situaci\u00f3n.    Todos los niveles descritos aqu\u00ed son inseparables y se complementan entre s\u00ed. El objetivo de los sistemas de  NLP  es incluir estas definiciones en una  computadora  y luego usarlas para crear una oraci\u00f3n estructurada y sin ambig\u00fcedades con un significado bien definido.",
            "title": "\u00bfQu\u00e9 es el Procesamiento del Lenguaje Natural?"
        },
        {
            "location": "/procesamiento-del-lenguaje-natural/#aplicaciones-del-procesamiento-del-lenguaje-natural",
            "text": "Los algoritmos de  Procesamiento del Lenguaje Natural  suelen basarse en algoritmos de  aprendizaje autom\u00e1tico . En lugar de codificar manualmente grandes conjuntos de reglas, el  NLP  puede confiar en el  aprendizaje autom\u00e1tico  para aprender estas reglas autom\u00e1ticamente analizando un conjunto de ejemplos y haciendo una  inferencia estad\u00edstica . En general, cuanto m\u00e1s datos analizados, m\u00e1s preciso ser\u00e1 el modelo. Estos algoritmos pueden ser utilizados en algunas de las siguientes aplicaciones:    Resumir texto:  Podemos utilizar los modelos de  NLP  para extraer las ideas m\u00e1s importantes y centrales mientras ignoramos la informaci\u00f3n irrelevante.    Crear chatbots:  Podemos utilizar las t\u00e9cnicas de  NLP  para crear  chatbots  que puedan interactuar con las personas.    Generar autom\u00e1ticamente  etiquetas de palabras clave : Con  NLP  tambi\u00e9n podemos realizar un an\u00e1lisis de contenido aprovechando el algoritmo de  LDA  para asignar palabras claves a p\u00e1rrafos del texto.    Reconocer entidades : Con  NLP  podemos identificar a las distintas entidades del texto como ser una persona, lugar u organizaci\u00f3n.    An\u00e1lisis de sentimiento : Tambi\u00e9n podemos utilizar  NLP  para identificar el  sentimiento  de una cadena de texto, desde muy negativo a neutral y a muy positivo.",
            "title": "Aplicaciones del Procesamiento del Lenguaje Natural"
        },
        {
            "location": "/procesamiento-del-lenguaje-natural/#corpus-linguistico",
            "text": "Hoy en d\u00eda, es indispensable el uso de buenos recursos ling\u00fc\u00edsticos para el desarrollo de los sistemas de  NLP . Estos recursos son esenciales para la creaci\u00f3n de gram\u00e1ticas, en el marco de aproximaciones simb\u00f3licas; o para llevar a cabo la formaci\u00f3n de m\u00f3dulos basados en el  aprendizaje autom\u00e1tico .  Un  corpus ling\u00fc\u00edstico  es un conjunto amplio y estructurado de ejemplos reales de uso de la lengua. Estos ejemplos pueden ser textos (los m\u00e1s comunes), o muestras orales (generalmente transcritas). Un  corpus ling\u00fc\u00edstico  es un conjunto de textos relativamente grande, creado independientemente de sus posibles formas o usos. Es decir, en cuanto a su estructura, variedad y complejidad, un corpus debe reflejar una lengua, o su modalidad, de la forma m\u00e1s exacta posible; en cuanto a su uso, preocuparse de que su representaci\u00f3n sea real. La idea es que representen al lenguaje de la mejor forma posible para que los modelos de  NLP  puedan aprender los patrones necesarios para entender el  lenguaje . Encontrar un buen  corpus  sobre el cual trabajar no suele ser una tarea sencilla; uno que se suele utilizar para entrenar modelos es la informaci\u00f3n de  wikipedia .",
            "title": "Corpus ling\u00fc\u00edstico"
        },
        {
            "location": "/procesamiento-del-lenguaje-natural/#librerias-de-python-para-procesamiento-del-lenguaje-natural",
            "text": "Actualmente,  Python  es uno de los lenguajes m\u00e1s populares para trabajar en el campo la  inteligencia artificial . Para abordar los problemas relacionados con el  Procesamiento del Lenguaje Natural   Python  nos proporciona las siguientes librer\u00edas:    NLTK :  Es la librer\u00eda l\u00edder para el  Procesamiento del Lenguaje Natural . Proporciona interfaces f\u00e1ciles de usar a m\u00e1s de  50 corpus  y recursos l\u00e9xicos, junto con un conjunto de bibliotecas de procesamiento de texto para la clasificaci\u00f3n, tokenizaci\u00f3n, el etiquetado, el an\u00e1lisis y el razonamiento sem\u00e1ntico.    Spacy :  Es una librer\u00eda relativamente nueva que sobresale por su facilidad de uso y su velocidad a la hora de realizar el procesamiento de texto.    Gensim :  Es una librer\u00eda dise\u00f1ada para extraer autom\u00e1ticamente los temas sem\u00e1nticos de los documentos de la forma m\u00e1s eficiente y con menos complicaciones posible.    pyLDAvis :  Esta librer\u00eda est\u00e1 dise\u00f1ado para ayudar a los usuarios a interpretar los temas que surgen de un an\u00e1lisis de t\u00f3picos. Nos permite visualizar en forma muy sencilla cada uno de los temas incluidos en el texto.    Actualmente el  NLP  se esta sumando a la popularidad del  Deep Learning , por lo que muchos de los  frameworks  que se utilizan en  Deep Learning  pueden ser aplicados para realizar modelos de  NLP .",
            "title": "Librer\u00edas de Python para Procesamiento del Lenguaje Natural"
        },
        {
            "location": "/procesamiento-del-lenguaje-natural/#deep-learning-y-procesamiento-del-lenguaje-natural",
            "text": "Durante mucho tiempo, las t\u00e9cnicas principales de  Procesamiento del Lenguaje Natural  fueron dominadas por m\u00e9todos de  aprendizaje autom\u00e1tico  que utilizaron  modelos lineales  como las  m\u00e1quinas de vectores de soporte  o la  regresi\u00f3n log\u00edstica , entrenados sobre  vectores  de caracter\u00edsticas de muy alta dimensional pero muy escasos.\nRecientemente, el campo ha tenido cierto \u00e9xito en el cambio hacia modelos de  deep learning  sobre entradas densas.  Las  redes neuronales  proporcionan una poderosa maquina de aprendizaje que es muy atractiva para su uso en problemas de  lenguaje natural . Un componente importante en las  redes neuronales  para el  lenguaje  es el uso de una capa de  word embedding , una asignaci\u00f3n de s\u00edmbolos discretos a vectores continuos en un espacio dimensional relativamente bajo. Cuando se utiliza  word embedding , se transforman los distintos s\u00edmbolos en objetos matem\u00e1ticos sobre los que se pueden realizar operaciones. En particular, la distancia entre vectores puede equipararse a la distancia entre palabras, facilitando la generalizaci\u00f3n del comportamiento de una palabra sobre otra.\nEsta representaci\u00f3n de palabras como vectores es aprendida por la red como parte del proceso de entrenamiento.\nSubiendo en la jerarqu\u00eda, la red tambi\u00e9n aprende a combinar los vectores de palabras de una manera que es \u00fatil para la predicci\u00f3n. Esta capacidad alivia en cierta medida los problemas de dispersi\u00f3n de los datos.\nHay dos tipos principales de  arquitecturas de redes neuronales  que resultan muy \u00fatiles en los problemas de  Procesamiento del Lenguaje Natural : las  Redes neuronales prealimentadas  y las  Redes neuronales recurrentes .",
            "title": "Deep Learning y Procesamiento del Lenguaje Natural"
        },
        {
            "location": "/vision-por-computadora/",
            "text": "Visi\u00f3n por Computadora\n\n\n\n\nDe nuestros cinco sentidos, la \nvisi\u00f3n\n es sin duda uno de los m\u00e1s importantes y uno de los que m\u00e1s dependemos. Hay quienes dicen incluso, que los ojos son una ventana hacia nuestro cerebro; ya que alrededor del 50 % de las \u00e1reas de nuestro cerebro se utilizan en funciones para procesar la gran cantidad de informaci\u00f3n que ingresa a trav\u00e9s de nuestra vista.\n\n\nEn cierto sentido se podr\u00eda decir que la \nvisi\u00f3n\n es, en primer lugar, una tarea de procesamiento de informaci\u00f3n; pero a si mismo, es algo mucho m\u00e1s complejo, ya que para poder saber que es lo qu\u00e9 hay en el mundo nuestros cerebros deben ser capaces de \nrepresentar\n esta informaci\u00f3n en toda su abundancia de color, forma, movimiento, detalle y belleza. El estudio de la \nvisi\u00f3n\n debe entonces incluir, no solo el estudio de c\u00f3mo extraer de las im\u00e1genes los diversos aspectos del mundo que nos son \u00fatiles, sino tambi\u00e9n una investigaci\u00f3n sobre la naturaleza de las representaciones internas mediante las cuales captamos esta informaci\u00f3n y la utilizamos como base para las decisiones sobre nuestros pensamientos y acciones. Esta dualidad, de \nrepresentaci\u00f3n\n y \nprocesamiento de informaci\u00f3n\n esta en el coraz\u00f3n de la mayor\u00eda de las tareas de procesamiento de la informaci\u00f3n y se profundizar\u00e1 m\u00e1s en el estudio de los problemas relacionados con la \nVisi\u00f3n por computadora\n. \n\n\n\u00bfQu\u00e9 es la visi\u00f3n por computadora?\n\n\nHoy en d\u00eda, podemos encontrar im\u00e1genes y videos por todas partes. Los sitios para compartir videos y las redes sociales los tienen por billones. Pr\u00e1cticamente todos los tel\u00e9fonos m\u00f3viles y computadoras poseen c\u00e1maras incorporadas. Es muy com\u00fan que las personas tengan varios giga-bytes de fotos y videos en sus dispositivos. Programar una computadora y dise\u00f1ar los \nalgoritmos\n para entender qu\u00e9 hay en esas fotos y videos, es el campo de estudio de la \nVisi\u00f3n por computadora\n o \nComputer vision\n.\n\n\nLa \nVisi\u00f3n por computadora\n consiste en la extracci\u00f3n automatizada de informaci\u00f3n de las im\u00e1genes. Por informaci\u00f3n podemos entender casi cualquier cosa; desde modelos 3D, posici\u00f3n de la c\u00e1mara, reconocimiento de objetos, y agrupaci\u00f3n y b\u00fasqueda de contenido. Tambi\u00e9n puede incluir la deformaci\u00f3n de las im\u00e1genes, la eliminaci\u00f3n de ruidos y la realidad aumentada. Muchas veces la \nVisi\u00f3n por computadora\n trata de imitar a la visi\u00f3n humana, pero otras ves la geometr\u00eda o un enfoque m\u00e1s estad\u00edstico puede ser fundamental para resolver un problema. La \nVisi\u00f3n por computadora\n contiene una mezcla de programaci\u00f3n, modelado y matem\u00e1tica que lo convierte en un campo de estudio sumamente atractivo. \n\n\nAplicaciones de visi\u00f3n por computadora\n\n\nLa \nVisi\u00f3n por computadora\n es un \u00e1rea con m\u00e1s de 40 a\u00f1os de investigaci\u00f3n, por lo que ya contamos con varias aplicaciones de las t\u00e9cnicas desarrolladas. Estas aplicaciones incluyen:\n\n\n\n\n\n\nReconocimiento \u00f3ptico de caracteres (OCR)\n: Que consiste en la identificaci\u00f3n autom\u00e1ticamente a partir de una imagen de s\u00edmbolos o caracteres que pertenecen a un determinado alfabeto, para luego almacenarlos en forma de datos.\n\n\n\n\n\n\nInspecci\u00f3n robotizada\n: La inspecci\u00f3n r\u00e1pida de las piezas para garantizar la calidad de los componentes de fabricaci\u00f3n utilizando una visi\u00f3n est\u00e9reo con iluminaci\u00f3n especializada. \n\n\n\n\n\n\nVenta al por menor\n: Como ser los cl\u00e1sicos lectores de barras que encontramos en los supermercados para reconocer los precios de los productos en la l\u00ednea de cajas. \n\n\n\n\n\n\nConstrucci\u00f3n de modelos 3D\n: La construcci\u00f3n automatizada de modelos 3D a partir de fotograf\u00edas. \n\n\n\n\n\n\nIm\u00e1genes m\u00e9dicas\n: Como ser la tecnolog\u00eda utilizada para tomar radiograf\u00edas y las t\u00e9cnicas para detectar tumores malignos y anomal\u00edas en las mismas. \n\n\n\n\n\n\nSeguridad automotriz\n: Ayudando a detectar obst\u00e1culos mediante un sistema de conducci\u00f3n asistida utilizando diferentes c\u00e1maras.    \n\n\n\n\n\n\nCaptura de movimiento\n: Utilizando marcadores retro-reflexivos vistos desde m\u00faltiples c\u00e1maras u otras t\u00e9cnicas para la captura de movimientos de los actores para utilizar en \nanimaci\u00f3n por computadora\n.\n\n\n\n\n\n\nVigilancia\n: Monitoreo de intrusos, an\u00e1lisis del tr\u00e1fico vial, y monitoreo de piscinas para v\u00edctimas de ahogamiento. \n\n\n\n\n\n\nReconocimiento de huellas dactilares y biometr\u00eda\n: Para la identificaci\u00f3n autom\u00e1tica de accesos y tambi\u00e9n utilizada para aplicaciones forenses.\n\n\n\n\n\n\nDetecci\u00f3n de caras\n: Utilizado para mejorar el foco de las c\u00e1maras y para hacer una b\u00fasqueda m\u00e1s relevante de personas en im\u00e1genes. \n\n\n\n\n\n\nComo estas, existen muchas m\u00e1s aplicaciones de las t\u00e9cnicas de \nVisi\u00f3n por computadora\n; muchas de las cuales son algo ya com\u00fan para nosotros, como la sugerencia para etiquetar de Facebook. \n\n\nRedes neuronales convolucionales\n\n\nGracias a que el \u00e1rea del cerebro responsable de la \nvisi\u00f3n\n es una de las zonas m\u00e1s estudiadas y que m\u00e1s conocemos; sabemos que la \ncorteza visual\n contiene una disposici\u00f3n jer\u00e1rquica compleja de \nneuronas\n.  Por ejemplo, la informaci\u00f3n visual es introducida en la corteza a trav\u00e9s del \u00e1rea visual primaria, llamada V1. Las \nneuronas\n de V1 se ocupan de caracter\u00edsticas visuales de bajo nivel, tales como peque\u00f1os segmentos de contorno, componentes de peque\u00f1a escala del movimiento, \ndisparidad binocular\n, e informaci\u00f3n b\u00e1sica de contraste y color. V1 luego alimenta de informaci\u00f3n a otras \u00e1reas, como V2, V4 y V5. Cada una de estas \u00e1reas se ocupa de los aspectos m\u00e1s espec\u00edficos o abstractas de la informaci\u00f3n. Por ejemplo, las \nneuronas\n en V4 se ocupan de objetos de mediana complejidad, tales como formas de estrellas en diferentes colores. La \ncorteza visual\n de los animales es el m\u00e1s potente sistema de procesamiento visual que conocemos, por lo que suena l\u00f3gico inspirarse en ella para crear una variante de \nredes neuronales artificiales\n que ayude a identificar im\u00e1genes; es as\u00ed como surgen las \nredes neuronales convolucionales\n.\n\n\nLas \nredes neuronales convolucionales\n se componen de \nneuronas\n que tienen \npesos\n y \nsesgos\n que pueden aprender. Cada \nneurona\n recibe algunas entradas, realiza un \nproducto escalar\n y luego aplica una funci\u00f3n de activaci\u00f3n. Lo que diferencia a las \nredes neuronales convolucionales\n es que suponen expl\u00edcitamente que las entradas son im\u00e1genes, lo que nos permite codificar ciertas propiedades en la arquitectura; permitiendo ganar en eficiencia y reducir la cantidad de par\u00e1metros en la red. Las \nredes neuronales convolucionales\n trabajan modelando de forma consecutiva peque\u00f1as piezas de informaci\u00f3n, y luego combinando esta informaci\u00f3n en las capas m\u00e1s profundas de la red. Una manera de entenderlas es que la primera capa intentar\u00e1 detectar los bordes y establecer patrones de detecci\u00f3n de bordes. Luego, las capas posteriores trataran de combinarlos en formas m\u00e1s simples y, finalmente, en patrones de las diferentes posiciones de los objetos, iluminaci\u00f3n, escalas, etc. Las capas finales intentar\u00e1n hacer coincidir una imagen de entrada con todos los patrones y arribar a una predicci\u00f3n final como una suma ponderada de todos ellos. De esta forma las \nredes neuronales convolucionales\n son capaces de modelar complejas variaciones y comportamientos dando predicciones bastantes precisas.\n\n\nEstructura de las Redes Neuronales Convolucionales\n\n\nEn general, las \nredes neuronales convolucionales\n van a estar construidas con una estructura que contendr\u00e1 3 tipos distintos de capas:\n\n\nCapa convolucional\n\n\nEsta es la capa que le nombre a la red, lo que distingue a las \nredes neuronales convolucionales\n de cualquier otra \nred neuronal\n es utilizan un operaci\u00f3n llamada \nconvoluci\u00f3n\n en alguna de sus capas; en lugar de utilizar la multiplicaci\u00f3n de matrices que se aplica generalmente.\nLa operaci\u00f3n de \nconvoluci\u00f3n\n recibe como \nentrada o input\n la imagen y luego aplica sobre ella un \nfiltro o kernel\n que nos devuelve un \nmapa de las caracter\u00edsticas\n de la imagen original, de esta forma logramos reducir el tama\u00f1o de los par\u00e1metros.\nLa \nconvoluci\u00f3n\n aprovecha tres ideas importantes que pueden ayudar a mejorar cualquier sistema de \nMachine Learning\n, ellas son: \n\n \ninteracciones dispersas\n, ya que al aplicar un filtro de menor tama\u00f1o sobre la entrada original podemos reducir dr\u00e1sticamente la cantidad de par\u00e1metros y c\u00e1lculos;\n\n los \npar\u00e1metros compartidos\n, que hace referencia a compartir los par\u00e1metros entre los distintos tipos de filtros, ayudando tambi\u00e9n a mejorar la eficiencia del sistema; y las \nrepresentaciones equivariante\n, que indican que si las entradas cambian, las salidas van a cambiar tambi\u00e9n en forma similar. \n\n\nPor otra parte, la \nconvoluci\u00f3n\n proporciona un medio para trabajar con entradas de tama\u00f1o variable, lo que puede ser tambi\u00e9n muy conveniente.\n\n\n\n\nCapa de reducci\u00f3n o pooling\n\n\nLa capa de reducci\u00f3n o \npooling\n se coloca generalmente despu\u00e9s de la capa \nconvolucional\n. Su utilidad principal radica en la reducci\u00f3n de las dimensiones espaciales (ancho x alto) del volumen de entrada para la siguiente capa \nconvolucional\n. No afecta a la dimensi\u00f3n de profundidad del volumen.\nLa operaci\u00f3n realizada por esta capa tambi\u00e9n se llama \nreducci\u00f3n de muestreo\n, ya que la reducci\u00f3n de tama\u00f1o conduce tambi\u00e9n a la p\u00e9rdida de informaci\u00f3n. Sin embargo, una p\u00e9rdida de este tipo puede ser beneficioso para la red por dos razones:\n\n\n\n\nla disminuci\u00f3n en el tama\u00f1o conduce a una menor sobrecarga de c\u00e1lculo para las pr\u00f3ximas capas de la red;\n\n\ntambi\u00e9n trabaja para reducir el \nsobreajuste\n.\n\n\n\n\nLa operaci\u00f3n que se suele utilizar en esta capa es \nmax-pooling\n, que divide a la imagen de entrada en un conjunto de rect\u00e1ngulos y, respecto de cada subregi\u00f3n, se va quedando con el m\u00e1ximo valor.\n\n\n\n\nCapa clasificadora totalmente conectada\n\n\nAl final de las capas \nconvolucional\n y de \npooling\n, las redes utilizan generalmente capas completamente conectados en la que cada pixel se considera como una \nneurona\n separada al igual que en una \nred neuronal\n regular. Esta \u00faltima capa clasificadora tendr\u00e1 tantas \nneuronas\n como el n\u00famero de clases que se debe predecir.\n\n\nLos modelos de \nDeep Learning\n basados en \nredes neuronales convolucionales\n han logrado resultados sorprendentes en varios problemas de \nVisi\u00f3n por computadora\n, como ser los casos de clasificaci\u00f3n de im\u00e1genes y detecci\u00f3n de objetos. \n\n\nOpenCV\n\n\nOpenCV\n es una librer\u00eda \nOpen Source\n de \nVisi\u00f3n por computadora\n que nos permite manipular im\u00e1genes y videos para realizar una variedad de tareas que van desde la detecci\u00f3n autom\u00e1tica de caras, a la visualizaci\u00f3n de las im\u00e1genes de una c\u00e1mara Web; o hasta ense\u00f1arle a un robot a reconocer objetos en la vida real. Fue creada en 1999, por Gary Bradski, quien trabajaba en \nIntel\n, y que lanz\u00f3 \nOpenCV\n con la intensi\u00f3n de acelerar la \nVisi\u00f3n por computadora\n y la \nInteligencia Artificial\n proporcionando una infraestructura s\u00f3lida para todos los que trabajan en el campo. La librer\u00eda est\u00e1 escrita en \nC\n y \nC++\n y se puede ejecutar bajo \nLinux\n, Windows y Mac OS X. Posee un desarrollo activo de interfaces para \nPython\n, \nJava\n , MATLAB y otros lenguajes, incluyendo el soporte para plataformas como \nAndroid\n e iOS para aplicaciones m\u00f3viles. \n\n\nOpenCV\n contiene m\u00e1s de 500 funciones que abarcan muchas \u00e1reas de \nVisi\u00f3n por computadora\n, incluyendo tales como: inspecci\u00f3n de productos de f\u00e1brica, im\u00e1genes m\u00e9dicas, seguridad, interfaz de usuario, calibraci\u00f3n de c\u00e1mara, visi\u00f3n est\u00e9reo y rob\u00f3tica. Debido a que la \nVisi\u00f3n por computadora\n y el \nMachine Learning\n a menudo van de la mano, \nOpenCV\n tambi\u00e9n contiene una librer\u00eda completa de uso general de \nMachine Learning\n (ML); la cual se centra en el reconocimiento de patrones estad\u00edsticos y el agrupamiento. \nOpenCV\n es sin dudas el lugar por d\u00f3nde comenzar para empezar a incursionar en el mundo de la \nVisi\u00f3n por computadora\n.",
            "title": "Visi\u00f3n por computadora"
        },
        {
            "location": "/vision-por-computadora/#vision-por-computadora",
            "text": "De nuestros cinco sentidos, la  visi\u00f3n  es sin duda uno de los m\u00e1s importantes y uno de los que m\u00e1s dependemos. Hay quienes dicen incluso, que los ojos son una ventana hacia nuestro cerebro; ya que alrededor del 50 % de las \u00e1reas de nuestro cerebro se utilizan en funciones para procesar la gran cantidad de informaci\u00f3n que ingresa a trav\u00e9s de nuestra vista.  En cierto sentido se podr\u00eda decir que la  visi\u00f3n  es, en primer lugar, una tarea de procesamiento de informaci\u00f3n; pero a si mismo, es algo mucho m\u00e1s complejo, ya que para poder saber que es lo qu\u00e9 hay en el mundo nuestros cerebros deben ser capaces de  representar  esta informaci\u00f3n en toda su abundancia de color, forma, movimiento, detalle y belleza. El estudio de la  visi\u00f3n  debe entonces incluir, no solo el estudio de c\u00f3mo extraer de las im\u00e1genes los diversos aspectos del mundo que nos son \u00fatiles, sino tambi\u00e9n una investigaci\u00f3n sobre la naturaleza de las representaciones internas mediante las cuales captamos esta informaci\u00f3n y la utilizamos como base para las decisiones sobre nuestros pensamientos y acciones. Esta dualidad, de  representaci\u00f3n  y  procesamiento de informaci\u00f3n  esta en el coraz\u00f3n de la mayor\u00eda de las tareas de procesamiento de la informaci\u00f3n y se profundizar\u00e1 m\u00e1s en el estudio de los problemas relacionados con la  Visi\u00f3n por computadora .",
            "title": "Visi\u00f3n por Computadora"
        },
        {
            "location": "/vision-por-computadora/#que-es-la-vision-por-computadora",
            "text": "Hoy en d\u00eda, podemos encontrar im\u00e1genes y videos por todas partes. Los sitios para compartir videos y las redes sociales los tienen por billones. Pr\u00e1cticamente todos los tel\u00e9fonos m\u00f3viles y computadoras poseen c\u00e1maras incorporadas. Es muy com\u00fan que las personas tengan varios giga-bytes de fotos y videos en sus dispositivos. Programar una computadora y dise\u00f1ar los  algoritmos  para entender qu\u00e9 hay en esas fotos y videos, es el campo de estudio de la  Visi\u00f3n por computadora  o  Computer vision .  La  Visi\u00f3n por computadora  consiste en la extracci\u00f3n automatizada de informaci\u00f3n de las im\u00e1genes. Por informaci\u00f3n podemos entender casi cualquier cosa; desde modelos 3D, posici\u00f3n de la c\u00e1mara, reconocimiento de objetos, y agrupaci\u00f3n y b\u00fasqueda de contenido. Tambi\u00e9n puede incluir la deformaci\u00f3n de las im\u00e1genes, la eliminaci\u00f3n de ruidos y la realidad aumentada. Muchas veces la  Visi\u00f3n por computadora  trata de imitar a la visi\u00f3n humana, pero otras ves la geometr\u00eda o un enfoque m\u00e1s estad\u00edstico puede ser fundamental para resolver un problema. La  Visi\u00f3n por computadora  contiene una mezcla de programaci\u00f3n, modelado y matem\u00e1tica que lo convierte en un campo de estudio sumamente atractivo.",
            "title": "\u00bfQu\u00e9 es la visi\u00f3n por computadora?"
        },
        {
            "location": "/vision-por-computadora/#aplicaciones-de-vision-por-computadora",
            "text": "La  Visi\u00f3n por computadora  es un \u00e1rea con m\u00e1s de 40 a\u00f1os de investigaci\u00f3n, por lo que ya contamos con varias aplicaciones de las t\u00e9cnicas desarrolladas. Estas aplicaciones incluyen:    Reconocimiento \u00f3ptico de caracteres (OCR) : Que consiste en la identificaci\u00f3n autom\u00e1ticamente a partir de una imagen de s\u00edmbolos o caracteres que pertenecen a un determinado alfabeto, para luego almacenarlos en forma de datos.    Inspecci\u00f3n robotizada : La inspecci\u00f3n r\u00e1pida de las piezas para garantizar la calidad de los componentes de fabricaci\u00f3n utilizando una visi\u00f3n est\u00e9reo con iluminaci\u00f3n especializada.     Venta al por menor : Como ser los cl\u00e1sicos lectores de barras que encontramos en los supermercados para reconocer los precios de los productos en la l\u00ednea de cajas.     Construcci\u00f3n de modelos 3D : La construcci\u00f3n automatizada de modelos 3D a partir de fotograf\u00edas.     Im\u00e1genes m\u00e9dicas : Como ser la tecnolog\u00eda utilizada para tomar radiograf\u00edas y las t\u00e9cnicas para detectar tumores malignos y anomal\u00edas en las mismas.     Seguridad automotriz : Ayudando a detectar obst\u00e1culos mediante un sistema de conducci\u00f3n asistida utilizando diferentes c\u00e1maras.        Captura de movimiento : Utilizando marcadores retro-reflexivos vistos desde m\u00faltiples c\u00e1maras u otras t\u00e9cnicas para la captura de movimientos de los actores para utilizar en  animaci\u00f3n por computadora .    Vigilancia : Monitoreo de intrusos, an\u00e1lisis del tr\u00e1fico vial, y monitoreo de piscinas para v\u00edctimas de ahogamiento.     Reconocimiento de huellas dactilares y biometr\u00eda : Para la identificaci\u00f3n autom\u00e1tica de accesos y tambi\u00e9n utilizada para aplicaciones forenses.    Detecci\u00f3n de caras : Utilizado para mejorar el foco de las c\u00e1maras y para hacer una b\u00fasqueda m\u00e1s relevante de personas en im\u00e1genes.     Como estas, existen muchas m\u00e1s aplicaciones de las t\u00e9cnicas de  Visi\u00f3n por computadora ; muchas de las cuales son algo ya com\u00fan para nosotros, como la sugerencia para etiquetar de Facebook.",
            "title": "Aplicaciones de visi\u00f3n por computadora"
        },
        {
            "location": "/vision-por-computadora/#redes-neuronales-convolucionales",
            "text": "Gracias a que el \u00e1rea del cerebro responsable de la  visi\u00f3n  es una de las zonas m\u00e1s estudiadas y que m\u00e1s conocemos; sabemos que la  corteza visual  contiene una disposici\u00f3n jer\u00e1rquica compleja de  neuronas .  Por ejemplo, la informaci\u00f3n visual es introducida en la corteza a trav\u00e9s del \u00e1rea visual primaria, llamada V1. Las  neuronas  de V1 se ocupan de caracter\u00edsticas visuales de bajo nivel, tales como peque\u00f1os segmentos de contorno, componentes de peque\u00f1a escala del movimiento,  disparidad binocular , e informaci\u00f3n b\u00e1sica de contraste y color. V1 luego alimenta de informaci\u00f3n a otras \u00e1reas, como V2, V4 y V5. Cada una de estas \u00e1reas se ocupa de los aspectos m\u00e1s espec\u00edficos o abstractas de la informaci\u00f3n. Por ejemplo, las  neuronas  en V4 se ocupan de objetos de mediana complejidad, tales como formas de estrellas en diferentes colores. La  corteza visual  de los animales es el m\u00e1s potente sistema de procesamiento visual que conocemos, por lo que suena l\u00f3gico inspirarse en ella para crear una variante de  redes neuronales artificiales  que ayude a identificar im\u00e1genes; es as\u00ed como surgen las  redes neuronales convolucionales .  Las  redes neuronales convolucionales  se componen de  neuronas  que tienen  pesos  y  sesgos  que pueden aprender. Cada  neurona  recibe algunas entradas, realiza un  producto escalar  y luego aplica una funci\u00f3n de activaci\u00f3n. Lo que diferencia a las  redes neuronales convolucionales  es que suponen expl\u00edcitamente que las entradas son im\u00e1genes, lo que nos permite codificar ciertas propiedades en la arquitectura; permitiendo ganar en eficiencia y reducir la cantidad de par\u00e1metros en la red. Las  redes neuronales convolucionales  trabajan modelando de forma consecutiva peque\u00f1as piezas de informaci\u00f3n, y luego combinando esta informaci\u00f3n en las capas m\u00e1s profundas de la red. Una manera de entenderlas es que la primera capa intentar\u00e1 detectar los bordes y establecer patrones de detecci\u00f3n de bordes. Luego, las capas posteriores trataran de combinarlos en formas m\u00e1s simples y, finalmente, en patrones de las diferentes posiciones de los objetos, iluminaci\u00f3n, escalas, etc. Las capas finales intentar\u00e1n hacer coincidir una imagen de entrada con todos los patrones y arribar a una predicci\u00f3n final como una suma ponderada de todos ellos. De esta forma las  redes neuronales convolucionales  son capaces de modelar complejas variaciones y comportamientos dando predicciones bastantes precisas.",
            "title": "Redes neuronales convolucionales"
        },
        {
            "location": "/vision-por-computadora/#estructura-de-las-redes-neuronales-convolucionales",
            "text": "En general, las  redes neuronales convolucionales  van a estar construidas con una estructura que contendr\u00e1 3 tipos distintos de capas:",
            "title": "Estructura de las Redes Neuronales Convolucionales"
        },
        {
            "location": "/vision-por-computadora/#capa-convolucional",
            "text": "Esta es la capa que le nombre a la red, lo que distingue a las  redes neuronales convolucionales  de cualquier otra  red neuronal  es utilizan un operaci\u00f3n llamada  convoluci\u00f3n  en alguna de sus capas; en lugar de utilizar la multiplicaci\u00f3n de matrices que se aplica generalmente.\nLa operaci\u00f3n de  convoluci\u00f3n  recibe como  entrada o input  la imagen y luego aplica sobre ella un  filtro o kernel  que nos devuelve un  mapa de las caracter\u00edsticas  de la imagen original, de esta forma logramos reducir el tama\u00f1o de los par\u00e1metros.\nLa  convoluci\u00f3n  aprovecha tres ideas importantes que pueden ayudar a mejorar cualquier sistema de  Machine Learning , ellas son:    interacciones dispersas , ya que al aplicar un filtro de menor tama\u00f1o sobre la entrada original podemos reducir dr\u00e1sticamente la cantidad de par\u00e1metros y c\u00e1lculos;  los  par\u00e1metros compartidos , que hace referencia a compartir los par\u00e1metros entre los distintos tipos de filtros, ayudando tambi\u00e9n a mejorar la eficiencia del sistema; y las  representaciones equivariante , que indican que si las entradas cambian, las salidas van a cambiar tambi\u00e9n en forma similar.   Por otra parte, la  convoluci\u00f3n  proporciona un medio para trabajar con entradas de tama\u00f1o variable, lo que puede ser tambi\u00e9n muy conveniente.",
            "title": "Capa convolucional"
        },
        {
            "location": "/vision-por-computadora/#capa-de-reduccion-o-pooling",
            "text": "La capa de reducci\u00f3n o  pooling  se coloca generalmente despu\u00e9s de la capa  convolucional . Su utilidad principal radica en la reducci\u00f3n de las dimensiones espaciales (ancho x alto) del volumen de entrada para la siguiente capa  convolucional . No afecta a la dimensi\u00f3n de profundidad del volumen.\nLa operaci\u00f3n realizada por esta capa tambi\u00e9n se llama  reducci\u00f3n de muestreo , ya que la reducci\u00f3n de tama\u00f1o conduce tambi\u00e9n a la p\u00e9rdida de informaci\u00f3n. Sin embargo, una p\u00e9rdida de este tipo puede ser beneficioso para la red por dos razones:   la disminuci\u00f3n en el tama\u00f1o conduce a una menor sobrecarga de c\u00e1lculo para las pr\u00f3ximas capas de la red;  tambi\u00e9n trabaja para reducir el  sobreajuste .   La operaci\u00f3n que se suele utilizar en esta capa es  max-pooling , que divide a la imagen de entrada en un conjunto de rect\u00e1ngulos y, respecto de cada subregi\u00f3n, se va quedando con el m\u00e1ximo valor.",
            "title": "Capa de reducci\u00f3n o pooling"
        },
        {
            "location": "/vision-por-computadora/#capa-clasificadora-totalmente-conectada",
            "text": "Al final de las capas  convolucional  y de  pooling , las redes utilizan generalmente capas completamente conectados en la que cada pixel se considera como una  neurona  separada al igual que en una  red neuronal  regular. Esta \u00faltima capa clasificadora tendr\u00e1 tantas  neuronas  como el n\u00famero de clases que se debe predecir.  Los modelos de  Deep Learning  basados en  redes neuronales convolucionales  han logrado resultados sorprendentes en varios problemas de  Visi\u00f3n por computadora , como ser los casos de clasificaci\u00f3n de im\u00e1genes y detecci\u00f3n de objetos.",
            "title": "Capa clasificadora totalmente conectada"
        },
        {
            "location": "/vision-por-computadora/#opencv",
            "text": "OpenCV  es una librer\u00eda  Open Source  de  Visi\u00f3n por computadora  que nos permite manipular im\u00e1genes y videos para realizar una variedad de tareas que van desde la detecci\u00f3n autom\u00e1tica de caras, a la visualizaci\u00f3n de las im\u00e1genes de una c\u00e1mara Web; o hasta ense\u00f1arle a un robot a reconocer objetos en la vida real. Fue creada en 1999, por Gary Bradski, quien trabajaba en  Intel , y que lanz\u00f3  OpenCV  con la intensi\u00f3n de acelerar la  Visi\u00f3n por computadora  y la  Inteligencia Artificial  proporcionando una infraestructura s\u00f3lida para todos los que trabajan en el campo. La librer\u00eda est\u00e1 escrita en  C  y  C++  y se puede ejecutar bajo  Linux , Windows y Mac OS X. Posee un desarrollo activo de interfaces para  Python ,  Java  , MATLAB y otros lenguajes, incluyendo el soporte para plataformas como  Android  e iOS para aplicaciones m\u00f3viles.   OpenCV  contiene m\u00e1s de 500 funciones que abarcan muchas \u00e1reas de  Visi\u00f3n por computadora , incluyendo tales como: inspecci\u00f3n de productos de f\u00e1brica, im\u00e1genes m\u00e9dicas, seguridad, interfaz de usuario, calibraci\u00f3n de c\u00e1mara, visi\u00f3n est\u00e9reo y rob\u00f3tica. Debido a que la  Visi\u00f3n por computadora  y el  Machine Learning  a menudo van de la mano,  OpenCV  tambi\u00e9n contiene una librer\u00eda completa de uso general de  Machine Learning  (ML); la cual se centra en el reconocimiento de patrones estad\u00edsticos y el agrupamiento.  OpenCV  es sin dudas el lugar por d\u00f3nde comenzar para empezar a incursionar en el mundo de la  Visi\u00f3n por computadora .",
            "title": "OpenCV"
        },
        {
            "location": "/interfaz-cerebro-computadora-BCI/",
            "text": "Interfaz cerebro computadora (BCI)\n\n\n\n\nDesde hace mucho tiempo la fantas\u00eda de poder comunicarnos con las m\u00e1quinas con solo utilizar nuestro pensamiento ha cautivado nuestra imaginaci\u00f3n. Estamos tan acostumbrados a nuestros cuerpos, que no tomamos conciencia de los complejos mecanismos biol\u00f3gicos, qu\u00edmicos y el\u00e9ctricos que ocurren entre nuestro cerebro y nuestro cuerpo para poder comunicarse f\u00e1cilmente e interactuar con el mundo. Los nuevos avances tecnol\u00f3gicos est\u00e1n haciendo cada vez m\u00e1s sencillo que nuestro cerebro pueda transcender nuestro cuerpo y logre comunicarse con el mundo exterior utilizando otros dispositivos. El incipiente campo de la \ninterfaz cerebro computadora\n o \nBCI\n, por sus siglas en ingl\u00e9s, consiste en el estudio multidisciplinario de los  avances en \nneurociencia\n, \nprocesamiento de se\u00f1ales\n, \nmachine learning\n y las \ntecnolog\u00edas de la informaci\u00f3n\n para explorar la forma de comunicar nuestro cerebro en forma directa con las m\u00e1quinas de la misma forma en que lo hacemos con nuestro cuerpo. \n\n\n\u00bfQu\u00e9 es la interfaz cerebro computadora?\n\n\nLa forma natural de comunicaci\u00f3n entre el cerebro y nuestro cuerpo requiere de nervios y m\u00fasculos. El proceso comienza con nuestra intenci\u00f3n. Esta intenci\u00f3n desencadena un proceso complejo en el que algunas \u00e1reas del cerebro son activadas y luego se env\u00edan se\u00f1ales a trav\u00e9s de nuestro \nsistema nervioso perif\u00e9rico\n a los m\u00fasculos correspondientes, que son los que en definitiva terminan realizando la tarea que comenz\u00f3 en nuestro pensamiento. La \ninterfaz cerebro computadora\n nos ofrece una alternativa a esta forma natural de comunicaci\u00f3n y control. Consiste en un sistema artificial, que en lugar de depender de nervios y m\u00fasculos, mide directamente la actividad cerebral asociada a nuestra intenci\u00f3n y la traduce en se\u00f1alas que pueden ser luego interpretadas por diferentes dispositivos. Esta traducci\u00f3n involucra \nprocesamiento de se\u00f1ales\n y \nreconocimiento de patrones\n que son generalmente realizadas por una computadora. \n\n\n\u00bfC\u00f3mo funciona?\n\n\nEl objetivo de los sistemas de \ninterfaz cerebro computadora\n es el de traducir la actividad cerebral en comandos de control para dispositivos y en estimular el cerebro para provocar una respuesta sensorial o recuperar una funci\u00f3n neuronal. En todo sistema \nBCI\n podemos encontrar una o varias de las siguientes etapas de procesamiento:\n\n\n\n\n\n\nObtenci\u00f3n de la actividad cerebral:\n Las se\u00f1ales de la actividad cerebral se obtiene utilizando tanto m\u00e9todos invasivos como no invasivos.\n\n\n\n\n\n\nProcesamiento de se\u00f1ales:\n Las se\u00f1ales en bruto son procesadas luego de ser adquiridas; se utilizan diferentes t\u00e9cnicas de \nReducci\u00f3n de dimensionalidad\n y de \nselecci\u00f3n de atributos\n para procesarlas. \n\n\n\n\n\n\nReconocimiento de patrones y \nmachine learning\n:\n En esta etapa se utilizan las t\u00e9cnicas de \nmachine learning\n para generar las se\u00f1ales de control para los dispositivos.\n\n\n\n\n\n\nRespuesta sensorial:\n La se\u00f1al de control de la etapa anterior genera un cambio en el entorno. Algunos de estos cambios pueden ser escuchados, vistos o sentidos por el usuario; pero en general se pueden utilizar sensores en el ambiente (sensores t\u00e1ctiles, c\u00e1maras, micr\u00f3fonos) para enviar una respuesta directa al cerebro v\u00eda estimulaci\u00f3n.\n\n\n\n\n\n\nProcesamiento de se\u00f1ales por estimulaci\u00f3n:\n Se requiere un buen entendimiento de la regi\u00f3n del cerebro que se quiere estimular para lograr la respuesta deseada; Aqu\u00ed se utilizan t\u00e9cnicas de \nprocesamiento de se\u00f1ales\n y \nmachine learning\n para intentar encontrar el correcto patr\u00f3n de estimulaci\u00f3n.\n\n\n\n\n\n\nEstimulaci\u00f3n cerebral:\n El patr\u00f3n de estimulaci\u00f3n obtenido en el punto anterior es utilizado en conjunci\u00f3n con t\u00e9cnicas invasivas o no invasivas de estimulaci\u00f3n para estimular el cerebro. \n\n\n\n\n\n\nA continuaci\u00f3n podemos ver un diagrama de como funciona una \ninterfaz cerebro computadora\n.\n\n\n\n\nFormas de medir la actividad cerebral\n\n\nLa actividad cerebral produce actividad el\u00e9ctrica y magn\u00e9tica. Por lo tanto, se pueden utilizar sensores para detectar diferentes tipos de cambios en la actividad el\u00e9ctrica o magn\u00e9tica, en diferentes momentos,sobre diferentes \u00e1reas del cerebro para ,de esta forma, poder estudiar la actividad cerebral.\nLa mayor\u00eda de los sistemas \nBCI\n se basan en mediciones el\u00e9ctricas de actividad cerebral y dependen de sensores colocados sobre la cabeza para medir esta actividad. La \nelectroencefalograf\u00eda (EEG)\n es una de las principales herramientas que se utiliza y consiste en capturar la actividad cerebral por medio de colocar unos electrodos sobre el cuero cabelludo. Otros dispositivos, pero ya no tan comunes por su alto costo que se pueden utilizar para medir la actividad cerebral son: la \nmagnetoencefalograf\u00eda (MEG)\n, la cual registra los campos magn\u00e9ticos asociados con la actividad cerebral; La \nresonancia magn\u00e9tica funcional (fMRI)\n que mide peque\u00f1os cambios en las se\u00f1ales dependientes del nivel de oxigenaci\u00f3n de la sangre (BOLD) asociadas con la activaci\u00f3n cortical. Al igual que la fMRI, la \nespectroscop\u00eda de infrarrojo cercano (NIRS)\n es una t\u00e9cnica hemodin\u00e1mica para la evaluaci\u00f3n de la actividad funcional en la corteza humana.\n\n\n\n\nAplicaciones\n\n\nLos sistemas \nBCI\n pueden tener muchas aplicaciones terap\u00e9uticas, por ejemplo se pueden utilizar para proporcionar comunicaci\u00f3n y control a usuarios con discapacidades leves, e incluso a usuarios sanos en algunas situaciones. Los sistemas \nBCI\n tambi\u00e9n pueden proporcionar nuevos medios para tratar los accidentes cerebrovasculares, el autismo y otros trastornos cerebrales. \n\n\nAqu\u00ed concluye esta introducci\u00f3n a la \ninterfaz cerebro computadora\n el reciente resurgimiento del inter\u00e9s en este campo se debe principalmente  a que existen computadoras m\u00e1s r\u00e1pidas y baratas; a los avances en el conocimiento de c\u00f3mo el cerebro procesa la informaci\u00f3n sensorial y produce una salida motriz; a la mayor disponibilidad de dispositivos para registrar se\u00f1ales cerebrales; y a las t\u00e9cnicas cada vez m\u00e1s poderosas de \nprocesamiento de se\u00f1ales\n y \nalgoritmos\n de \nmachine learning\n.\n\n\nLes dejo el link de un demo sobre \nBCI\n realizada por \nGabriel Maggiotti\n un miembro de la comuindad \nIAAR\n\n\nLa demo de mi charla de hoy en la UTN.  \n#DeepLearning\n \n#ML\n \n#BCI\n \npic.twitter.com/QdHLoj2I8p\n\u2014 Gabriel Maggiotti (@gmaggiotti) \n25 de noviembre de 2017",
            "title": "Interfaz Cerebro Computadora (BCI)"
        },
        {
            "location": "/interfaz-cerebro-computadora-BCI/#interfaz-cerebro-computadora-bci",
            "text": "Desde hace mucho tiempo la fantas\u00eda de poder comunicarnos con las m\u00e1quinas con solo utilizar nuestro pensamiento ha cautivado nuestra imaginaci\u00f3n. Estamos tan acostumbrados a nuestros cuerpos, que no tomamos conciencia de los complejos mecanismos biol\u00f3gicos, qu\u00edmicos y el\u00e9ctricos que ocurren entre nuestro cerebro y nuestro cuerpo para poder comunicarse f\u00e1cilmente e interactuar con el mundo. Los nuevos avances tecnol\u00f3gicos est\u00e1n haciendo cada vez m\u00e1s sencillo que nuestro cerebro pueda transcender nuestro cuerpo y logre comunicarse con el mundo exterior utilizando otros dispositivos. El incipiente campo de la  interfaz cerebro computadora  o  BCI , por sus siglas en ingl\u00e9s, consiste en el estudio multidisciplinario de los  avances en  neurociencia ,  procesamiento de se\u00f1ales ,  machine learning  y las  tecnolog\u00edas de la informaci\u00f3n  para explorar la forma de comunicar nuestro cerebro en forma directa con las m\u00e1quinas de la misma forma en que lo hacemos con nuestro cuerpo.",
            "title": "Interfaz cerebro computadora (BCI)"
        },
        {
            "location": "/interfaz-cerebro-computadora-BCI/#que-es-la-interfaz-cerebro-computadora",
            "text": "La forma natural de comunicaci\u00f3n entre el cerebro y nuestro cuerpo requiere de nervios y m\u00fasculos. El proceso comienza con nuestra intenci\u00f3n. Esta intenci\u00f3n desencadena un proceso complejo en el que algunas \u00e1reas del cerebro son activadas y luego se env\u00edan se\u00f1ales a trav\u00e9s de nuestro  sistema nervioso perif\u00e9rico  a los m\u00fasculos correspondientes, que son los que en definitiva terminan realizando la tarea que comenz\u00f3 en nuestro pensamiento. La  interfaz cerebro computadora  nos ofrece una alternativa a esta forma natural de comunicaci\u00f3n y control. Consiste en un sistema artificial, que en lugar de depender de nervios y m\u00fasculos, mide directamente la actividad cerebral asociada a nuestra intenci\u00f3n y la traduce en se\u00f1alas que pueden ser luego interpretadas por diferentes dispositivos. Esta traducci\u00f3n involucra  procesamiento de se\u00f1ales  y  reconocimiento de patrones  que son generalmente realizadas por una computadora.",
            "title": "\u00bfQu\u00e9 es la interfaz cerebro computadora?"
        },
        {
            "location": "/interfaz-cerebro-computadora-BCI/#como-funciona",
            "text": "El objetivo de los sistemas de  interfaz cerebro computadora  es el de traducir la actividad cerebral en comandos de control para dispositivos y en estimular el cerebro para provocar una respuesta sensorial o recuperar una funci\u00f3n neuronal. En todo sistema  BCI  podemos encontrar una o varias de las siguientes etapas de procesamiento:    Obtenci\u00f3n de la actividad cerebral:  Las se\u00f1ales de la actividad cerebral se obtiene utilizando tanto m\u00e9todos invasivos como no invasivos.    Procesamiento de se\u00f1ales:  Las se\u00f1ales en bruto son procesadas luego de ser adquiridas; se utilizan diferentes t\u00e9cnicas de  Reducci\u00f3n de dimensionalidad  y de  selecci\u00f3n de atributos  para procesarlas.     Reconocimiento de patrones y  machine learning :  En esta etapa se utilizan las t\u00e9cnicas de  machine learning  para generar las se\u00f1ales de control para los dispositivos.    Respuesta sensorial:  La se\u00f1al de control de la etapa anterior genera un cambio en el entorno. Algunos de estos cambios pueden ser escuchados, vistos o sentidos por el usuario; pero en general se pueden utilizar sensores en el ambiente (sensores t\u00e1ctiles, c\u00e1maras, micr\u00f3fonos) para enviar una respuesta directa al cerebro v\u00eda estimulaci\u00f3n.    Procesamiento de se\u00f1ales por estimulaci\u00f3n:  Se requiere un buen entendimiento de la regi\u00f3n del cerebro que se quiere estimular para lograr la respuesta deseada; Aqu\u00ed se utilizan t\u00e9cnicas de  procesamiento de se\u00f1ales  y  machine learning  para intentar encontrar el correcto patr\u00f3n de estimulaci\u00f3n.    Estimulaci\u00f3n cerebral:  El patr\u00f3n de estimulaci\u00f3n obtenido en el punto anterior es utilizado en conjunci\u00f3n con t\u00e9cnicas invasivas o no invasivas de estimulaci\u00f3n para estimular el cerebro.     A continuaci\u00f3n podemos ver un diagrama de como funciona una  interfaz cerebro computadora .",
            "title": "\u00bfC\u00f3mo funciona?"
        },
        {
            "location": "/interfaz-cerebro-computadora-BCI/#formas-de-medir-la-actividad-cerebral",
            "text": "La actividad cerebral produce actividad el\u00e9ctrica y magn\u00e9tica. Por lo tanto, se pueden utilizar sensores para detectar diferentes tipos de cambios en la actividad el\u00e9ctrica o magn\u00e9tica, en diferentes momentos,sobre diferentes \u00e1reas del cerebro para ,de esta forma, poder estudiar la actividad cerebral.\nLa mayor\u00eda de los sistemas  BCI  se basan en mediciones el\u00e9ctricas de actividad cerebral y dependen de sensores colocados sobre la cabeza para medir esta actividad. La  electroencefalograf\u00eda (EEG)  es una de las principales herramientas que se utiliza y consiste en capturar la actividad cerebral por medio de colocar unos electrodos sobre el cuero cabelludo. Otros dispositivos, pero ya no tan comunes por su alto costo que se pueden utilizar para medir la actividad cerebral son: la  magnetoencefalograf\u00eda (MEG) , la cual registra los campos magn\u00e9ticos asociados con la actividad cerebral; La  resonancia magn\u00e9tica funcional (fMRI)  que mide peque\u00f1os cambios en las se\u00f1ales dependientes del nivel de oxigenaci\u00f3n de la sangre (BOLD) asociadas con la activaci\u00f3n cortical. Al igual que la fMRI, la  espectroscop\u00eda de infrarrojo cercano (NIRS)  es una t\u00e9cnica hemodin\u00e1mica para la evaluaci\u00f3n de la actividad funcional en la corteza humana.",
            "title": "Formas de medir la actividad cerebral"
        },
        {
            "location": "/interfaz-cerebro-computadora-BCI/#aplicaciones",
            "text": "Los sistemas  BCI  pueden tener muchas aplicaciones terap\u00e9uticas, por ejemplo se pueden utilizar para proporcionar comunicaci\u00f3n y control a usuarios con discapacidades leves, e incluso a usuarios sanos en algunas situaciones. Los sistemas  BCI  tambi\u00e9n pueden proporcionar nuevos medios para tratar los accidentes cerebrovasculares, el autismo y otros trastornos cerebrales.   Aqu\u00ed concluye esta introducci\u00f3n a la  interfaz cerebro computadora  el reciente resurgimiento del inter\u00e9s en este campo se debe principalmente  a que existen computadoras m\u00e1s r\u00e1pidas y baratas; a los avances en el conocimiento de c\u00f3mo el cerebro procesa la informaci\u00f3n sensorial y produce una salida motriz; a la mayor disponibilidad de dispositivos para registrar se\u00f1ales cerebrales; y a las t\u00e9cnicas cada vez m\u00e1s poderosas de  procesamiento de se\u00f1ales  y  algoritmos  de  machine learning .  Les dejo el link de un demo sobre  BCI  realizada por  Gabriel Maggiotti  un miembro de la comuindad  IAAR  La demo de mi charla de hoy en la UTN.   #DeepLearning   #ML   #BCI   pic.twitter.com/QdHLoj2I8p \u2014 Gabriel Maggiotti (@gmaggiotti)  25 de noviembre de 2017",
            "title": "Aplicaciones"
        },
        {
            "location": "/python/",
            "text": "Introducci\u00f3n a Python\n\n\n\n\nPython\n es actualmente uno de los lenguajes m\u00e1s utilizados en \ninteligencia artificial\n y \nCiencia de datos\n; es un lenguaje de programaci\u00f3n de alto nivel que se caracteriza por hacer hincapi\u00e9 en una sintaxis limpia, que favorece un c\u00f3digo legible y f\u00e1cilmente administrable. \nPython\n funciona en las plataformas Windows, Linux/Unix, Mac OS X e incluso ha sido portado a las m\u00e1quinas virtuales de \nJava\n (a trav\u00e9s de \nJython\n) y \n.Net\n (a trav\u00e9s de \nIronPython\n). \nPython\n es un lenguaje libre y f\u00e1cil de aprender que te permite trabajar m\u00e1s r\u00e1pido e integrar tus sistemas de manera m\u00e1s eficaz; con \nPython\n se puede ganar r\u00e1pidamente en productividad.\n\n\nPython\n, a diferencia de otros lenguajes de programaci\u00f3n como \nC\n, \nC++\n o \nJava\n es \ninterpretado y din\u00e1micamente tipado\n; lo que quiere decir que no es necesario compilar el fuente para poder ejecutarlo (\ninterpretado\n) y que sus variables pueden tomar distintos tipos de objetos (\ndin\u00e1micamente tipado\n); esto hace que el lenguaje sea sumamente flexible y de r\u00e1pida implementaci\u00f3n; aunque pierde en rendimiento y es m\u00e1s propenso a errores de programaci\u00f3n que los lenguajes antes mencionados.\n\n\nPrincipales fortalezas de Python\n\n\nLas principales fortalezas que hacen que uno ame a \nPython\n son:\n\n\n\n\n\n\nEs Orientado a Objetos.\n \nPython\n es un lenguaje de programaci\u00f3n \nOrientado a Objetos\n desde casi su concepci\u00f3n, su modelo de clases soporta las notaciones avanzadas de polimorfismo, sobrecarga de operadores y herencia m\u00faltiple. La programaci\u00f3n \nOrientado a Objetos\n es sumamente f\u00e1cil de aplicar con la sintaxis simple que nos proporciona \nPython\n. Asimismo, tambi\u00e9n es importante destacar que en \nPython\n, la programaci\u00f3n \nOrientado a Objetos\n es una opci\u00f3n y no algo obligatorio como es en \nJava\n; ya que \nPython\n es \nmultiparadigma\n y nos permite programar siguiendo un modelo \nOrientado a Objetos\n o un modelo \nimperativo\n.\n\n\n\n\n\n\nEs software libre\n. \nPython\n es completamente libre para ser utilizado y redistribuido; no posee restricciones para copiarlo, embeberlo en nuestros sistemas o ser vendido junto con otros productos. \nPython\n es un proyecto \nopen source\n que es administrado por \nPython Software Foundation\n, instituci\u00f3n que se encarga de su soporte y desarrollo.\n\n\n\n\n\n\nEs portable\n. La implementaci\u00f3n est\u00e1ndar de \nPython\n esta escrita en \nC\n, y puede ser compilada y ejecutada en pr\u00e1cticamente cualquier plataforma que se les ocurra. Podemos encontrar a \nPython\n en peque\u00f1os dispositivos, como tel\u00e9fonos celulares, hasta grandes infraestructuras de \nHardware\n, como las supercomputadoras. Al ser un lenguaje \ninterpretado\n el mismo \nc\u00f3digo fuente\n puede ser ejecutado en cualquier plataforma sin necesidad de realizar grandes cambios.\n\n\n\n\n\n\nEs poderoso\n. \nPython\n proporciona toda la sencillez y facilidad de uso de un lenguaje de programaci\u00f3n \ninterpretado\n, junto con las m\u00e1s avanzadas herramientas de ingenier\u00eda de software que se encuentran t\u00edpicamente en los lenguajes compilados. A diferencia de otros lenguajes \ninterpretados\n, esta combinaci\u00f3n hace a \nPython\n sumamente \u00fatil para proyectos de desarrollo a gran escala.\n\n\n\n\n\n\nF\u00e1cil integraci\u00f3n\n. Los programas escritos en \nPython\n pueden ser f\u00e1cilmente integrados con componentes escritos en otros lenguajes. Por ejemplo la \nC\n \nAPI\n de \nPython\n permite una f\u00e1cil integraci\u00f3n entre los dos lenguajes, permitiendo que los programas escritos en \nPython\n puedan llamar a funciones escritas en \nC\n y viceversa. \n\n\n\n\n\n\nF\u00e1cil de usar\n. Para ejecutar un programa en \nPython\n simplemente debemos escribirlo y ejecutarlo, no existen pasos intermedios de linkeo o compilaci\u00f3n como podemos tener en otros lenguajes de programaci\u00f3n. Con \nPython\n podemos programar en forma interactiva, basta tipear una sentencia para poder ver inmediatamente el resultado. Adem\u00e1s los programas en \nPython\n son m\u00e1s simples, m\u00e1s peque\u00f1os y m\u00e1s flexibles que los programas equivalentes en lenguajes como \nC\n, \nC++\n o \nJava\n.\n\n\n\n\n\n\nF\u00e1cil de aprender\n. Desde mi punto de vista, esta es sin duda la principal fortaleza del lenguaje; comparado con otros lenguajes de programaci\u00f3n, \nPython\n es sumamente f\u00e1cil de aprender, en tan s\u00f3lo un par de d\u00edas se puede estar programando eficientemente con \nPython\n.\n\n\n\n\n\n\nInstalando Python\n\n\nEn Linux\n\n\nInstalar \nPython\n en \nLinux\n no es necesario, ya que viene preinstalado en todas las distribuciones m\u00e1s populares.\n\n\nEn Windows\n\n\nLa forma m\u00e1s sencilla de poder instalar \nPython\n en Windows es instalando alguna de las distribuciones de \nPython\n que ya vienen armadas con los principales m\u00f3dulos. Yo les recomiendo la distribuci\u00f3n \nAnaconda\n, que se puede descargar en forma gratuita y viene integrada con todos los principales paquetes que vamos a necesitar para trabajar con \nPython\n. Una vez que la descargan, simplemente siguen los pasos del instalador y listo, ya tendr\u00e1n todo un ambiente \nPython\n para trabajar en Windows.\n\n\nOtra distribuci\u00f3n de \nPython\n que pueden utilizar en Windows, es \nWinPython\n, la cual puede ser utilizada incluso en forma portable.\n\n\nLibrer\u00edas esenciales para el an\u00e1lisis de datos\n\n\nNumpy\n\n\nNumpy\n, abreviatura de Numerical \nPython\n , es el paquete fundamental para la computaci\u00f3n cient\u00edfica en \nPython\n. Dispone, entre otras cosas de:\n\n\n\n\nUn objeto \nmatriz\n multidimensional \nndarray\n,r\u00e1pido y eficiente.\n\n\nFunciones para realizar c\u00e1lculos elemento a elemento u otras operaciones matem\u00e1ticas con \nmatrices\n. \n\n\nHerramientas para la lectura y escritura de los conjuntos de datos basados \nmatrices\n.\n\n\nOperaciones de \n\u00e1lgebra lineal\n, \ntransformaciones de Fourier\n, y generaci\u00f3n de n\u00fameros aleatorios.\n\n\nHerramientas de integraci\u00f3n para conectar \nC\n, \nC++\n y \nFortran\n con \nPython\n\n\n\n\nM\u00e1s all\u00e1 de las capacidades de procesamiento r\u00e1pido de \nmatrices\n que \nNumpy\n a\u00f1ade a \nPython\n, uno de sus\nprop\u00f3sitos principales con respecto al an\u00e1lisis de datos es la utilizaci\u00f3n de sus \nestructuras de datos\n como contenedores para transmitir los datos entre diferentes algoritmos. Para datos num\u00e9ricos , las \nmatrices\n de \nNumpy\n son una forma mucho m\u00e1s eficiente de almacenar y manipular datos que cualquier otra de las \nestructuras de datos\n est\u00e1ndar incorporadas en \nPython\n. Asimismo, librer\u00edas escritas en un lenguaje de bajo nivel, como \nC\n o \nFortran\n, pueden operar en los datos almacenados en \nmatrices\n de \nNumpy\n sin necesidad de copiar o modificar ning\u00fan dato.\n\n\nJupyter\n\n\nJupyter\n promueve un ambiente de trabajo de \nejecutar-explorar\n en contraposici\u00f3n al tradicional modelo de desarrollo de software de \neditar-compilar-ejecutar\n. Es decir, que el problema computacional a resolver es m\u00e1s visto como todo un proceso de ejecuci\u00f3n de tareas, en lugar del tradicional modelo de producir una respuesta(\noutput\n) a una pregunta(\ninput\n).  \nJupyter\n tambi\u00e9n provee una estrecha integraci\u00f3n con nuestro sistema operativo, permitiendo acceder f\u00e1cilmente a todos nuestros archivos desde la misma herramienta.\n\n\nAlgunas de las caracter\u00edsticas sobresalientes de \nJupyter\n son:\n\n\n\n\nSu poderoso \nshell\n interactivo con soporte para m\u00faltiples lenguajes.\n\n\nNotebook\n, su interfase web con soporte para c\u00f3digo, texto, expresiones matem\u00e1ticas, gr\u00e1ficos en l\u00ednea y multimedia.\n\n\nSu soporte para poder realizar visualizaciones de datos en forma interactiva. \nJupyter\n esta totalmente integrado con \nmatplotlib\n.\n\n\nSu simple y flexible interfase para trabajar con la \ncomputaci\u00f3n paralela\n.\n\n\n\n\nMatplotlib\n\n\nMatplotlib\n es la librer\u00eda m\u00e1s popular en \nPython\n para visualizaciones y gr\u00e1ficos. \nMatplotlib\n puede producir gr\u00e1ficos de alta calidad dignos de cualquier publicaci\u00f3n cient\u00edfica.\n\n\nAlgunas de las muchas ventajas que nos ofrece \nMatplotlib\n, incluyen:\n\n\n\n\nEs f\u00e1cil de aprender.\n\n\nSoporta texto, t\u00edtulos y etiquetas en formato $\\LaTeX$.\n\n\nProporciona un gran control sobre cada uno de los elementos de las figuras, como ser su tama\u00f1o, el trazado de sus l\u00edneas, etc.\n\n\nNos permite crear gr\u00e1ficos y figuras de gran calidad que pueden ser guardados en varios formatos, como ser: PNG, PDF, SVG, EPS, y PGF.\n\n\n\n\nMatplotlib\n se integra de maravilla con \nJupyter\n (ver m\u00e1s abajo), lo que nos proporciona un ambiente confortable para las visualizaciones y la exploraci\u00f3n de datos interactiva.\n\n\nPandas\n\n\nPandas\n es una librer\u00eda \nopen source\n que aporta a \nPython\n unas estructuras de datos f\u00e1ciles de usar y de alta performance, junto con un gran n\u00famero de funciones esenciales para el an\u00e1lisis de datos. Con la ayuda de \nPandas\n podemos trabajar con \ndatos estructurados\n de una forma m\u00e1s r\u00e1pida y expresiva.\n\n\nAlgunas de las cosas sobresalientes que nos aporta \nPandas\n son:\n\n\n\n\nUn r\u00e1pido y eficiente objeto \nDataFrame\n para manipular datos con indexaci\u00f3n integrada;\n\n\nherramientas para la \nlectura y escritura de datos\n entre estructuras de datos r\u00e1pidas y eficientes manejadas en memoria, como el \nDataFrame\n, con la mayor\u00eda de los formatos conocidos para el manejo de datos, como ser: CSV y archivos de texto, archivos Microsoft Excel, bases de datos \nSQL\n, y el formato cient\u00edfico HDF5.\n\n\nProporciona una \nalineaci\u00f3n inteligente de datos\n y un manejo integrado de los datos faltantes; con estas funciones podemos obtener una ganancia de performance en los c\u00e1lculos entre \nDataFrames\n y una f\u00e1cil manipulaci\u00f3n y ordenamiento de los datos de nuestro \ndata set\n;\n\n\nFlexibilidad para \nmanipular y redimensionar\n nuestro \ndata set\n, facilidad para construir \ntablas pivote\n;\n\n\nLa posibilidad de \nfiltrar los datos, agregar o eliminar columnas\n de una forma sumamente expresiva;\n\n\nOperaciones de \nmerge\n y *join\n* altamente eficientes sobre nuestros conjuntos de datos;\n\n\nIndexaci\u00f3n jer\u00e1rquica\n que proporciona una forma intuitiva de trabajar con datos de alta dimensi\u00f3n en una estructura de datos de menor dimensi\u00f3n ;\n\n\nPosibilidad de realizar c\u00e1lculos agregados o transformaciones de datos con el poderoso motor \ngroup by\n que nos permite dividir-aplicar-combinar nuestros conjuntos de datos;\n\n\ncombina las \ncaracter\u00edsticas de las matrices de alto rendimiento de \nNumpy\n con las flexibles capacidades de manipulaci\u00f3n de datos de las hojas de c\u00e1lculo\n y bases de datos relacionales (tales como \nSQL\n);\n\n\nGran n\u00famero de funcionalidades para el manejo de \nseries de tiempo\n ideales para el an\u00e1lisis financiero;\n\n\nTodas sus funciones y estructuras de datos est\u00e1n \noptimizadas para el alto rendimiento\n, con las partes cr\u00edticas del c\u00f3digo escritas en \nCython\n o \nC\n.\n\n\n\n\nScikit-Lean\n\n\nScikit-learn\n es una librer\u00eda especializada en algoritmos para \ndata mining\n y \nmachine learning\n.  \n\n\nAlgunos de los problemas que podemos resolver utilizando las herramientas de \nScikit-learn\n, son:\n\n\n\n\nClasificaciones\n: Identificar las categor\u00edas a que cada observaci\u00f3n del conjunto de datos pertenece.\n\n\nRegresiones\n: Predecir el valor continuo para cada nuevo ejemplo.\n\n\nAgrupaciones\n: Agrupaci\u00f3n autom\u00e1tica de objetos similares en un conjunto.\n\n\nReducci\u00f3n de dimensiones\n: Reducir el n\u00famero de variables aleatorias a considerar.\n\n\nSelecci\u00f3n de Modelos\n: Comparar, validar y elegir par\u00e1metros y modelos.\n\n\nPreprocesamiento\n: Extracci\u00f3n de caracter\u00edsticas a analizar y normalizaci\u00f3n de datos.\n\n\n\n\nSciPy\n\n\nSciPy\n es un conjunto de paquetes donde cada uno ellos ataca un problema distinto dentro de la computaci\u00f3n cient\u00edfica y el an\u00e1lisis num\u00e9rico. Algunos de los paquetes que incluye, son:\n\n\n\n\nscipy.integrate\n: que proporciona diferentes funciones para resolver problemas de integraci\u00f3n num\u00e9rica.\n\n\nscipy.linalg\n: que proporciona funciones para resolver problemas de \u00e1lgebra lineal.\n\n\nscipy.optimize\n: para los problemas de optimizaci\u00f3n y minimizaci\u00f3n.\n\n\nscipy.signal\n: para el an\u00e1lisis y procesamiento de se\u00f1ales.\n\n\nscipy.sparse\n: para matrices dispersas y solucionar sistemas lineales dispersos\n\n\nscipy.stats\n: para el an\u00e1lisis de estad\u00edstica y probabilidades.\n\n\n\n\nFrameworks para Deep Learning\n\n\nEn estos momentos, si hay un campo en donde \nPython\n sobresale sobre cualquier otro lenguaje, es en su soporte para frameworks de \nDeep Learning\n. Existen una gran variedad y de muy buena calidad, entre los que se destacan:\n\n\n\n\n\n\nTensorFlow\n: \nTensorFlow\n es un frameworks desarrollado por Google. Es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos. \n\n\n\n\n\n\nPyTorch\n: \nPyTorch\n es un framework de \nDeep Learning\n que utiliza el lenguaje \nPython\n y cuenta con el apoyo de Facebook.\n\n\n\n\n\n\nTheano\n: \nTheano\n es una librer\u00eda de \nPython\n que permite definir, optimizar y evaluar expresiones matem\u00e1ticas que involucran tensores de manera eficiente. \n\n\n\n\n\n\nCNTK\n: \nCNTK\n es un conjunto de herramientas, desarrolladas por Microsoft, f\u00e1ciles de usar, de c\u00f3digo abierto que entrena algoritmos de \nDeep Learning\n para aprender como el cerebro humano.\n\n\n\n\n\n\nKeras\n: \nKeras\n es una librer\u00eda de alto nivel, muy f\u00e1cil de utilizar. Est\u00e1 escrita y mantenida por Francis Chollet, miembro del equipo de Google Brain. Permite a los usuarios elegir si los modelos que se construyen ser\u00e1n ejecutados en el grafo simb\u00f3lico de \nTheano\n, \nTensorFlow\n o \nCNTK\n.\n\n\n\n\n\n\nMXNet\n: \nMXNet\n es una librer\u00eda flexible y eficiente para armar modelos de \nDeep Learning\n con soporte para varios idiomas.",
            "title": "Intro Python"
        },
        {
            "location": "/python/#introduccion-a-python",
            "text": "Python  es actualmente uno de los lenguajes m\u00e1s utilizados en  inteligencia artificial  y  Ciencia de datos ; es un lenguaje de programaci\u00f3n de alto nivel que se caracteriza por hacer hincapi\u00e9 en una sintaxis limpia, que favorece un c\u00f3digo legible y f\u00e1cilmente administrable.  Python  funciona en las plataformas Windows, Linux/Unix, Mac OS X e incluso ha sido portado a las m\u00e1quinas virtuales de  Java  (a trav\u00e9s de  Jython ) y  .Net  (a trav\u00e9s de  IronPython ).  Python  es un lenguaje libre y f\u00e1cil de aprender que te permite trabajar m\u00e1s r\u00e1pido e integrar tus sistemas de manera m\u00e1s eficaz; con  Python  se puede ganar r\u00e1pidamente en productividad.  Python , a diferencia de otros lenguajes de programaci\u00f3n como  C ,  C++  o  Java  es  interpretado y din\u00e1micamente tipado ; lo que quiere decir que no es necesario compilar el fuente para poder ejecutarlo ( interpretado ) y que sus variables pueden tomar distintos tipos de objetos ( din\u00e1micamente tipado ); esto hace que el lenguaje sea sumamente flexible y de r\u00e1pida implementaci\u00f3n; aunque pierde en rendimiento y es m\u00e1s propenso a errores de programaci\u00f3n que los lenguajes antes mencionados.",
            "title": "Introducci\u00f3n a Python"
        },
        {
            "location": "/python/#principales-fortalezas-de-python",
            "text": "Las principales fortalezas que hacen que uno ame a  Python  son:    Es Orientado a Objetos.   Python  es un lenguaje de programaci\u00f3n  Orientado a Objetos  desde casi su concepci\u00f3n, su modelo de clases soporta las notaciones avanzadas de polimorfismo, sobrecarga de operadores y herencia m\u00faltiple. La programaci\u00f3n  Orientado a Objetos  es sumamente f\u00e1cil de aplicar con la sintaxis simple que nos proporciona  Python . Asimismo, tambi\u00e9n es importante destacar que en  Python , la programaci\u00f3n  Orientado a Objetos  es una opci\u00f3n y no algo obligatorio como es en  Java ; ya que  Python  es  multiparadigma  y nos permite programar siguiendo un modelo  Orientado a Objetos  o un modelo  imperativo .    Es software libre .  Python  es completamente libre para ser utilizado y redistribuido; no posee restricciones para copiarlo, embeberlo en nuestros sistemas o ser vendido junto con otros productos.  Python  es un proyecto  open source  que es administrado por  Python Software Foundation , instituci\u00f3n que se encarga de su soporte y desarrollo.    Es portable . La implementaci\u00f3n est\u00e1ndar de  Python  esta escrita en  C , y puede ser compilada y ejecutada en pr\u00e1cticamente cualquier plataforma que se les ocurra. Podemos encontrar a  Python  en peque\u00f1os dispositivos, como tel\u00e9fonos celulares, hasta grandes infraestructuras de  Hardware , como las supercomputadoras. Al ser un lenguaje  interpretado  el mismo  c\u00f3digo fuente  puede ser ejecutado en cualquier plataforma sin necesidad de realizar grandes cambios.    Es poderoso .  Python  proporciona toda la sencillez y facilidad de uso de un lenguaje de programaci\u00f3n  interpretado , junto con las m\u00e1s avanzadas herramientas de ingenier\u00eda de software que se encuentran t\u00edpicamente en los lenguajes compilados. A diferencia de otros lenguajes  interpretados , esta combinaci\u00f3n hace a  Python  sumamente \u00fatil para proyectos de desarrollo a gran escala.    F\u00e1cil integraci\u00f3n . Los programas escritos en  Python  pueden ser f\u00e1cilmente integrados con componentes escritos en otros lenguajes. Por ejemplo la  C   API  de  Python  permite una f\u00e1cil integraci\u00f3n entre los dos lenguajes, permitiendo que los programas escritos en  Python  puedan llamar a funciones escritas en  C  y viceversa.     F\u00e1cil de usar . Para ejecutar un programa en  Python  simplemente debemos escribirlo y ejecutarlo, no existen pasos intermedios de linkeo o compilaci\u00f3n como podemos tener en otros lenguajes de programaci\u00f3n. Con  Python  podemos programar en forma interactiva, basta tipear una sentencia para poder ver inmediatamente el resultado. Adem\u00e1s los programas en  Python  son m\u00e1s simples, m\u00e1s peque\u00f1os y m\u00e1s flexibles que los programas equivalentes en lenguajes como  C ,  C++  o  Java .    F\u00e1cil de aprender . Desde mi punto de vista, esta es sin duda la principal fortaleza del lenguaje; comparado con otros lenguajes de programaci\u00f3n,  Python  es sumamente f\u00e1cil de aprender, en tan s\u00f3lo un par de d\u00edas se puede estar programando eficientemente con  Python .",
            "title": "Principales fortalezas de Python"
        },
        {
            "location": "/python/#instalando-python",
            "text": "",
            "title": "Instalando Python"
        },
        {
            "location": "/python/#en-linux",
            "text": "Instalar  Python  en  Linux  no es necesario, ya que viene preinstalado en todas las distribuciones m\u00e1s populares.",
            "title": "En Linux"
        },
        {
            "location": "/python/#en-windows",
            "text": "La forma m\u00e1s sencilla de poder instalar  Python  en Windows es instalando alguna de las distribuciones de  Python  que ya vienen armadas con los principales m\u00f3dulos. Yo les recomiendo la distribuci\u00f3n  Anaconda , que se puede descargar en forma gratuita y viene integrada con todos los principales paquetes que vamos a necesitar para trabajar con  Python . Una vez que la descargan, simplemente siguen los pasos del instalador y listo, ya tendr\u00e1n todo un ambiente  Python  para trabajar en Windows.  Otra distribuci\u00f3n de  Python  que pueden utilizar en Windows, es  WinPython , la cual puede ser utilizada incluso en forma portable.",
            "title": "En Windows"
        },
        {
            "location": "/python/#librerias-esenciales-para-el-analisis-de-datos",
            "text": "",
            "title": "Librer\u00edas esenciales para el an\u00e1lisis de datos"
        },
        {
            "location": "/python/#numpy",
            "text": "Numpy , abreviatura de Numerical  Python  , es el paquete fundamental para la computaci\u00f3n cient\u00edfica en  Python . Dispone, entre otras cosas de:   Un objeto  matriz  multidimensional  ndarray ,r\u00e1pido y eficiente.  Funciones para realizar c\u00e1lculos elemento a elemento u otras operaciones matem\u00e1ticas con  matrices .   Herramientas para la lectura y escritura de los conjuntos de datos basados  matrices .  Operaciones de  \u00e1lgebra lineal ,  transformaciones de Fourier , y generaci\u00f3n de n\u00fameros aleatorios.  Herramientas de integraci\u00f3n para conectar  C ,  C++  y  Fortran  con  Python   M\u00e1s all\u00e1 de las capacidades de procesamiento r\u00e1pido de  matrices  que  Numpy  a\u00f1ade a  Python , uno de sus\nprop\u00f3sitos principales con respecto al an\u00e1lisis de datos es la utilizaci\u00f3n de sus  estructuras de datos  como contenedores para transmitir los datos entre diferentes algoritmos. Para datos num\u00e9ricos , las  matrices  de  Numpy  son una forma mucho m\u00e1s eficiente de almacenar y manipular datos que cualquier otra de las  estructuras de datos  est\u00e1ndar incorporadas en  Python . Asimismo, librer\u00edas escritas en un lenguaje de bajo nivel, como  C  o  Fortran , pueden operar en los datos almacenados en  matrices  de  Numpy  sin necesidad de copiar o modificar ning\u00fan dato.",
            "title": "Numpy"
        },
        {
            "location": "/python/#jupyter",
            "text": "Jupyter  promueve un ambiente de trabajo de  ejecutar-explorar  en contraposici\u00f3n al tradicional modelo de desarrollo de software de  editar-compilar-ejecutar . Es decir, que el problema computacional a resolver es m\u00e1s visto como todo un proceso de ejecuci\u00f3n de tareas, en lugar del tradicional modelo de producir una respuesta( output ) a una pregunta( input ).   Jupyter  tambi\u00e9n provee una estrecha integraci\u00f3n con nuestro sistema operativo, permitiendo acceder f\u00e1cilmente a todos nuestros archivos desde la misma herramienta.  Algunas de las caracter\u00edsticas sobresalientes de  Jupyter  son:   Su poderoso  shell  interactivo con soporte para m\u00faltiples lenguajes.  Notebook , su interfase web con soporte para c\u00f3digo, texto, expresiones matem\u00e1ticas, gr\u00e1ficos en l\u00ednea y multimedia.  Su soporte para poder realizar visualizaciones de datos en forma interactiva.  Jupyter  esta totalmente integrado con  matplotlib .  Su simple y flexible interfase para trabajar con la  computaci\u00f3n paralela .",
            "title": "Jupyter"
        },
        {
            "location": "/python/#matplotlib",
            "text": "Matplotlib  es la librer\u00eda m\u00e1s popular en  Python  para visualizaciones y gr\u00e1ficos.  Matplotlib  puede producir gr\u00e1ficos de alta calidad dignos de cualquier publicaci\u00f3n cient\u00edfica.  Algunas de las muchas ventajas que nos ofrece  Matplotlib , incluyen:   Es f\u00e1cil de aprender.  Soporta texto, t\u00edtulos y etiquetas en formato $\\LaTeX$.  Proporciona un gran control sobre cada uno de los elementos de las figuras, como ser su tama\u00f1o, el trazado de sus l\u00edneas, etc.  Nos permite crear gr\u00e1ficos y figuras de gran calidad que pueden ser guardados en varios formatos, como ser: PNG, PDF, SVG, EPS, y PGF.   Matplotlib  se integra de maravilla con  Jupyter  (ver m\u00e1s abajo), lo que nos proporciona un ambiente confortable para las visualizaciones y la exploraci\u00f3n de datos interactiva.",
            "title": "Matplotlib"
        },
        {
            "location": "/python/#pandas",
            "text": "Pandas  es una librer\u00eda  open source  que aporta a  Python  unas estructuras de datos f\u00e1ciles de usar y de alta performance, junto con un gran n\u00famero de funciones esenciales para el an\u00e1lisis de datos. Con la ayuda de  Pandas  podemos trabajar con  datos estructurados  de una forma m\u00e1s r\u00e1pida y expresiva.  Algunas de las cosas sobresalientes que nos aporta  Pandas  son:   Un r\u00e1pido y eficiente objeto  DataFrame  para manipular datos con indexaci\u00f3n integrada;  herramientas para la  lectura y escritura de datos  entre estructuras de datos r\u00e1pidas y eficientes manejadas en memoria, como el  DataFrame , con la mayor\u00eda de los formatos conocidos para el manejo de datos, como ser: CSV y archivos de texto, archivos Microsoft Excel, bases de datos  SQL , y el formato cient\u00edfico HDF5.  Proporciona una  alineaci\u00f3n inteligente de datos  y un manejo integrado de los datos faltantes; con estas funciones podemos obtener una ganancia de performance en los c\u00e1lculos entre  DataFrames  y una f\u00e1cil manipulaci\u00f3n y ordenamiento de los datos de nuestro  data set ;  Flexibilidad para  manipular y redimensionar  nuestro  data set , facilidad para construir  tablas pivote ;  La posibilidad de  filtrar los datos, agregar o eliminar columnas  de una forma sumamente expresiva;  Operaciones de  merge  y *join * altamente eficientes sobre nuestros conjuntos de datos;  Indexaci\u00f3n jer\u00e1rquica  que proporciona una forma intuitiva de trabajar con datos de alta dimensi\u00f3n en una estructura de datos de menor dimensi\u00f3n ;  Posibilidad de realizar c\u00e1lculos agregados o transformaciones de datos con el poderoso motor  group by  que nos permite dividir-aplicar-combinar nuestros conjuntos de datos;  combina las  caracter\u00edsticas de las matrices de alto rendimiento de  Numpy  con las flexibles capacidades de manipulaci\u00f3n de datos de las hojas de c\u00e1lculo  y bases de datos relacionales (tales como  SQL );  Gran n\u00famero de funcionalidades para el manejo de  series de tiempo  ideales para el an\u00e1lisis financiero;  Todas sus funciones y estructuras de datos est\u00e1n  optimizadas para el alto rendimiento , con las partes cr\u00edticas del c\u00f3digo escritas en  Cython  o  C .",
            "title": "Pandas"
        },
        {
            "location": "/python/#scikit-lean",
            "text": "Scikit-learn  es una librer\u00eda especializada en algoritmos para  data mining  y  machine learning .    Algunos de los problemas que podemos resolver utilizando las herramientas de  Scikit-learn , son:   Clasificaciones : Identificar las categor\u00edas a que cada observaci\u00f3n del conjunto de datos pertenece.  Regresiones : Predecir el valor continuo para cada nuevo ejemplo.  Agrupaciones : Agrupaci\u00f3n autom\u00e1tica de objetos similares en un conjunto.  Reducci\u00f3n de dimensiones : Reducir el n\u00famero de variables aleatorias a considerar.  Selecci\u00f3n de Modelos : Comparar, validar y elegir par\u00e1metros y modelos.  Preprocesamiento : Extracci\u00f3n de caracter\u00edsticas a analizar y normalizaci\u00f3n de datos.",
            "title": "Scikit-Lean"
        },
        {
            "location": "/python/#scipy",
            "text": "SciPy  es un conjunto de paquetes donde cada uno ellos ataca un problema distinto dentro de la computaci\u00f3n cient\u00edfica y el an\u00e1lisis num\u00e9rico. Algunos de los paquetes que incluye, son:   scipy.integrate : que proporciona diferentes funciones para resolver problemas de integraci\u00f3n num\u00e9rica.  scipy.linalg : que proporciona funciones para resolver problemas de \u00e1lgebra lineal.  scipy.optimize : para los problemas de optimizaci\u00f3n y minimizaci\u00f3n.  scipy.signal : para el an\u00e1lisis y procesamiento de se\u00f1ales.  scipy.sparse : para matrices dispersas y solucionar sistemas lineales dispersos  scipy.stats : para el an\u00e1lisis de estad\u00edstica y probabilidades.",
            "title": "SciPy"
        },
        {
            "location": "/python/#frameworks-para-deep-learning",
            "text": "En estos momentos, si hay un campo en donde  Python  sobresale sobre cualquier otro lenguaje, es en su soporte para frameworks de  Deep Learning . Existen una gran variedad y de muy buena calidad, entre los que se destacan:    TensorFlow :  TensorFlow  es un frameworks desarrollado por Google. Es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos.     PyTorch :  PyTorch  es un framework de  Deep Learning  que utiliza el lenguaje  Python  y cuenta con el apoyo de Facebook.    Theano :  Theano  es una librer\u00eda de  Python  que permite definir, optimizar y evaluar expresiones matem\u00e1ticas que involucran tensores de manera eficiente.     CNTK :  CNTK  es un conjunto de herramientas, desarrolladas por Microsoft, f\u00e1ciles de usar, de c\u00f3digo abierto que entrena algoritmos de  Deep Learning  para aprender como el cerebro humano.    Keras :  Keras  es una librer\u00eda de alto nivel, muy f\u00e1cil de utilizar. Est\u00e1 escrita y mantenida por Francis Chollet, miembro del equipo de Google Brain. Permite a los usuarios elegir si los modelos que se construyen ser\u00e1n ejecutados en el grafo simb\u00f3lico de  Theano ,  TensorFlow  o  CNTK .    MXNet :  MXNet  es una librer\u00eda flexible y eficiente para armar modelos de  Deep Learning  con soporte para varios idiomas.",
            "title": "Frameworks para Deep Learning"
        },
        {
            "location": "/rlang/",
            "text": "Introducci\u00f3n a R\n\n\n\n\n\u00bfQu\u00e9 es R?\n\n\nR\n es un lenguaje de programaci\u00f3n \ninterpretado\n dise\u00f1ado espec\u00edficamente para el \nan\u00e1lisis estad\u00edstico\n y la manipulaci\u00f3n de datos. Esta inspirado, y es en su mayor medida compatible, por el lenguaje de programaci\u00f3n \nS\n desarrollado por AT&T. Es ampliamente utilizado en todos los campos donde se deben manipular datos, como ser: los negocios, la industria, el gobierno, la medicina, el \u00e1mbito acad\u00e9mico, y dem\u00e1s.\n\n\n\u00bfPor qu\u00e9 utilizar R?\n\n\nR\n cuenta con varias virtudes, como ser:\n\n\n\n\n\n\nEs una implementaci\u00f3n de dominio p\u00fablico del lenguaje estad\u00edstico \nS\n; y la plataforma R/S se ha convertido en el defecto dentro del c\u00edrculo de los profesionales de la \nestad\u00edstica\n.\n\n\n\n\n\n\nEs comparable, y a menudo superior en funcionalidad a productos comerciales; ya sea en gr\u00e1ficas, variedad de operaciones, y algoritmos implementados.\n\n\n\n\n\n\nEs multiplataforma, se encuentra disponible para los sistemas operativos Windows, Mac y Linux.\n\n\n\n\n\n\nAdem\u00e1s de proporcionar operaciones estad\u00edsticas, \nR\n es un lenguaje de programaci\u00f3n de prop\u00f3sito general; es decir, que puede ser utilizado para automatizar an\u00e1lisis y crear nuevas funciones que ampl\u00eden las funcionalidades existentes.\n\n\n\n\n\n\nIncorpora caracter\u00edsticas encontradas en la programaci\u00f3n \norientados a objetos\n y \nfuncional\n.\n\n\n\n\n\n\nEl sistema guarda los conjuntos de datos entre sesiones, por lo que no es necesario volver a cargar los datos cada vez que ingresamos. Tambi\u00e9n guarda nuestro historial de comandos, lo que nos ahorra bastante tiempo y mejora la productividad.\n\n\n\n\n\n\nDebido a que \nR\n es un software de \nc\u00f3digo abierto\n, es f\u00e1cil obtener ayuda de la comunidad de usuarios. Adem\u00e1s, muchas nuevas funciones son aportadas por los usuarios, los cuales son prominentes estad\u00edsticos.\n\n\n\n\n\n\nSi bien existe una peque\u00f1a curva de aprendizaje, \u00e9sta es bastante m\u00ednima en comparaci\u00f3n con otros lenguajes y programas. Asimismo existe una enorme red de colaboradores que constantemente est\u00e1n creando nuevos paquetes que hacen que sea mucho m\u00e1s f\u00e1cil aplicar todo tipo de t\u00e9cnicas y funciones para manipular y analizar nuestros datos con la ayuda de \nR\n.\n\n\n\u00bfC\u00f3mo obtengo R?\n\n\nPara descargar \nR\n, deben dirigirse a \nCRAN\n, la red de archivos de \nR\n. \nCRAN\n se compone de un conjunto de servidores distribuidos en todo el mundo y se utiliza para distribuir \nR\n junto con sus paquetes.\n\n\nRStudio\n\n\nRStudio\n es un entorno de desarrollo integrado, o \nIDE\n, dise\u00f1ado espec\u00edficamente para la programaci\u00f3n con \nR\n. \nRStudio\n hace que \nR\n sea m\u00e1s f\u00e1cil de usar. Incluye un editor de c\u00f3digo, herramientas de depuraci\u00f3n y visualizaci\u00f3n. Si estas dando tus primeros pasos con \nR\n, \nRStudio\n hace la experiencia mucho m\u00e1s amigable.\n\n\nLibrer\u00edas para Ciencia de datos\n\n\nBien, luego de este paseo por las principales estructuras de datos que podemos encontrar en \nR\n, lleg\u00f3 el momento de adentrarnos en el fascinante mundo de la ciencia de datos. Algunas de las librer\u00edas que se han vuelto sumamente \u00fatiles para analizar y manipular datos con \nR\n, son las siguientes:\n\n\nTidyverse\n\n\nUna de las tareas m\u00e1s importantes en cualquier proceso de an\u00e1lisis de datos consiste en ordenarlos y darles una estructura. En general recibimos datos en crudo y debemos procesarlos para poder luego utilizarlos en nuestros modelos. Si de explorar, ordenar y analizar datos se trata el paquete \ntidiverse\n es fundamental. Este paquete incluye las librer\u00edas ggplot2, tibble, tidyr, readr, purrr, y dplyr; las cuales comparten una filosof\u00eda propia y est\u00e1n dise\u00f1ados para trabajar naturalmente entre ellos.\n\n\nCaret\n\n\nA la hora de simplificar el proceso de \nMachine Learning\n, el paquete \nCaret\n puede sernos de gran ayuda. Este paquete nos ofrece una serie de herramientas para la construcci\u00f3n de modelos de \nMachine Learning\n en \nR\n. \nCaret\n nos proporciona herramientas esenciales para: la etapa de preparaci\u00f3n de los datos, para dividir el conjunto de datos, seleccionar los principales atributos, y para evaluar los modelos.\n\n\nData.table\n\n\nSi de organizar grandes vol\u00famenes de datos de una manera intuitiva se trata, \ndata.table\n es el paquete indicado. Esta librer\u00eda nos extiende la estructura de datos del dataFrame para poder trabajar con archivos realmente extensos, y poder realizar operaciones de agregado, agrupado y uniones de una forma m\u00e1s sencilla.\n\n\nOtras librer\u00edas que deber\u00edamos conocer\n\n\nE1071\n\n\nSi lo que buscamos es trabajar con \nM\u00e1quinas de vectores de soporte, SVM\n; o con cualquiera de las principales funciones que podemos encontrar en las clases de probabilidad y estad\u00edstica; entonces el paquete \nE1071\n es exactamente lo que necesitamos. \n\n\nrandomForest\n\n\nPara trabajar espec\u00edficamente con modelos de \nRandom Forest\n el paquete \nrandomForest\n puede ser una buena opci\u00f3n; este paquete nos permite crear este tipo de modelos en forma muy sencilla.\n\n\nrpart\n\n\nEl paquete \nrpart\n es una buena alternativa para trabajar con \u00e1rboles de clasificaciones. Implementa los principales algoritmos para trabajar con este tipo de modelos.\n\n\nnnet\n\n\nLas \nredes neuronales\n han recibido mucha atenci\u00f3n \u00faltimamente por sus habilidades para \naprender\n las relaciones entre las variables. Representan una t\u00e9cnica innovadora para la adaptaci\u00f3n de los modelo que no se basa en los supuestos convencionales necesarios por el modelado est\u00e1ndar; y que adem\u00e1s pueden manejar muy eficazmente los datos multivariantes. Un gran paquete para trabajar con \nredes neuronales\n en forma muy sencilla es \nnnet\n.\n\n\nigraph\n\n\nSi lo que necesitamos es analizar y visualizar redes y grafos, el paquete \nigraph\n es la mejor opci\u00f3n. Este paquete nos proporciona una serie de rutinas altamente eficientes para visualizar y analizar las conexiones de las redes. \n\n\nOutliers\n\n\nSi lo que necesitamos es encontrar valores at\u00edpicos, entonces \noutliers\n es el paquete que debemos utilizar. Esta librer\u00eda nos ofrece varias funciones y tests para poder identificar los valores at\u00edpicos.   \n\n\nSurvival\n\n\nSurvival\n es un paquete que nos facilita la tarea de realizar \nan\u00e1lisis de supervivencia\n. \n\n\nForecast\n\n\nForecast\n proporciona m\u00e9todos y herramientas para mostrar y analizar predicciones univariadas de \nseries de tiempo\n, incluyendo el suavizado exponencial a trav\u00e9s de modelos de espacios de estados y el modelado \nARIMA\n autom\u00e1tico.",
            "title": "Intro R"
        },
        {
            "location": "/rlang/#introduccion-a-r",
            "text": "",
            "title": "Introducci\u00f3n a R"
        },
        {
            "location": "/rlang/#que-es-r",
            "text": "R  es un lenguaje de programaci\u00f3n  interpretado  dise\u00f1ado espec\u00edficamente para el  an\u00e1lisis estad\u00edstico  y la manipulaci\u00f3n de datos. Esta inspirado, y es en su mayor medida compatible, por el lenguaje de programaci\u00f3n  S  desarrollado por AT&T. Es ampliamente utilizado en todos los campos donde se deben manipular datos, como ser: los negocios, la industria, el gobierno, la medicina, el \u00e1mbito acad\u00e9mico, y dem\u00e1s.",
            "title": "\u00bfQu\u00e9 es R?"
        },
        {
            "location": "/rlang/#por-que-utilizar-r",
            "text": "R  cuenta con varias virtudes, como ser:    Es una implementaci\u00f3n de dominio p\u00fablico del lenguaje estad\u00edstico  S ; y la plataforma R/S se ha convertido en el defecto dentro del c\u00edrculo de los profesionales de la  estad\u00edstica .    Es comparable, y a menudo superior en funcionalidad a productos comerciales; ya sea en gr\u00e1ficas, variedad de operaciones, y algoritmos implementados.    Es multiplataforma, se encuentra disponible para los sistemas operativos Windows, Mac y Linux.    Adem\u00e1s de proporcionar operaciones estad\u00edsticas,  R  es un lenguaje de programaci\u00f3n de prop\u00f3sito general; es decir, que puede ser utilizado para automatizar an\u00e1lisis y crear nuevas funciones que ampl\u00eden las funcionalidades existentes.    Incorpora caracter\u00edsticas encontradas en la programaci\u00f3n  orientados a objetos  y  funcional .    El sistema guarda los conjuntos de datos entre sesiones, por lo que no es necesario volver a cargar los datos cada vez que ingresamos. Tambi\u00e9n guarda nuestro historial de comandos, lo que nos ahorra bastante tiempo y mejora la productividad.    Debido a que  R  es un software de  c\u00f3digo abierto , es f\u00e1cil obtener ayuda de la comunidad de usuarios. Adem\u00e1s, muchas nuevas funciones son aportadas por los usuarios, los cuales son prominentes estad\u00edsticos.    Si bien existe una peque\u00f1a curva de aprendizaje, \u00e9sta es bastante m\u00ednima en comparaci\u00f3n con otros lenguajes y programas. Asimismo existe una enorme red de colaboradores que constantemente est\u00e1n creando nuevos paquetes que hacen que sea mucho m\u00e1s f\u00e1cil aplicar todo tipo de t\u00e9cnicas y funciones para manipular y analizar nuestros datos con la ayuda de  R .",
            "title": "\u00bfPor qu\u00e9 utilizar R?"
        },
        {
            "location": "/rlang/#como-obtengo-r",
            "text": "Para descargar  R , deben dirigirse a  CRAN , la red de archivos de  R .  CRAN  se compone de un conjunto de servidores distribuidos en todo el mundo y se utiliza para distribuir  R  junto con sus paquetes.",
            "title": "\u00bfC\u00f3mo obtengo R?"
        },
        {
            "location": "/rlang/#rstudio",
            "text": "RStudio  es un entorno de desarrollo integrado, o  IDE , dise\u00f1ado espec\u00edficamente para la programaci\u00f3n con  R .  RStudio  hace que  R  sea m\u00e1s f\u00e1cil de usar. Incluye un editor de c\u00f3digo, herramientas de depuraci\u00f3n y visualizaci\u00f3n. Si estas dando tus primeros pasos con  R ,  RStudio  hace la experiencia mucho m\u00e1s amigable.",
            "title": "RStudio"
        },
        {
            "location": "/rlang/#librerias-para-ciencia-de-datos",
            "text": "Bien, luego de este paseo por las principales estructuras de datos que podemos encontrar en  R , lleg\u00f3 el momento de adentrarnos en el fascinante mundo de la ciencia de datos. Algunas de las librer\u00edas que se han vuelto sumamente \u00fatiles para analizar y manipular datos con  R , son las siguientes:",
            "title": "Librer\u00edas para Ciencia de datos"
        },
        {
            "location": "/rlang/#tidyverse",
            "text": "Una de las tareas m\u00e1s importantes en cualquier proceso de an\u00e1lisis de datos consiste en ordenarlos y darles una estructura. En general recibimos datos en crudo y debemos procesarlos para poder luego utilizarlos en nuestros modelos. Si de explorar, ordenar y analizar datos se trata el paquete  tidiverse  es fundamental. Este paquete incluye las librer\u00edas ggplot2, tibble, tidyr, readr, purrr, y dplyr; las cuales comparten una filosof\u00eda propia y est\u00e1n dise\u00f1ados para trabajar naturalmente entre ellos.",
            "title": "Tidyverse"
        },
        {
            "location": "/rlang/#caret",
            "text": "A la hora de simplificar el proceso de  Machine Learning , el paquete  Caret  puede sernos de gran ayuda. Este paquete nos ofrece una serie de herramientas para la construcci\u00f3n de modelos de  Machine Learning  en  R .  Caret  nos proporciona herramientas esenciales para: la etapa de preparaci\u00f3n de los datos, para dividir el conjunto de datos, seleccionar los principales atributos, y para evaluar los modelos.",
            "title": "Caret"
        },
        {
            "location": "/rlang/#datatable",
            "text": "Si de organizar grandes vol\u00famenes de datos de una manera intuitiva se trata,  data.table  es el paquete indicado. Esta librer\u00eda nos extiende la estructura de datos del dataFrame para poder trabajar con archivos realmente extensos, y poder realizar operaciones de agregado, agrupado y uniones de una forma m\u00e1s sencilla.",
            "title": "Data.table"
        },
        {
            "location": "/rlang/#otras-librerias-que-deberiamos-conocer",
            "text": "",
            "title": "Otras librer\u00edas que deber\u00edamos conocer"
        },
        {
            "location": "/rlang/#e1071",
            "text": "Si lo que buscamos es trabajar con  M\u00e1quinas de vectores de soporte, SVM ; o con cualquiera de las principales funciones que podemos encontrar en las clases de probabilidad y estad\u00edstica; entonces el paquete  E1071  es exactamente lo que necesitamos.",
            "title": "E1071"
        },
        {
            "location": "/rlang/#randomforest",
            "text": "Para trabajar espec\u00edficamente con modelos de  Random Forest  el paquete  randomForest  puede ser una buena opci\u00f3n; este paquete nos permite crear este tipo de modelos en forma muy sencilla.",
            "title": "randomForest"
        },
        {
            "location": "/rlang/#rpart",
            "text": "El paquete  rpart  es una buena alternativa para trabajar con \u00e1rboles de clasificaciones. Implementa los principales algoritmos para trabajar con este tipo de modelos.",
            "title": "rpart"
        },
        {
            "location": "/rlang/#nnet",
            "text": "Las  redes neuronales  han recibido mucha atenci\u00f3n \u00faltimamente por sus habilidades para  aprender  las relaciones entre las variables. Representan una t\u00e9cnica innovadora para la adaptaci\u00f3n de los modelo que no se basa en los supuestos convencionales necesarios por el modelado est\u00e1ndar; y que adem\u00e1s pueden manejar muy eficazmente los datos multivariantes. Un gran paquete para trabajar con  redes neuronales  en forma muy sencilla es  nnet .",
            "title": "nnet"
        },
        {
            "location": "/rlang/#igraph",
            "text": "Si lo que necesitamos es analizar y visualizar redes y grafos, el paquete  igraph  es la mejor opci\u00f3n. Este paquete nos proporciona una serie de rutinas altamente eficientes para visualizar y analizar las conexiones de las redes.",
            "title": "igraph"
        },
        {
            "location": "/rlang/#outliers",
            "text": "Si lo que necesitamos es encontrar valores at\u00edpicos, entonces  outliers  es el paquete que debemos utilizar. Esta librer\u00eda nos ofrece varias funciones y tests para poder identificar los valores at\u00edpicos.",
            "title": "Outliers"
        },
        {
            "location": "/rlang/#survival",
            "text": "Survival  es un paquete que nos facilita la tarea de realizar  an\u00e1lisis de supervivencia .",
            "title": "Survival"
        },
        {
            "location": "/rlang/#forecast",
            "text": "Forecast  proporciona m\u00e9todos y herramientas para mostrar y analizar predicciones univariadas de  series de tiempo , incluyendo el suavizado exponencial a trav\u00e9s de modelos de espacios de estados y el modelado  ARIMA  autom\u00e1tico.",
            "title": "Forecast"
        },
        {
            "location": "/bibliografia/",
            "text": "Bibliograf\u00eda\n\n\nAqu\u00ed van a poder encontrar un listado de libros recomendados relacionados con la la \nInteligencia Artificial\n y la \nCiencia de Datos\n.\n\n\n\n\nThe Master Algorithm\n - Pedro Domingos\n\n\nEn este libro, Pedro Domingos levanta el velo para darnos un vistazo dentro de las herramientas de \nMachine Learning\n que impulsan a Google, Amazon y nuestro tel\u00e9fono inteligente. Intenta abordar un modelo para el futuro \nMaster Algorithm\n que ser\u00e1 el que rompa la brecha de la \nInteligencia Artificial\n; y discute lo que significar\u00e1 para los negocios, la ciencia y la sociedad. \n\n\nInteligencia Artificial: Un enfoque moderno\n - Stuart J. Russell y Peter Norvig\n\n\nEste libro se puede considerar la biblia de la \nInteligencia Artificial\n. Es una lectura obligatoria para cualquier persona interesada en el campo. \n\n\nReinforcement Learning: An Introduction\n - Richard S. Sutton y Andrew G. Barto\n\n\nSi el anterior era la biblia de la \nIA\n, este libro se puede considerar la biblia del \naprendizaje por refuerzo\n. \n\n\nDeep Learning\n - Ian Goodfellow y Yoshua Bengio\n\n\nEl libro de texto de \nDeep Learning\n es un recurso de lectura obligatoria para todos aquellos interesados en el campo del \nMachine Learning\n en general y del \nDeep Learning\n en particular. Se puede consultar en forma gratuita en la web.\n\n\nThe Elements of Statistical Learning\n - Trevor Hastie, Robert Tibshirani y Jerome Friedman.\n\n\nEste libro que se puede conseguir gratuitamente explica los principales algoritmos de \nMachine Learning\n desde una perspectiva \nestad\u00edstica\n. Dado que hace bastante \u00e9nfasis en la matem\u00e1tica detr\u00e1s de los algoritmos, no es tan f\u00e1cil de seguir y requiere un conocimiento de \nAlgebra lineal\n.\n\n\nAn Introduction to Statistical Learning\n - Gareth James, Daniela Witten, Trevor Hastie y Robert Tibshirani\n\n\nEste libro, que tambi\u00e9n se puede descargar gratuitamente, ofrece una introducci\u00f3n a los m\u00e9todos de \nMachine Learning\n. Est\u00e1 dirigido a estudiantes de nivel superior, estudiantes de maestr\u00eda y doctorados en las ciencias no matem\u00e1ticas. El libro tambi\u00e9n contiene una serie de ejemplos en \nR\n con explicaciones detalladas sobre c\u00f3mo implementar los diversos m\u00e9todos en la vida real. Es un recurso valioso para todo \nCient\u00edfico de datos\n.\n\n\nFoundations of Statistical Natural Language Processing\n -  Christopher D. Manning y Hinrich Sch\u00fctze\n\n\nEste texto fundamental es la primera introducci\u00f3n completa al \nprocesamiento del lenguaje natural\n estad\u00edstico. El libro contiene toda la teor\u00eda y algoritmos necesarios para la construcci\u00f3n de herramientas de \nNLP\n. Proporciona una amplia pero rigurosa cobertura de fundamentos matem\u00e1ticos y ling\u00fc\u00edsticos, as\u00ed como una discusi\u00f3n detallada de los m\u00e9todos estad\u00edsticos, permitiendo a los estudiantes e investigadores construir sus propias implementaciones.\n\n\nData Science for Business\n - Foster Provost y Tom Fawcett\n\n\nEste libro es una muy completa introducci\u00f3n a los principios fundamentales de la \nCiencia de Datos\n. Nos guiar\u00e1 a trav\u00e9s del \npensamiento anal\u00edtico de datos\n necesario para extraer conocimientos \u00fatiles y valor comercial de los datos que recopilamos. Tambi\u00e9n ayudar\u00e1 a comprender muchas t\u00e9cnicas de \nMachine Learning\n que se utilizan actualmente.",
            "title": "Bibliograf\u00eda"
        },
        {
            "location": "/bibliografia/#bibliografia",
            "text": "Aqu\u00ed van a poder encontrar un listado de libros recomendados relacionados con la la  Inteligencia Artificial  y la  Ciencia de Datos .",
            "title": "Bibliograf\u00eda"
        },
        {
            "location": "/bibliografia/#the-master-algorithm-pedro-domingos",
            "text": "En este libro, Pedro Domingos levanta el velo para darnos un vistazo dentro de las herramientas de  Machine Learning  que impulsan a Google, Amazon y nuestro tel\u00e9fono inteligente. Intenta abordar un modelo para el futuro  Master Algorithm  que ser\u00e1 el que rompa la brecha de la  Inteligencia Artificial ; y discute lo que significar\u00e1 para los negocios, la ciencia y la sociedad.",
            "title": "The Master Algorithm - Pedro Domingos"
        },
        {
            "location": "/bibliografia/#inteligencia-artificial-un-enfoque-moderno-stuart-j-russell-y-peter-norvig",
            "text": "Este libro se puede considerar la biblia de la  Inteligencia Artificial . Es una lectura obligatoria para cualquier persona interesada en el campo.",
            "title": "Inteligencia Artificial: Un enfoque moderno - Stuart J. Russell y Peter Norvig"
        },
        {
            "location": "/bibliografia/#reinforcement-learning-an-introduction-richard-s-sutton-y-andrew-g-barto",
            "text": "Si el anterior era la biblia de la  IA , este libro se puede considerar la biblia del  aprendizaje por refuerzo .",
            "title": "Reinforcement Learning: An Introduction - Richard S. Sutton y Andrew G. Barto"
        },
        {
            "location": "/bibliografia/#deep-learning-ian-goodfellow-y-yoshua-bengio",
            "text": "El libro de texto de  Deep Learning  es un recurso de lectura obligatoria para todos aquellos interesados en el campo del  Machine Learning  en general y del  Deep Learning  en particular. Se puede consultar en forma gratuita en la web.",
            "title": "Deep Learning - Ian Goodfellow y Yoshua Bengio"
        },
        {
            "location": "/bibliografia/#the-elements-of-statistical-learning-trevor-hastie-robert-tibshirani-y-jerome-friedman",
            "text": "Este libro que se puede conseguir gratuitamente explica los principales algoritmos de  Machine Learning  desde una perspectiva  estad\u00edstica . Dado que hace bastante \u00e9nfasis en la matem\u00e1tica detr\u00e1s de los algoritmos, no es tan f\u00e1cil de seguir y requiere un conocimiento de  Algebra lineal .",
            "title": "The Elements of Statistical Learning - Trevor Hastie, Robert Tibshirani y Jerome Friedman."
        },
        {
            "location": "/bibliografia/#an-introduction-to-statistical-learning-gareth-james-daniela-witten-trevor-hastie-y-robert-tibshirani",
            "text": "Este libro, que tambi\u00e9n se puede descargar gratuitamente, ofrece una introducci\u00f3n a los m\u00e9todos de  Machine Learning . Est\u00e1 dirigido a estudiantes de nivel superior, estudiantes de maestr\u00eda y doctorados en las ciencias no matem\u00e1ticas. El libro tambi\u00e9n contiene una serie de ejemplos en  R  con explicaciones detalladas sobre c\u00f3mo implementar los diversos m\u00e9todos en la vida real. Es un recurso valioso para todo  Cient\u00edfico de datos .",
            "title": "An Introduction to Statistical Learning - Gareth James, Daniela Witten, Trevor Hastie y Robert Tibshirani"
        },
        {
            "location": "/bibliografia/#foundations-of-statistical-natural-language-processing-christopher-d-manning-y-hinrich-schutze",
            "text": "Este texto fundamental es la primera introducci\u00f3n completa al  procesamiento del lenguaje natural  estad\u00edstico. El libro contiene toda la teor\u00eda y algoritmos necesarios para la construcci\u00f3n de herramientas de  NLP . Proporciona una amplia pero rigurosa cobertura de fundamentos matem\u00e1ticos y ling\u00fc\u00edsticos, as\u00ed como una discusi\u00f3n detallada de los m\u00e9todos estad\u00edsticos, permitiendo a los estudiantes e investigadores construir sus propias implementaciones.",
            "title": "Foundations of Statistical Natural Language Processing -  Christopher D. Manning y Hinrich Sch\u00fctze"
        },
        {
            "location": "/bibliografia/#data-science-for-business-foster-provost-y-tom-fawcett",
            "text": "Este libro es una muy completa introducci\u00f3n a los principios fundamentales de la  Ciencia de Datos . Nos guiar\u00e1 a trav\u00e9s del  pensamiento anal\u00edtico de datos  necesario para extraer conocimientos \u00fatiles y valor comercial de los datos que recopilamos. Tambi\u00e9n ayudar\u00e1 a comprender muchas t\u00e9cnicas de  Machine Learning  que se utilizan actualmente.",
            "title": "Data Science for Business - Foster Provost y Tom Fawcett"
        },
        {
            "location": "/glosario/",
            "text": "Glosario de Inteligencia Artificial, Machine Learning y Ciencia de Datos\n\n\nEn esta secci\u00f3n van a poder encontrar las definiciones de los t\u00e9rminos m\u00e1s utilizados en el mundo de la \nInteligencia Artificial\n y la \nCiencia de Datos\n.\n\n\nA\n - \nB\n - \nC\n - \nD\n - \nE\n - \nF\n - \nG\n - \nH\n - \nI\n - \nJ\n - \nK\n - \nL\n - \nM\n - \nN\n - \nO\n - \nP\n - \nQ\n - \nR\n - \nS\n - \nT\n - \nU\n - \nV\n - \nW\n - \nX\n - \nY\n - \nZ\n\n\n\n\nAlgoritmo \n\n\nUna serie de pasos repetibles para llevar a cabo cierto tipo de tarea con datos. Al estudiar \nCiencia de Datos\n debemos conocer los diferentes algoritmos y sus respectivas ventajas y desventajas.\n\n\nAprendizaje supervisado \n\n\nEn \nMachine Learning\n el \naprendizaje supervisado\n es una t\u00e9cnica para deducir una funci\u00f3n a partir de datos de entrenamiento. Los datos de entrenamiento consisten de pares de objetos (normalmente \nvectores\n): un componente del par son los datos de entrada y el otro, los resultados deseados, es decir, los resultados a los que debe arribar el \nmodelo\n.\n\n\nAprendizaje no supervisado \n\n\nEl \naprendizaje no supervisado\n es un m\u00e9todo de \nMachine Learning\n en donde el \nmodelo\n es ajustado a las observaciones. En este caso el \nalgoritmo\n es entrenado usando un \nconjuntos de datos\n que no tiene ninguna etiqueta; nunca se le dice lo que representan los datos. La idea es que el \nalgoritmo\n pueda encontrar por si solo patrones que ayuden a entender los datos.\n\n\nAprendizaje por refuerzo \n\n\nEn los problemas de \naprendizaje por refuerzo\n, el \nalgoritmo\n aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende en base a prueba-error.\n\n\nArboles de Decisi\u00f3n \n\n\nLos \nArboles de Decision\n son un \nalgoritmo\n de \nMachine Learning\n que consisten en diagramas con construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema.\n\n\nAtributos \n\n\nLos \nAtributos\n son las propiedades individuales que se pueden medir de un fen\u00f3meno que se observa. La elecci\u00f3n de atributos informativos, discriminatorios e independientes es un paso crucial para la eficacia de los algoritmos de \nMachine Learning\n.\n\n\n\n\nBackpropagation \n\n\nLa \npropagaci\u00f3n hacia atr\u00e1s\n o backpropagation es un  \nalgoritmo\n para el ajuste iterativo de los pesos utilizados por las \nredes neuronales\n.\n\n\nBCI \n\n\nBCI\n o \nBrain Computer interfaces\n constituyen una tecnolog\u00eda que se basa en la adquisici\u00f3n de \nondas cerebrales\n para luego ser procesadas e interpretadas por una m\u00e1quina u ordenador. Establecen el camino para interactuar con el exterior mediante nuestro pensamiento.\n\n\nBig Data \n\n\nLa \nBig Data\n es la rama de las \nTeconlog\u00edas de la informaci\u00f3n\n que estudia las dificultades inherentes a la manipulaci\u00f3n de grandes \nconjuntos de datos\n.\n\n\n\n\nCiencia de datos \n\n\nLa \nCiencia de Datos\n es un campo interdisciplinario que involucra m\u00e9todos cient\u00edficos, procesos y sistemas para extraer conocimiento o un mejor entendimiento de datos en sus diferentes formas, ya sea estructurados o no estructurados. \n\n\nClasificaci\u00f3n \n\n\nEn \nMachine Learning\n los problemas de \nClasificaci\u00f3n\n son aquellos en d\u00f3nde el \nalgoritmo\n de aprendizaje debe \nclasificar\n una serie de \nvectores\n en base a informaci\u00f3n de ejemplos previamente etiquetados. Es una caso t\u00edpico del \nAprendizaje supervisado\n\n\nClustering \n\n\nEl \nClustering\n o agrupamiento consiste en agrupar un un conjunto de objetos de tal manera que los miembros del mismo grupo (llamado \ncl\u00faster\n) sean m\u00e1s similares, en alg\u00fan sentido u otro. Es el caso t\u00edpico del \nAprendizaje no supervisado\n.\n\n\nComputaci\u00f3n en la nube \n\n\nLa \ncomputaci\u00f3n en la nube\n es un paradigma que permite ofrecer servicios de computaci\u00f3n a trav\u00e9s de una red, que usualmente es la \ninternet\n. Los servicios que generalmente se ofrecen, se dividen en tres grandes categor\u00edas: \nInfraestructura como servicio (IaaS)\n, \nplataforma como servicio (PaaS)\n y \nsoftware como servicio (SaaS)\n.\n\n\nConjunto de datos \n\n\nUn \nConjunto de datos\n o dataset es una colecci\u00f3n de \nDatos\n que habitualmente est\u00e1n estructurados en forma tabular.\n\n\n\n\nDatos \n\n\nUn \ndato\n es una representaci\u00f3n simb\u00f3lica (num\u00e9rica, alfab\u00e9tica, algor\u00edtmica, espacial, etc.) de un atributo o variable cuantitativa o cualitativa. Los datos describen hechos emp\u00edricos, sucesos y entidades. Es el elemento fundamental con el que trabaja la \nCiencia de Datos\n.\n\n\nDeep Learning \n\n\nEl \nDeep Learning\n o aprendizaje profundo es un subcampo dentro del \nMachine Learning\n, el cu\u00e1l utiliza distintas estructuras de \nredes neuronales\n para lograr el aprendizaje de sucesivas capas de representaciones cada vez m\u00e1s significativas de los datos. Actualmente es el campo con m\u00e1s popularidad dentro de la \nInteligencia Artificial\n.\n\n\n\n\nEstad\u00edstica \n\n\nLa \nEstad\u00edstica\n suele ser definida como la ciencia de aprender de los datos o como la ciencia de obtener conclusiones en la presencia de incertidumbre. Se divide en dos grandes ramas: \nEstad\u00edstica descriptiva\n y \nEstad\u00edstica inferencial\n\n\nEstad\u00edstica Descriptiva \n\n\nLa \nestad\u00edstica descriptiva\n se dedica a recolectar, ordenar, analizar y representar a un \nconjunto de datos\n, con el fin de \ndescribir\n apropiadamente las caracter\u00edsticas de este. Calcula los par\u00e1metros estad\u00edsticos que describen el conjunto estudiado. Algunas de las herramientas que utiliza son gr\u00e1ficos, medidas de frecuencias, medidas de centralizaci\u00f3n, medidas de posici\u00f3n, medidas de dispersi\u00f3n, entre otras.\n\n\nEstad\u00edstica Inferencial \n\n\nLa \nestadistica inferencial\n estudia c\u00f3mo sacar conclusiones generales para toda la poblaci\u00f3n a partir del estudio de una \nmuestra\n, y el grado de fiabilidad o significaci\u00f3n de los resultados obtenidos. Sus principales herramientas son el muestreo, la estimaci\u00f3n de par\u00e1metros y el contraste de \nhip\u00f3tesis\n.\n\n\n\n\nFunci\u00f3n de activaci\u00f3n \n\n\nEn \nredes neuronales\n la \nFunci\u00f3n de activaci\u00f3n\n es la que define la forma en que una \nneurona\n se activa de acuerdo a una entrada o conjunto de entradas. \n\n\nFunci\u00f3n de p\u00e9rdida \n\n\nEn \nMachine Learning\n y \nOptimizaci\u00f3n\n, la \nFunci\u00f3n de p\u00e9rdida\n es aquella que representa la \np\u00e9rdida\n de informaci\u00f3n o el precio pagado por la inexactitud en las predicciones. \n\n\n\n\nGradiente \n\n\nEl concepto de \nGradiente\n es la generalizaci\u00f3n de derivada a funciones de m\u00e1s de una variable o \nvectores\n. Un m\u00e9todo de \nOptimizaci\u00f3n\n muy utilizado en \nDeep Learning\n es el de \ngradientes descendientes\n. \n\n\n\n\nHadoop \n\n\nHadoop\n es un \nframework de software\n, desarrollado en el lenguaje de programaci\u00f3n \nJava\n, que permite el procesamiento distribuido de grandes \nconjuntos de datos\n a trav\u00e9s de \nclusters\n de computadoras utilizando simples modelos de programaci\u00f3n.\n\n\nHip\u00f3tesis \n\n\nEn \nEstad\u00edstica\n, una \nHip\u00f3tesis\n es una suposici\u00f3n de algo posible o imposible para sacar de ello una o m\u00e1s conclusiones. Su valor reside en la capacidad para establecer m\u00e1s relaciones entre los hechos y explicar por qu\u00e9 se producen. La misma debe ser contrastada contra los \ndatos\n que la soporten. \n\n\n\n\nIAAR\n \n\n\nIAAR\n es la comunidad argentina de \nInteligencia Artificial\n. \n\n\nInteligencia Artificial \n\n\nLa \nInteligencia Artificial\n es el estudio de la inforseram\u00e1tica centr\u00e1ndose en el desarrollo de software o \nm\u00e1quinas que exhiben una inteligencia humana\n.\n\n\nInterfaz cerebro computadora \n\n\nLa \ninterfaz cerebro computadora\n o BCI es un campo multidisciplinario que utiliza los nuevos avances en \nneurociencia\n, \nprocesamiento de se\u00f1ales\n, \nmachine learning\n y las \ntecnolog\u00edas de la informaci\u00f3n\n para explorar la forma de comunicar nuestro cerebro en forma directa con las m\u00e1quinas, de la misma forma en que lo hacemos con nuestro cuerpo. \n\n\nInternet de las cosas \n\n\nLa \nInternet de las cosas\n o IoT es un concepto que se refiere a la interconexi\u00f3n digital de objetos cotidianos con internet, permitiendo la creaci\u00f3n de un sin fin de sistemas inteligentes que aprovechan los beneficios de la \nBig Data\n.\n\n\n\n\nJava \n\n\nJava\n es un lenguaje de programaci\u00f3n \norientado a objetos\n dise\u00f1ado para ser multiplataforma y poder ser empleado el mismo programa en diversos sistemas operativos. Es uno de los lenguajes m\u00e1s utilizados en el mundo empresarial por su alto rendimiento. \n\n\nJavascript \n\n\nJavascript\n es el lenguaje de programaci\u00f3n de la \nWeb\n. Se caracteriza por ser f\u00e1cil de aprender, \norientado a objetos\n, interpretado y basado en prototipos. Es ideal para generar contenido din\u00e1mico en \ninternet\n.\n\n\n\n\nKeras \n\n\nKeras\n es una librer\u00eda de alto nivel para \nDeep Learning\n, muy f\u00e1cil de utilizar. Est\u00e1 escrita y mantenida por Francis Chollet, miembro del equipo de Google Brain. Permite a los usuarios elegir si los modelos que se construyen ser\u00e1n ejecutados en el grafo simb\u00f3lico de \nTheano\n, \nTensorFlow\n o \nCNTK\n.\n\n\nK-Means \n\n\nK-means\n es un \nalgoritmo\n de \nMachine Learning\n \nno supervisado\n muy popular para problemas de \nAgrupamiento\n; funciona reduciendo al m\u00ednimo la suma de las distancias cuadradas desde la media dentro de un agrupamiento. Para hacer esto establece primero un n\u00famero previamente especificado de conglomerados, $K$, y luego va asignando cada observaci\u00f3n a la agrupaci\u00f3n m\u00e1s cercana de acuerdo a su media.\n\n\nKNN \n\n\nKNN\n o K vecinos m\u00e1s cercanos es un \nalgoritmo\n de \nMachine Learning\n que consiste en realizar predicciones sobre una \nclase\n en base a la \nclase\n a la que pertenecen los puntos vecinos m\u00e1s cercanos al que intentamos predecir.\n\n\n\n\nMachine Learning \n\n\nEl \nMachine Learning\n o aprendizaje autom\u00e1tico es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que \nutilizan la experiencia pasada para tomar decisiones futuras\n; es el estudio de programas que pueden aprender de los datos. El objetivo fundamental del \nMachine Learning\n es \ngeneralizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada\n. \n\n\nMatrices \n\n\nUna \nmatriz\n es un arreglo bidimensional de n\u00fameros (llamados entradas de la matriz) ordenados en filas (o renglones) y columnas, donde una fila es cada una de las l\u00edneas horizontales de la matriz y una columna es cada una de las l\u00edneas verticales. En una matriz cada elemento puede ser identificado utilizando dos \u00edndices, uno para la fila y otro para la columna en que se encuentra.\n\n\nModelo \n\n\nEn \nMachine Learning\n, un \nmodelo\n es el objeto que va a representar la salida del \nalgoritmo\n de aprendizaje. El \nmodelo\n es lo que utilizamos para realizar las predicciones. \n\n\nMuestra \n\n\nEn \nEstad\u00edstica\n un \nmuestra\n es un subconjunto de casos o individuos de una poblaci\u00f3n. Debemos tratar que la misma sea lo m\u00e1s representativa posible.\n\n\n\n\nNeurona \n\n\nUna \nNeurona\n en una \nred neuronal artificial\n es una aproximaci\u00f3n matem\u00e1tica de una \nneurona biol\u00f3gica\n. Requiere un \nvector\n de entradas, realiza una transformaci\u00f3n en los datos y genera un \u00fanico valor de salida. Puede ser pensado como un filtro.\n\n\n\n\nOpen Source \n\n\nOpen Source\n es un modelo de desarrollo de software que se caracteriza por promover el r\u00e1pido desarrollo e implementaci\u00f3n de mejoras y correcci\u00f3n de errores en una soluci\u00f3n de software. Su principal caracter\u00edstica es que el c\u00f3digo fuente es distribuido junto con la soluci\u00f3n de software; por lo que cualquiera puede acceder a ver como esta construido el software y proponer mejoras  o modificarlo a su gusto. Se basa en el principio fundamental de que la informaci\u00f3n debe circular libremente, sin restricciones.\n\n\nOptimizaci\u00f3n \n\n\nLa \nOptimizaci\u00f3n\n consiste en la selecci\u00f3n del mejor elemento (con respecto a alg\u00fan criterio) de un conjunto de elementos disponibles. En el caso m\u00e1s simple, un \nproblema de optimizaci\u00f3n\n consiste en \nmaximizar\n o \nminimizar\n una funci\u00f3n real eligiendo sistem\u00e1ticamente valores de entrada (tomados de un conjunto permitido) y computando el valor de la funci\u00f3n.\n\n\n\n\nProbabilidad \n\n\nLa \nProbabilidad\n es la rama de las matem\u00e1ticas que se ocupa de los fen\u00f3menos aleatorios y de la incertidumbre. Existen muchos eventos que no se pueden predecir con certeza; ya que su observaci\u00f3n repetida bajo un mismo conjunto espec\u00edfico de condiciones puede arrojar resultados distintos, mostrando un comportamiento err\u00e1tico e impredecible. En estas situaciones, la \nProbabilidad\n proporciona los m\u00e9todos para cuantificar las posibilidades asociadas con los diversos resultados. \n\n\nProcesamiento del lenguaje natural \n\n\nEl \nProcesamiento del lenguaje natural\n es una disciplina interdisciplinaria cuya idea central es la de darle a las m\u00e1quinas la capacidad de leer y comprender los idiomas que hablamos los humanos. La investigaci\u00f3n del \nProcesamiento del lenguaje natural\n tiene como objetivo responder a la pregunta de c\u00f3mo las personas son capaces de comprender el significado de una oraci\u00f3n oral / escrita y c\u00f3mo las personas entienden lo que sucedi\u00f3, cu\u00e1ndo y d\u00f3nde sucedi\u00f3; y las diferencias entre una suposici\u00f3n, una creencia o un hecho.\n\n\nPython \n\n\nPython\n es actualmente uno de los lenguajes m\u00e1s utilizados en \nInteligencia Artificial\n y la \nCiencia de Datos\n; es un lenguaje de programaci\u00f3n de alto nivel que se caracteriza por hacer hincapi\u00e9 en una sintaxis limpia, que favorece un c\u00f3digo legible y f\u00e1cilmente administrable.\n\n\n\n\nR \n\n\nR\n es un lenguaje de programaci\u00f3n interpretado dise\u00f1ado espec\u00edficamente para el an\u00e1lisis estad\u00edstico y la manipulaci\u00f3n de datos. Junto con \nPython\n son los lenguajes m\u00e1s populares en \nCiencia de Datos\n.\n\n\nRed Neuronal \n\n\nLas \nRedes Neuronales\n son un modelo computacional basado en un gran conjunto de unidades neuronales simples (\nneuronas artificiales\n), de forma aproximadamente an\u00e1loga al comportamiento observado en los axones de las neuronas en los cerebros biol\u00f3gicos. Son la unidad de trabajo fundamental del \nDeep Learning\n. \n\n\nRegresi\u00f3n \n\n\nEn \nMachine Learning\n, la \nRegresi\u00f3n\n consiste en encontrar la mejor relaci\u00f3n que representa al \nconjuntos de datos\n. Es una caso t\u00edpico del \nAprendizaje supervisado\n.\n\n\n\n\nSobreajuste \n\n\nEn \nMachine Learning\n un \nmodelo\n va a estar \nsobreajustado\n cuando vemos que se desempe\u00f1a bien con los datos de entrenamiento, pero su precisi\u00f3n es notablemente m\u00e1s baja con los datos de evaluaci\u00f3n; esto se debe a que el modelo ha memorizado los datos que ha visto y no pudo \ngeneralizar\n las reglas para predecir los datos que no ha visto.\n\n\nSVM \n\n\nLas m\u00e1quinas de vectores de soporte o \nSVM\n es un \nalgoritmo\n de \nMachine Learning\n cuya idea central consiste en encontrar un plano que separe los grupos dentro de los datos de la mejor forma posible. Aqu\u00ed, la separaci\u00f3n significa que la elecci\u00f3n del plano maximiza el margen entre los puntos m\u00e1s cercanos en el plano; \u00e9stos puntos se denominan vectores de soporte.\n\n\n\n\nTensor \n\n\nUn \nTensor\n un un arreglo de n\u00fameros que generaliza los conceptos de \nescalares\n, \nvectores\n, y \nmatrices\n a un grado mayor de dimensiones. Es la estructura de datos fundamental que utilizan los principales frameworks de \nDeep Learning\n.\n\n\nTensorFlow \n\n\nTensorFlow\n es un frameworks desarrollado por Google para \nDeep Learning\n. Es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos. Actualmente es la librer\u00eda m\u00e1s popular para el armado de modelos de \nDeep Learning\n.\n\n\n\n\nVector \n\n\nUn \nvector\n es una serie de n\u00fameros. Los n\u00fameros tienen una orden preestablecido, y podemos identificar cada n\u00famero individual por su \u00edndice en ese orden. Podemos pensar en los vectores como la identificaci\u00f3n de puntos en el espacio, con cada elemento que da la coordenada a lo largo de un eje diferente.\n\n\nVisi\u00f3n por computadora \n\n\nLa \nVisi\u00f3n por computadora\n es una disciplina cient\u00edfica que incluye m\u00e9todos para adquirir, procesar, analizar y comprender las im\u00e1genes del mundo real con el fin de producir informaci\u00f3n num\u00e9rica o simb\u00f3lica para que puedan ser tratados por una computadora. Es una de las ramas de la \nInteligencia Artificial\n.",
            "title": "Glosario"
        },
        {
            "location": "/glosario/#glosario-de-inteligencia-artificial-machine-learning-y-ciencia-de-datos",
            "text": "En esta secci\u00f3n van a poder encontrar las definiciones de los t\u00e9rminos m\u00e1s utilizados en el mundo de la  Inteligencia Artificial  y la  Ciencia de Datos .  A  -  B  -  C  -  D  -  E  -  F  -  G  -  H  -  I  -  J  -  K  -  L  -  M  -  N  -  O  -  P  -  Q  -  R  -  S  -  T  -  U  -  V  -  W  -  X  -  Y  -  Z",
            "title": "Glosario de Inteligencia Artificial, Machine Learning y Ciencia de Datos"
        },
        {
            "location": "/glosario/#algoritmo",
            "text": "Una serie de pasos repetibles para llevar a cabo cierto tipo de tarea con datos. Al estudiar  Ciencia de Datos  debemos conocer los diferentes algoritmos y sus respectivas ventajas y desventajas.",
            "title": "Algoritmo "
        },
        {
            "location": "/glosario/#aprendizaje-supervisado",
            "text": "En  Machine Learning  el  aprendizaje supervisado  es una t\u00e9cnica para deducir una funci\u00f3n a partir de datos de entrenamiento. Los datos de entrenamiento consisten de pares de objetos (normalmente  vectores ): un componente del par son los datos de entrada y el otro, los resultados deseados, es decir, los resultados a los que debe arribar el  modelo .",
            "title": "Aprendizaje supervisado "
        },
        {
            "location": "/glosario/#aprendizaje-no-supervisado",
            "text": "El  aprendizaje no supervisado  es un m\u00e9todo de  Machine Learning  en donde el  modelo  es ajustado a las observaciones. En este caso el  algoritmo  es entrenado usando un  conjuntos de datos  que no tiene ninguna etiqueta; nunca se le dice lo que representan los datos. La idea es que el  algoritmo  pueda encontrar por si solo patrones que ayuden a entender los datos.",
            "title": "Aprendizaje no supervisado "
        },
        {
            "location": "/glosario/#aprendizaje-por-refuerzo",
            "text": "En los problemas de  aprendizaje por refuerzo , el  algoritmo  aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende en base a prueba-error.",
            "title": "Aprendizaje por refuerzo "
        },
        {
            "location": "/glosario/#arboles-de-decision",
            "text": "Los  Arboles de Decision  son un  algoritmo  de  Machine Learning  que consisten en diagramas con construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema.",
            "title": "Arboles de Decisi\u00f3n "
        },
        {
            "location": "/glosario/#atributos",
            "text": "Los  Atributos  son las propiedades individuales que se pueden medir de un fen\u00f3meno que se observa. La elecci\u00f3n de atributos informativos, discriminatorios e independientes es un paso crucial para la eficacia de los algoritmos de  Machine Learning .",
            "title": "Atributos "
        },
        {
            "location": "/glosario/#backpropagation",
            "text": "La  propagaci\u00f3n hacia atr\u00e1s  o backpropagation es un   algoritmo  para el ajuste iterativo de los pesos utilizados por las  redes neuronales .",
            "title": "Backpropagation "
        },
        {
            "location": "/glosario/#bci",
            "text": "BCI  o  Brain Computer interfaces  constituyen una tecnolog\u00eda que se basa en la adquisici\u00f3n de  ondas cerebrales  para luego ser procesadas e interpretadas por una m\u00e1quina u ordenador. Establecen el camino para interactuar con el exterior mediante nuestro pensamiento.",
            "title": "BCI "
        },
        {
            "location": "/glosario/#big-data",
            "text": "La  Big Data  es la rama de las  Teconlog\u00edas de la informaci\u00f3n  que estudia las dificultades inherentes a la manipulaci\u00f3n de grandes  conjuntos de datos .",
            "title": "Big Data "
        },
        {
            "location": "/glosario/#ciencia-de-datos",
            "text": "La  Ciencia de Datos  es un campo interdisciplinario que involucra m\u00e9todos cient\u00edficos, procesos y sistemas para extraer conocimiento o un mejor entendimiento de datos en sus diferentes formas, ya sea estructurados o no estructurados.",
            "title": "Ciencia de datos "
        },
        {
            "location": "/glosario/#clasificacion",
            "text": "En  Machine Learning  los problemas de  Clasificaci\u00f3n  son aquellos en d\u00f3nde el  algoritmo  de aprendizaje debe  clasificar  una serie de  vectores  en base a informaci\u00f3n de ejemplos previamente etiquetados. Es una caso t\u00edpico del  Aprendizaje supervisado",
            "title": "Clasificaci\u00f3n "
        },
        {
            "location": "/glosario/#clustering",
            "text": "El  Clustering  o agrupamiento consiste en agrupar un un conjunto de objetos de tal manera que los miembros del mismo grupo (llamado  cl\u00faster ) sean m\u00e1s similares, en alg\u00fan sentido u otro. Es el caso t\u00edpico del  Aprendizaje no supervisado .",
            "title": "Clustering "
        },
        {
            "location": "/glosario/#computacion-en-la-nube",
            "text": "La  computaci\u00f3n en la nube  es un paradigma que permite ofrecer servicios de computaci\u00f3n a trav\u00e9s de una red, que usualmente es la  internet . Los servicios que generalmente se ofrecen, se dividen en tres grandes categor\u00edas:  Infraestructura como servicio (IaaS) ,  plataforma como servicio (PaaS)  y  software como servicio (SaaS) .",
            "title": "Computaci\u00f3n en la nube "
        },
        {
            "location": "/glosario/#conjunto-de-datos",
            "text": "Un  Conjunto de datos  o dataset es una colecci\u00f3n de  Datos  que habitualmente est\u00e1n estructurados en forma tabular.",
            "title": "Conjunto de datos "
        },
        {
            "location": "/glosario/#datos",
            "text": "Un  dato  es una representaci\u00f3n simb\u00f3lica (num\u00e9rica, alfab\u00e9tica, algor\u00edtmica, espacial, etc.) de un atributo o variable cuantitativa o cualitativa. Los datos describen hechos emp\u00edricos, sucesos y entidades. Es el elemento fundamental con el que trabaja la  Ciencia de Datos .",
            "title": "Datos "
        },
        {
            "location": "/glosario/#deep-learning",
            "text": "El  Deep Learning  o aprendizaje profundo es un subcampo dentro del  Machine Learning , el cu\u00e1l utiliza distintas estructuras de  redes neuronales  para lograr el aprendizaje de sucesivas capas de representaciones cada vez m\u00e1s significativas de los datos. Actualmente es el campo con m\u00e1s popularidad dentro de la  Inteligencia Artificial .",
            "title": "Deep Learning "
        },
        {
            "location": "/glosario/#estadistica",
            "text": "La  Estad\u00edstica  suele ser definida como la ciencia de aprender de los datos o como la ciencia de obtener conclusiones en la presencia de incertidumbre. Se divide en dos grandes ramas:  Estad\u00edstica descriptiva  y  Estad\u00edstica inferencial",
            "title": "Estad\u00edstica "
        },
        {
            "location": "/glosario/#estadistica-descriptiva",
            "text": "La  estad\u00edstica descriptiva  se dedica a recolectar, ordenar, analizar y representar a un  conjunto de datos , con el fin de  describir  apropiadamente las caracter\u00edsticas de este. Calcula los par\u00e1metros estad\u00edsticos que describen el conjunto estudiado. Algunas de las herramientas que utiliza son gr\u00e1ficos, medidas de frecuencias, medidas de centralizaci\u00f3n, medidas de posici\u00f3n, medidas de dispersi\u00f3n, entre otras.",
            "title": "Estad\u00edstica Descriptiva "
        },
        {
            "location": "/glosario/#estadistica-inferencial",
            "text": "La  estadistica inferencial  estudia c\u00f3mo sacar conclusiones generales para toda la poblaci\u00f3n a partir del estudio de una  muestra , y el grado de fiabilidad o significaci\u00f3n de los resultados obtenidos. Sus principales herramientas son el muestreo, la estimaci\u00f3n de par\u00e1metros y el contraste de  hip\u00f3tesis .",
            "title": "Estad\u00edstica Inferencial "
        },
        {
            "location": "/glosario/#funcion-de-activacion",
            "text": "En  redes neuronales  la  Funci\u00f3n de activaci\u00f3n  es la que define la forma en que una  neurona  se activa de acuerdo a una entrada o conjunto de entradas.",
            "title": "Funci\u00f3n de activaci\u00f3n "
        },
        {
            "location": "/glosario/#funcion-de-perdida",
            "text": "En  Machine Learning  y  Optimizaci\u00f3n , la  Funci\u00f3n de p\u00e9rdida  es aquella que representa la  p\u00e9rdida  de informaci\u00f3n o el precio pagado por la inexactitud en las predicciones.",
            "title": "Funci\u00f3n de p\u00e9rdida "
        },
        {
            "location": "/glosario/#gradiente",
            "text": "El concepto de  Gradiente  es la generalizaci\u00f3n de derivada a funciones de m\u00e1s de una variable o  vectores . Un m\u00e9todo de  Optimizaci\u00f3n  muy utilizado en  Deep Learning  es el de  gradientes descendientes .",
            "title": "Gradiente "
        },
        {
            "location": "/glosario/#hadoop",
            "text": "Hadoop  es un  framework de software , desarrollado en el lenguaje de programaci\u00f3n  Java , que permite el procesamiento distribuido de grandes  conjuntos de datos  a trav\u00e9s de  clusters  de computadoras utilizando simples modelos de programaci\u00f3n.",
            "title": "Hadoop "
        },
        {
            "location": "/glosario/#hipotesis",
            "text": "En  Estad\u00edstica , una  Hip\u00f3tesis  es una suposici\u00f3n de algo posible o imposible para sacar de ello una o m\u00e1s conclusiones. Su valor reside en la capacidad para establecer m\u00e1s relaciones entre los hechos y explicar por qu\u00e9 se producen. La misma debe ser contrastada contra los  datos  que la soporten.",
            "title": "Hip\u00f3tesis "
        },
        {
            "location": "/glosario/#iaar",
            "text": "IAAR  es la comunidad argentina de  Inteligencia Artificial .",
            "title": "IAAR "
        },
        {
            "location": "/glosario/#inteligencia-artificial",
            "text": "La  Inteligencia Artificial  es el estudio de la inforseram\u00e1tica centr\u00e1ndose en el desarrollo de software o  m\u00e1quinas que exhiben una inteligencia humana .",
            "title": "Inteligencia Artificial "
        },
        {
            "location": "/glosario/#interfaz-cerebro-computadora",
            "text": "La  interfaz cerebro computadora  o BCI es un campo multidisciplinario que utiliza los nuevos avances en  neurociencia ,  procesamiento de se\u00f1ales ,  machine learning  y las  tecnolog\u00edas de la informaci\u00f3n  para explorar la forma de comunicar nuestro cerebro en forma directa con las m\u00e1quinas, de la misma forma en que lo hacemos con nuestro cuerpo.",
            "title": "Interfaz cerebro computadora "
        },
        {
            "location": "/glosario/#internet-de-las-cosas",
            "text": "La  Internet de las cosas  o IoT es un concepto que se refiere a la interconexi\u00f3n digital de objetos cotidianos con internet, permitiendo la creaci\u00f3n de un sin fin de sistemas inteligentes que aprovechan los beneficios de la  Big Data .",
            "title": "Internet de las cosas "
        },
        {
            "location": "/glosario/#java",
            "text": "Java  es un lenguaje de programaci\u00f3n  orientado a objetos  dise\u00f1ado para ser multiplataforma y poder ser empleado el mismo programa en diversos sistemas operativos. Es uno de los lenguajes m\u00e1s utilizados en el mundo empresarial por su alto rendimiento.",
            "title": "Java "
        },
        {
            "location": "/glosario/#javascript",
            "text": "Javascript  es el lenguaje de programaci\u00f3n de la  Web . Se caracteriza por ser f\u00e1cil de aprender,  orientado a objetos , interpretado y basado en prototipos. Es ideal para generar contenido din\u00e1mico en  internet .",
            "title": "Javascript "
        },
        {
            "location": "/glosario/#keras",
            "text": "Keras  es una librer\u00eda de alto nivel para  Deep Learning , muy f\u00e1cil de utilizar. Est\u00e1 escrita y mantenida por Francis Chollet, miembro del equipo de Google Brain. Permite a los usuarios elegir si los modelos que se construyen ser\u00e1n ejecutados en el grafo simb\u00f3lico de  Theano ,  TensorFlow  o  CNTK .",
            "title": "Keras "
        },
        {
            "location": "/glosario/#k-means",
            "text": "K-means  es un  algoritmo  de  Machine Learning   no supervisado  muy popular para problemas de  Agrupamiento ; funciona reduciendo al m\u00ednimo la suma de las distancias cuadradas desde la media dentro de un agrupamiento. Para hacer esto establece primero un n\u00famero previamente especificado de conglomerados, $K$, y luego va asignando cada observaci\u00f3n a la agrupaci\u00f3n m\u00e1s cercana de acuerdo a su media.",
            "title": "K-Means "
        },
        {
            "location": "/glosario/#knn",
            "text": "KNN  o K vecinos m\u00e1s cercanos es un  algoritmo  de  Machine Learning  que consiste en realizar predicciones sobre una  clase  en base a la  clase  a la que pertenecen los puntos vecinos m\u00e1s cercanos al que intentamos predecir.",
            "title": "KNN "
        },
        {
            "location": "/glosario/#machine-learning",
            "text": "El  Machine Learning  o aprendizaje autom\u00e1tico es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que  utilizan la experiencia pasada para tomar decisiones futuras ; es el estudio de programas que pueden aprender de los datos. El objetivo fundamental del  Machine Learning  es  generalizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada .",
            "title": "Machine Learning "
        },
        {
            "location": "/glosario/#matrices",
            "text": "Una  matriz  es un arreglo bidimensional de n\u00fameros (llamados entradas de la matriz) ordenados en filas (o renglones) y columnas, donde una fila es cada una de las l\u00edneas horizontales de la matriz y una columna es cada una de las l\u00edneas verticales. En una matriz cada elemento puede ser identificado utilizando dos \u00edndices, uno para la fila y otro para la columna en que se encuentra.",
            "title": "Matrices "
        },
        {
            "location": "/glosario/#modelo",
            "text": "En  Machine Learning , un  modelo  es el objeto que va a representar la salida del  algoritmo  de aprendizaje. El  modelo  es lo que utilizamos para realizar las predicciones.",
            "title": "Modelo "
        },
        {
            "location": "/glosario/#muestra",
            "text": "En  Estad\u00edstica  un  muestra  es un subconjunto de casos o individuos de una poblaci\u00f3n. Debemos tratar que la misma sea lo m\u00e1s representativa posible.",
            "title": "Muestra "
        },
        {
            "location": "/glosario/#neurona",
            "text": "Una  Neurona  en una  red neuronal artificial  es una aproximaci\u00f3n matem\u00e1tica de una  neurona biol\u00f3gica . Requiere un  vector  de entradas, realiza una transformaci\u00f3n en los datos y genera un \u00fanico valor de salida. Puede ser pensado como un filtro.",
            "title": "Neurona "
        },
        {
            "location": "/glosario/#open-source",
            "text": "Open Source  es un modelo de desarrollo de software que se caracteriza por promover el r\u00e1pido desarrollo e implementaci\u00f3n de mejoras y correcci\u00f3n de errores en una soluci\u00f3n de software. Su principal caracter\u00edstica es que el c\u00f3digo fuente es distribuido junto con la soluci\u00f3n de software; por lo que cualquiera puede acceder a ver como esta construido el software y proponer mejoras  o modificarlo a su gusto. Se basa en el principio fundamental de que la informaci\u00f3n debe circular libremente, sin restricciones.",
            "title": "Open Source "
        },
        {
            "location": "/glosario/#optimizacion",
            "text": "La  Optimizaci\u00f3n  consiste en la selecci\u00f3n del mejor elemento (con respecto a alg\u00fan criterio) de un conjunto de elementos disponibles. En el caso m\u00e1s simple, un  problema de optimizaci\u00f3n  consiste en  maximizar  o  minimizar  una funci\u00f3n real eligiendo sistem\u00e1ticamente valores de entrada (tomados de un conjunto permitido) y computando el valor de la funci\u00f3n.",
            "title": "Optimizaci\u00f3n "
        },
        {
            "location": "/glosario/#probabilidad",
            "text": "La  Probabilidad  es la rama de las matem\u00e1ticas que se ocupa de los fen\u00f3menos aleatorios y de la incertidumbre. Existen muchos eventos que no se pueden predecir con certeza; ya que su observaci\u00f3n repetida bajo un mismo conjunto espec\u00edfico de condiciones puede arrojar resultados distintos, mostrando un comportamiento err\u00e1tico e impredecible. En estas situaciones, la  Probabilidad  proporciona los m\u00e9todos para cuantificar las posibilidades asociadas con los diversos resultados.",
            "title": "Probabilidad "
        },
        {
            "location": "/glosario/#procesamiento-del-lenguaje-natural",
            "text": "El  Procesamiento del lenguaje natural  es una disciplina interdisciplinaria cuya idea central es la de darle a las m\u00e1quinas la capacidad de leer y comprender los idiomas que hablamos los humanos. La investigaci\u00f3n del  Procesamiento del lenguaje natural  tiene como objetivo responder a la pregunta de c\u00f3mo las personas son capaces de comprender el significado de una oraci\u00f3n oral / escrita y c\u00f3mo las personas entienden lo que sucedi\u00f3, cu\u00e1ndo y d\u00f3nde sucedi\u00f3; y las diferencias entre una suposici\u00f3n, una creencia o un hecho.",
            "title": "Procesamiento del lenguaje natural "
        },
        {
            "location": "/glosario/#python",
            "text": "Python  es actualmente uno de los lenguajes m\u00e1s utilizados en  Inteligencia Artificial  y la  Ciencia de Datos ; es un lenguaje de programaci\u00f3n de alto nivel que se caracteriza por hacer hincapi\u00e9 en una sintaxis limpia, que favorece un c\u00f3digo legible y f\u00e1cilmente administrable.",
            "title": "Python "
        },
        {
            "location": "/glosario/#r",
            "text": "R  es un lenguaje de programaci\u00f3n interpretado dise\u00f1ado espec\u00edficamente para el an\u00e1lisis estad\u00edstico y la manipulaci\u00f3n de datos. Junto con  Python  son los lenguajes m\u00e1s populares en  Ciencia de Datos .",
            "title": "R "
        },
        {
            "location": "/glosario/#red-neuronal",
            "text": "Las  Redes Neuronales  son un modelo computacional basado en un gran conjunto de unidades neuronales simples ( neuronas artificiales ), de forma aproximadamente an\u00e1loga al comportamiento observado en los axones de las neuronas en los cerebros biol\u00f3gicos. Son la unidad de trabajo fundamental del  Deep Learning .",
            "title": "Red Neuronal "
        },
        {
            "location": "/glosario/#regresion",
            "text": "En  Machine Learning , la  Regresi\u00f3n  consiste en encontrar la mejor relaci\u00f3n que representa al  conjuntos de datos . Es una caso t\u00edpico del  Aprendizaje supervisado .",
            "title": "Regresi\u00f3n "
        },
        {
            "location": "/glosario/#sobreajuste",
            "text": "En  Machine Learning  un  modelo  va a estar  sobreajustado  cuando vemos que se desempe\u00f1a bien con los datos de entrenamiento, pero su precisi\u00f3n es notablemente m\u00e1s baja con los datos de evaluaci\u00f3n; esto se debe a que el modelo ha memorizado los datos que ha visto y no pudo  generalizar  las reglas para predecir los datos que no ha visto.",
            "title": "Sobreajuste "
        },
        {
            "location": "/glosario/#svm",
            "text": "Las m\u00e1quinas de vectores de soporte o  SVM  es un  algoritmo  de  Machine Learning  cuya idea central consiste en encontrar un plano que separe los grupos dentro de los datos de la mejor forma posible. Aqu\u00ed, la separaci\u00f3n significa que la elecci\u00f3n del plano maximiza el margen entre los puntos m\u00e1s cercanos en el plano; \u00e9stos puntos se denominan vectores de soporte.",
            "title": "SVM "
        },
        {
            "location": "/glosario/#tensor",
            "text": "Un  Tensor  un un arreglo de n\u00fameros que generaliza los conceptos de  escalares ,  vectores , y  matrices  a un grado mayor de dimensiones. Es la estructura de datos fundamental que utilizan los principales frameworks de  Deep Learning .",
            "title": "Tensor "
        },
        {
            "location": "/glosario/#tensorflow",
            "text": "TensorFlow  es un frameworks desarrollado por Google para  Deep Learning . Es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos. Actualmente es la librer\u00eda m\u00e1s popular para el armado de modelos de  Deep Learning .",
            "title": "TensorFlow "
        },
        {
            "location": "/glosario/#vector",
            "text": "Un  vector  es una serie de n\u00fameros. Los n\u00fameros tienen una orden preestablecido, y podemos identificar cada n\u00famero individual por su \u00edndice en ese orden. Podemos pensar en los vectores como la identificaci\u00f3n de puntos en el espacio, con cada elemento que da la coordenada a lo largo de un eje diferente.",
            "title": "Vector "
        },
        {
            "location": "/glosario/#vision-por-computadora",
            "text": "La  Visi\u00f3n por computadora  es una disciplina cient\u00edfica que incluye m\u00e9todos para adquirir, procesar, analizar y comprender las im\u00e1genes del mundo real con el fin de producir informaci\u00f3n num\u00e9rica o simb\u00f3lica para que puedan ser tratados por una computadora. Es una de las ramas de la  Inteligencia Artificial .",
            "title": "Visi\u00f3n por computadora "
        },
        {
            "location": "/recursos/",
            "text": "Recursos\n\n\n\n\nAqu\u00ed van a poder encontrar algunos enlaces de inter\u00e9s a blogs, art\u00edculos y cursos relacionados con la la \nInteligencia Artificial\n y la \nCiencia de Datos\n.\n\n\n\n\n\n\nrelopezbriega.github.io\n: En este blog van a poder encontrar varios art\u00edculos relacionados a las matem\u00e1ticas, el an\u00e1lisis de datos, la \nInteligencia Artificial\n y el \nMachine Learning\n utilizando \nPython\n. \n\n\n\n\n\n\nIAAR\n: El sitio de la comunidad argentina de inteligencia artificial \nIAAR\n.\n\n\n\n\n\n\niaarhub.github.io\n: El blog de capacitaciones de \nIAAR\n. \n\n\n\n\n\n\nmeetups\n: El meetup de la comunidad \nIAAR\n.\n\n\n\n\n\n\nGrupo en Facebook de IAAR\n: El grupo de facebook de \nIAAR\n.\n\n\n\n\n\n\nIAAR capacitaciones\n: El grupo de capacitaciones de \nIAAR\n\n\n\n\n\n\nIAAR debates\n: El grupo de debates de \nIAAR\n.\n\n\n\n\n\n\nIAAR proyectos\n: El grupo de proyectos de \nIAAR\n.\n\n\n\n\n\n\nPython Data Science Handbook\n: En este enlace se pueden ver y descargar las \nnotebooks\n de \nPython\n del libro Python Data Science Handbook de Jake VanderPlas.\n\n\n\n\n\n\nR for Data Science\n: Enlace al libro R for Data Science de Hadley Wickham.\n\n\n\n\n\n\nNeural Networks and Deep Learning\n: Libro online introductorio a las redes neuronales y el \nDeep Learning\n.\n\n\n\n\n\n\nawesome-deep-learning\n: Repositorio de \nGithub\n con enlaces a recursos relacionados al \nDeep Learning\n.\n\n\n\n\n\n\nawesome-datascience\n: Repositorio de \nGithub\n con enlaces a recursos relacionados a \nCiencia de Datos\n.",
            "title": "Recursos"
        },
        {
            "location": "/recursos/#recursos",
            "text": "Aqu\u00ed van a poder encontrar algunos enlaces de inter\u00e9s a blogs, art\u00edculos y cursos relacionados con la la  Inteligencia Artificial  y la  Ciencia de Datos .    relopezbriega.github.io : En este blog van a poder encontrar varios art\u00edculos relacionados a las matem\u00e1ticas, el an\u00e1lisis de datos, la  Inteligencia Artificial  y el  Machine Learning  utilizando  Python .     IAAR : El sitio de la comunidad argentina de inteligencia artificial  IAAR .    iaarhub.github.io : El blog de capacitaciones de  IAAR .     meetups : El meetup de la comunidad  IAAR .    Grupo en Facebook de IAAR : El grupo de facebook de  IAAR .    IAAR capacitaciones : El grupo de capacitaciones de  IAAR    IAAR debates : El grupo de debates de  IAAR .    IAAR proyectos : El grupo de proyectos de  IAAR .    Python Data Science Handbook : En este enlace se pueden ver y descargar las  notebooks  de  Python  del libro Python Data Science Handbook de Jake VanderPlas.    R for Data Science : Enlace al libro R for Data Science de Hadley Wickham.    Neural Networks and Deep Learning : Libro online introductorio a las redes neuronales y el  Deep Learning .    awesome-deep-learning : Repositorio de  Github  con enlaces a recursos relacionados al  Deep Learning .    awesome-datascience : Repositorio de  Github  con enlaces a recursos relacionados a  Ciencia de Datos .",
            "title": "Recursos"
        },
        {
            "location": "/sobre-iaar/",
            "text": "IAAR\n es la comunidad argentina de inteligencia artificial\n\n\nAgrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo \u00e9tico y humanitario de las tecnolog\u00edas cognitivas.\n\n\nObjetivos\n\n\n\n\nImpulsar el desarrollo y apoyar emprendimientos escalables, contribuyendo a la industria argentina en general, y a la Industria del Conocimiento argentina en particular.\n\n\nEntrenar y formar profesionales en Inteligencia Artificial y Ciencia de Datos que el pa\u00eds necesita y necesitar\u00e1 en un mundo cada vez m\u00e1s competitivo.\n\n\nCrear e incubar nuevos grupos interdisciplinarios de investigaci\u00f3n que exporten producci\u00f3n cient\u00edfica hacia el mundo.\n\n\nDivulgar el concepto, la importancia y los impactos tecnol\u00f3gicos, sociales y econ\u00f3micos de la Inteligencia Artificial al p\u00fablico argentino en general, ayudando a periodistas y difusores a lograr \u00e9ste cometido sin sensacionalismos.\n\n\n\n\nValores\n\n\n\n\nInclusi\u00f3n:\n Tecnolog\u00eda al servicio de la comunidad en su conjunto.\n\n\nDesarrollo:\n Industria argentina para el mundo.\n\n\nEmpat\u00eda:\n La visi\u00f3n de todos los afectados es tenida en cuenta.\n\n\nComunicaci\u00f3n:\n Abierta, honesta, directa y efectiva.\n\n\nTransparencia:\n Todas las operaciones est\u00e1n abiertas a la comunidad.\n\n\n\n\nSumarse a la comunidad\n\n\nLos invitamos a sumarse a comunidad, nos pueden encontrar en los grupos de facebook \nIAAR\n, \nIAAR capacitaciones\n, \nIAAR debates\n y \nIAAR proyectos\n; o visitando el \nblog de IAAR\n. Tambi\u00e9n les recomendamos estar atentos a nuestros \nmeetups\n.",
            "title": "Sobre IAAR"
        },
        {
            "location": "/sobre-iaar/#iaar-es-la-comunidad-argentina-de-inteligencia-artificial",
            "text": "Agrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo \u00e9tico y humanitario de las tecnolog\u00edas cognitivas.",
            "title": "IAAR es la comunidad argentina de inteligencia artificial"
        },
        {
            "location": "/sobre-iaar/#objetivos",
            "text": "Impulsar el desarrollo y apoyar emprendimientos escalables, contribuyendo a la industria argentina en general, y a la Industria del Conocimiento argentina en particular.  Entrenar y formar profesionales en Inteligencia Artificial y Ciencia de Datos que el pa\u00eds necesita y necesitar\u00e1 en un mundo cada vez m\u00e1s competitivo.  Crear e incubar nuevos grupos interdisciplinarios de investigaci\u00f3n que exporten producci\u00f3n cient\u00edfica hacia el mundo.  Divulgar el concepto, la importancia y los impactos tecnol\u00f3gicos, sociales y econ\u00f3micos de la Inteligencia Artificial al p\u00fablico argentino en general, ayudando a periodistas y difusores a lograr \u00e9ste cometido sin sensacionalismos.",
            "title": "Objetivos"
        },
        {
            "location": "/sobre-iaar/#valores",
            "text": "Inclusi\u00f3n:  Tecnolog\u00eda al servicio de la comunidad en su conjunto.  Desarrollo:  Industria argentina para el mundo.  Empat\u00eda:  La visi\u00f3n de todos los afectados es tenida en cuenta.  Comunicaci\u00f3n:  Abierta, honesta, directa y efectiva.  Transparencia:  Todas las operaciones est\u00e1n abiertas a la comunidad.",
            "title": "Valores"
        },
        {
            "location": "/sobre-iaar/#sumarse-a-la-comunidad",
            "text": "Los invitamos a sumarse a comunidad, nos pueden encontrar en los grupos de facebook  IAAR ,  IAAR capacitaciones ,  IAAR debates  y  IAAR proyectos ; o visitando el  blog de IAAR . Tambi\u00e9n les recomendamos estar atentos a nuestros  meetups .",
            "title": "Sumarse a la comunidad"
        },
        {
            "location": "/autor/",
            "text": "Ra\u00fal E. L\u00f3pez Briega\n\n\n\n\nYo vivo\u2026\n\n\nEn la ciudad de Buenos Aires, \nArgentina\n. Mi pa\u00eds cuenta con una gran variedad de bellezas naturales, que invito a todos a conocer; yo mismo aun no conozco ni la mitad de mi hermoso pa\u00eds, espero alg\u00fan d\u00eda tener la posibilidad de recorrerlo de punta a punta.\n\n\nYo soy\u2026\n\n\nContador Publico y Licenciado en Administraci\u00f3n. He estudiado mis dos carreras en la \nUniversidad Nacional de La Matanza\n. Muchos se preguntaran \u00bfqu\u00e9 hace un contador escribiendo sobre programaci\u00f3n?; la verdad que yo tampoco s\u00e9 la respuesta a esa pregunta!\n\n\nYo trabajo\u2026\n\n\nComo consultor, para \nTGV\n. Mi carrera profesional comenz\u00f3 en \nMolas y Asociados\n, un estudio contable; all\u00ed adquir\u00ed un conocimiento generalista de la contabilidad y los sistemas administrativos, pasando por la liquidaci\u00f3n de impuestos, la tenedur\u00eda de libros y la auditor\u00eda. Luego mi carrera continuo en \nIBM\n, all\u00ed me desempe\u00f1\u00e9 como analista de Revenue recognition; IBM me mostr\u00f3 lo que es trabajar para una gran multinacional y despert\u00f3 mi pasi\u00f3n por la tecnolog\u00eda. Posteriormente ingrese a \nGrupo ASSA\n, all\u00ed pude materializar la uni\u00f3n entre mis conocimientos administrativos-contables y mi pasi\u00f3n por la tecnolog\u00eda, trabajando en la consultor\u00eda del  ERP \nJD Edwards\n. Finalmente, despu\u00e9s de 5 a\u00f1os en Grupo Assa, ingres\u00e9 a \nTGV\n en busca de nuevos desaf\u00edos profesionales.\n\n\nPara los que quieran conocer mi perfil profesional en m\u00e1s profundidad pueden visitar mi perfil en \nLinkedIn\n.\n\n\nMis pasiones\u2026\n\n\nMis pasiones son:\n\n\n\n\nLa \nTecnolog\u00eda\n, me encanta estar al tanto de las \u00faltimas corrientes tecnol\u00f3gicas.\n\n\nLa \nLiteratura\n, me gusta mucho leer, leo de todo y variado, con una leve predilecci\u00f3n por las novelas de ciencia ficci\u00f3n; mis novelas favoritas son: \n1984, de George Orwell\n y \nLa rebelion de Atlas, de Ayn Rand\n.\n\n\nEl \nAjedrez\n, el juego que me ense\u00f1o mi padre a los 6 a\u00f1os, lo he jugado desde entonces y se convirti\u00f3 en el juego m\u00e1s apasionante que jam\u00e1s haya jugado.\n\n\nRiver Plate\n, el club de f\u00fatbol de mis amores, el m\u00e1s grande de la Argentina!!\n\n\n\n\nMi Misi\u00f3n\u2026\n\n\nContribuir al desarrollo de un mundo m\u00e1s inteligente y productivo a trav\u00e9s del uso de las tecnolog\u00edas de la informaci\u00f3n.\n\n\nMis Blogs\n\n\nPara aquellos que est\u00e9n interesados en las matem\u00e1ticas, el an\u00e1lisis de datos y las \u00faltimas novedades tecnol\u00f3gicas, los invito a que ingresen en mis blogs \nrelopezbriega.github.io\n y \nrelopezbriega.com.ar\n.\n\n\nIAAR\n\n\nTambi\u00e9n colaboro con la comunidad argentina de Inteligencia Artificial \nIAAR\n, con la cual desarrollamos este \nlibro online\n. Los invito a sumarse a los grupos de facebook \nIAAR\n, \nIAAR capacitaciones\n, \nIAAR debates\n y \nIAAR proyectos\n; o visitar el \nblog de IAAR\n.",
            "title": "Sobre el autor"
        },
        {
            "location": "/autor/#raul-e-lopez-briega",
            "text": "Yo vivo\u2026  En la ciudad de Buenos Aires,  Argentina . Mi pa\u00eds cuenta con una gran variedad de bellezas naturales, que invito a todos a conocer; yo mismo aun no conozco ni la mitad de mi hermoso pa\u00eds, espero alg\u00fan d\u00eda tener la posibilidad de recorrerlo de punta a punta.  Yo soy\u2026  Contador Publico y Licenciado en Administraci\u00f3n. He estudiado mis dos carreras en la  Universidad Nacional de La Matanza . Muchos se preguntaran \u00bfqu\u00e9 hace un contador escribiendo sobre programaci\u00f3n?; la verdad que yo tampoco s\u00e9 la respuesta a esa pregunta!  Yo trabajo\u2026  Como consultor, para  TGV . Mi carrera profesional comenz\u00f3 en  Molas y Asociados , un estudio contable; all\u00ed adquir\u00ed un conocimiento generalista de la contabilidad y los sistemas administrativos, pasando por la liquidaci\u00f3n de impuestos, la tenedur\u00eda de libros y la auditor\u00eda. Luego mi carrera continuo en  IBM , all\u00ed me desempe\u00f1\u00e9 como analista de Revenue recognition; IBM me mostr\u00f3 lo que es trabajar para una gran multinacional y despert\u00f3 mi pasi\u00f3n por la tecnolog\u00eda. Posteriormente ingrese a  Grupo ASSA , all\u00ed pude materializar la uni\u00f3n entre mis conocimientos administrativos-contables y mi pasi\u00f3n por la tecnolog\u00eda, trabajando en la consultor\u00eda del  ERP  JD Edwards . Finalmente, despu\u00e9s de 5 a\u00f1os en Grupo Assa, ingres\u00e9 a  TGV  en busca de nuevos desaf\u00edos profesionales.  Para los que quieran conocer mi perfil profesional en m\u00e1s profundidad pueden visitar mi perfil en  LinkedIn .  Mis pasiones\u2026  Mis pasiones son:   La  Tecnolog\u00eda , me encanta estar al tanto de las \u00faltimas corrientes tecnol\u00f3gicas.  La  Literatura , me gusta mucho leer, leo de todo y variado, con una leve predilecci\u00f3n por las novelas de ciencia ficci\u00f3n; mis novelas favoritas son:  1984, de George Orwell  y  La rebelion de Atlas, de Ayn Rand .  El  Ajedrez , el juego que me ense\u00f1o mi padre a los 6 a\u00f1os, lo he jugado desde entonces y se convirti\u00f3 en el juego m\u00e1s apasionante que jam\u00e1s haya jugado.  River Plate , el club de f\u00fatbol de mis amores, el m\u00e1s grande de la Argentina!!   Mi Misi\u00f3n\u2026  Contribuir al desarrollo de un mundo m\u00e1s inteligente y productivo a trav\u00e9s del uso de las tecnolog\u00edas de la informaci\u00f3n.  Mis Blogs  Para aquellos que est\u00e9n interesados en las matem\u00e1ticas, el an\u00e1lisis de datos y las \u00faltimas novedades tecnol\u00f3gicas, los invito a que ingresen en mis blogs  relopezbriega.github.io  y  relopezbriega.com.ar .  IAAR  Tambi\u00e9n colaboro con la comunidad argentina de Inteligencia Artificial  IAAR , con la cual desarrollamos este  libro online . Los invito a sumarse a los grupos de facebook  IAAR ,  IAAR capacitaciones ,  IAAR debates  y  IAAR proyectos ; o visitar el  blog de IAAR .",
            "title": "Ra\u00fal E. L\u00f3pez Briega"
        },
        {
            "location": "/contacto/",
            "text": "Contacto\n\n\nPonerse en contacto con nosotros es muy f\u00e1cil. Completa el siguiente formulario:\n\n\nTambi\u00e9n pueden anotarse en el siguiente \nformulario\n\n\n\n    \n\n        \nSu nombre\n\n        \n\n    \n\n    \n\n        \nSu email\n\n        \n\n    \n\n    \n\n        \nMensaje",
            "title": "Contacto"
        },
        {
            "location": "/contacto/#contacto",
            "text": "Ponerse en contacto con nosotros es muy f\u00e1cil. Completa el siguiente formulario:  Tambi\u00e9n pueden anotarse en el siguiente  formulario  \n     \n         Su nombre \n         \n     \n     \n         Su email \n         \n     \n     \n         Mensaje",
            "title": "Contacto"
        }
    ]
}