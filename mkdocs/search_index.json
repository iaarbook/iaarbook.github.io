{
    "docs": [
        {
            "location": "/",
            "text": "Introducci\u00f3n a la Inteligencia Artificial\n\n\n\n\nEl cerebro es el \u00f3rgano m\u00e1s incre\u00edble del cuerpo humano. Establece la forma en que percibimos las im\u00e1genes, el sonido, los olores, los sabores y el tacto. Nos permite almacenar recuerdos, experimentar emociones e incluso so\u00f1ar. Sin el, ser\u00edamos organismos primitivos, incapaces de otra cosa que el m\u00e1s simple de los reflejos. El cerebro es, en definitiva, lo que nos hace inteligentes.\nDurante d\u00e9cadas hemos so\u00f1ado con construir m\u00e1quinas inteligentes con cerebros como los nuestros; asistentes robotizados para limpiar nuestras casas, coches que se conducen por s\u00ed mismos, microscopios que detecten enfermedades autom\u00e1ticamente. Pero construir estas m\u00e1quinas \nartificialmente inteligentes\n nos obliga a resolver algunos de los problemas computacionales m\u00e1s complejos que hemos tenido; problemas que nuestros cerebros ya pueden resolver en una fracci\u00f3n de segundos. La forma de atacar y resolver estos problemas, es el campo de estudio de la \nInteligencia Artificial\n.\n\n\n\u00bfQu\u00e9 es la Inteligencia Artificial?\n\n\nDefinir el concepto de \nInteligencia Artificial\n no es nada f\u00e1cil. Una definici\u00f3n sumamente general ser\u00eda que la \nIA\n es el estudio de la \ninfrom\u00e1tica\n centr\u00e1ndose en el desarrollo de software o \nm\u00e1quinas que exhiben una inteligencia humana\n. \n\n\nObjetivos de la Inteligencia Artificial\n\n\nLos objetivos principales de la \nIA\n incluyen la deducci\u00f3n y el razonamiento, la representaci\u00f3n del conocimiento, la planificaci\u00f3n, el procesamiento del lenguaje natural (\nNLP\n), el aprendizaje, la percepci\u00f3n y la capacidad de manipular y mover objetos. Los objetivos a largo plazo incluyen el logro de la Creatividad, la Inteligencia Social y la Inteligencia General (a nivel Humano).\n\n\nCuatro enfoques distintos\n\n\nPodemos distinguir cuatro enfoques distintos de abordar el problema de la \nInteligencia Artificial\n. \n\n\n\n\nSistemas que se comportan como humanos\n: Aqu\u00ed la idea es desarrollar m\u00e1quinas capaces de realizar funciones para las cuales se requerir\u00eda un humano inteligente. Dentro de este enfoque podemos encontrar la famosa \nPrueba de Turing\n. Para poder superar esta prueba, la m\u00e1quina deber\u00eda poseer las siguientes capacidades:\n\n\nProcesamiento de lenguaje natural\n, que le permita comunicarse satisfactoriamente. \n\n\nRepresentaci\u00f3n del conocimiento\n, para almacenar lo que se conoce o se siente.\n\n\nRazonamiento autom\u00e1tico\n, para utilizar la informaci\u00f3n almacenada para responder a preguntas y extraer nuevas conclusiones.\n\n\nAprendizaje autom\u00e1tico\n, para adaptarse a nuevas circunstancias y para detectar y extrapolar patrones.\n\n\nVisi\u00f3n computacional\n, para percibir objetos.\n\n\n\n\nRob\u00f3tica\n, para manipular y mover objetos.  \n\n\n\n\n\n\nSistemas que piensan como humanos\n: Aqu\u00ed la idea es hacer que las m\u00e1quinas piensen como humanos en el sentido m\u00e1s literal; es decir, que tengan capacidades cognitivas de toma de decisiones, resoluci\u00f3n de problemas, aprendizaje, etc. Dentro de este enfoque podemos encontrar al campo\ninterdisciplinario de la \nciencia cognitiva\n, en el cual convergen modelos computacionales de \nIA\n y\nt\u00e9cnicas experimentales de \npsicolog\u00eda\n intentando elaborar teor\u00edas precisas y verificables sobre el funcionamiento de la mente humana.\n\n\n\n\n\n\nSistemas que piensan racionalmente\n: Aqu\u00ed la idea es descubrir los c\u00e1lculos que hacen posible percibir, razonar y actuar; es decir, encontrar las \nleyes\n que rigen el pensamiento racional. Dentro de este enfoque podemos encontrar a la \nL\u00f3gica\n, que intenta expresar las \nleyes\n que gobiernan la manera de operar de la mente.\n\n\n\n\n\n\nSistemas que se comportan racionalmente\n: Aqu\u00ed la idea es dise\u00f1ar \nagentes\n inteligentes. Dentro de este enfoque un \nagente racional\n es aquel que act\u00faa con la intenci\u00f3n de alcanzar el mejor resultado o, cuando hay incertidumbre, el mejor resultado esperado. Un elemento importante a tener en cuenta es que tarde o temprano uno se dar\u00e1 cuenta de que obtener una racionalidad perfecta (hacer siempre lo correcto) no es del todo posible en entornos complejos. La demanda computacional que esto implica es demasiado grande, por lo que debemos conformarnos con una racionalidad limitada. Como lo que se busca en este enfoque es realizar inferencias correctas, se necesitan las mismas habilidades que para la \nPrueba de Turing\n, es decir, es necesario contar con la \ncapacidad para representar el conocimiento y razonar bas\u00e1ndonos en \u00e9l\n, porque ello permitir\u00e1 alcanzar decisiones correctas en una amplia gama de situaciones. Es necesario \nser capaz de generar sentencias comprensibles en lenguaje natural\n, ya que el enunciado de tales oraciones permite a los agentes desenvolverse en una sociedad compleja. El \naprendizaje\n no se lleva a cabo por erudici\u00f3n exclusivamente, sino que \nprofundizar en el conocimiento\n de c\u00f3mo funciona el mundo facilita la concepci\u00f3n de estrategias mejores para manejarse en \u00e9l.\n\n\n\n\n\n\nFundamentos de la Inteligencia artificial\n\n\nExisten varias disciplinas que han contribuido con ideas, puntos de vista y t\u00e9cnicas al desarrollo del campo de la \nInteligencia Artificial\n. Ellas son:\n\n\nFilosof\u00eda\n\n\nMuchas han sido las contribuciones de la \nFilosof\u00eda\n a las ciencias. En el campo de la \nInteligencia Artificial\n a contribuido con varios aportes entre los que se destacan los conceptos de \nIA d\u00e9bil\n y \nIA fuerte\n. \n\n\nLa \nIA d\u00e9bil\n se define como la inteligencia artificial racional que se centra t\u00edpicamente en una tarea estrecha. La inteligencia de la \nIA d\u00e9bil\n es limitada, no hay autoconciencia o inteligencia genuina. \nSiri\n es un buen ejemplo de una \nIA d\u00e9bil\n que combina varias t\u00e9cnicas de \nIA d\u00e9bil\n para funcionar. \nSiri\n puede hacer un mont\u00f3n de cosas por nosotros, pero a medida que intentamos tener conversaciones con el asistente virtual, nos damos cuenta de cuan limitada es.\n\n\nLa \nIA fuerte\n es aquella \ninteligencia artificial\n que iguala o excede la inteligencia humana promedio. Este tipo de \nAI\n  ser\u00e1 capaz de realizar todas las tareas que un ser humano podr\u00eda hacer. Hay mucha investigaci\u00f3n en este campo, pero todav\u00eda no han habido grandes avances.\n\n\nMuchos son los debates filos\u00f3ficos alrededor de la \ninteligencia artificial\n, para aquellos interesados en los aspectos filos\u00f3ficos les recomiendo inscribirse en nuestro \ngrupo de debate de IAAR\n\n\nMatem\u00e1ticas\n\n\nSi de ciencias aplicadas se trata, no puede faltar el aporte de las \nMatem\u00e1ticas\n. Para entender y desarrollar los principales \nalgoritmos\n que se utilizan en el campo de la \nInteligencia Artificial\n, deber\u00edamos tener nociones de: \n\n\n\u00c1lgebra lineal\n\n\nEl \n\u00e1lgebra lineal\n es una rama de las matem\u00e1ticas que estudia conceptos tales como vectores, matrices, tensores, sistemas de ecuaciones lineales y en su enfoque de manera m\u00e1s formal, espacios vectoriales y sus transformaciones lineales. Una buena comprensi\u00f3n del \n\u00e1lgebra lineal\n es esencial para entender y trabajar con muchos algoritmos de \nMachine Learning\n, y especialmente para los algoritmos de \nDeep Learning\n.\n\n\nC\u00e1lculo\n\n\nEl \nC\u00e1lculo\n es el campo de la matem\u00e1tica que incluye el estudio de los l\u00edmites, derivadas, integrales y series infinitas, y m\u00e1s concretamente se puede decir que es el estudio del \ncambio\n. Particularmente para el campo de la \nInteligencia Artificial\n algunos conceptos que se deber\u00edan conocer incluyen: \nC\u00e1lculo Diferencial e Integral\n, \nDerivadas Parciales\n, Funciones de Valores Vectoriales, y \nGradientes\n.\n\n\nOptimizaci\u00f3n matem\u00e1tica\n\n\nLa \nOptimizaci\u00f3n matem\u00e1tica\n es la herramienta matem\u00e1tica que nos permite optimizar decisiones, es decir, seleccionar la mejor alternativa de un conjunto de criterios disponibles. Su comprensi\u00f3n es fundamental para poder entender la eficiencia computacional y la escalabilidad de los principales algoritmos de \nMachine Learning\n y \nDeep Learning\n, los cuales suelen trabajar con matrices dispersas de gran tama\u00f1o.\n\n\nProbabilidad y estad\u00edstica\n\n\nLa \nProbabilidad y estad\u00edstica\n es la rama de la matem\u00e1tica que trata con la \nincertidumbre\n, la \naleatoriedad\n y la \ninferencia\n. Sus conceptos son fundamentales para cualquier algoritmo de \nMachine Learning\n o \nDeep Learning\n.\n\n\nUna buena introducci\u00f3n a cada uno de estos campos de las matem\u00e1ticas que son fundamentales para la \nInteligencia Artificial\n, la pueden encontrar en mi \nblog\n.\n\n\nLing\u00fc\u00edstica\n\n\nLa \nLing\u00fc\u00edstica\n moderna y la \nInteligencia Artificial\n nacieron al mismo tiempo y maduraron juntas, solap\u00e1ndose en un campo h\u00edbrido llamado ling\u00fc\u00edstica computacional o \nprocesamiento de lenguaje natural\n. El entendimiento del lenguaje requiere la comprensi\u00f3n de la materia bajo estudio y de su contexto, y no solamente el entendimiento de la estructura de las sentencias; lo que lo convierte en un problema bastante complejo de abordar.\n\n\nNeurociencias\n\n\nLa \nNeurociencia\n es el estudio del sistema neurol\u00f3gico, y en especial del cerebro. La forma exacta en la que en un cerebro se genera el pensamiento es uno de los grandes misterios de la ciencia. El hecho de que una colecci\u00f3n de simples c\u00e9lulas puede llegar a generar razonamiento, acci\u00f3n, y conciencia es un enigma a resolver. Cerebros y computadores realizan tareas bastante diferentes y tienen propiedades muy distintas. Seg\u00fan los c\u00e1lculos de los expertos se estima que para el 2020 las computadoras igualaran la capacidad de procesamiento de los cerebros. Muchos modelos de \nIA\n fueron inspirados en la estructura y el funcionamiento de nuestro cerebro.\n\n\nPsicolog\u00eda\n\n\nLa \nPsicolog\u00eda\n trata sobre el estudio y an\u00e1lisis de la conducta y los procesos mentales de los individuos y grupos humanos. La rama que m\u00e1s influencia ha tenido para la \nInteligencia Artificial\n es la de la \npsicolog\u00eda cognitiva\n que se encarga del estudio de la cognici\u00f3n; es decir, de los procesos mentales implicados en el conocimiento. Tiene como objeto de estudio los mecanismos b\u00e1sicos y profundos por los que se elabora el conocimiento, desde la percepci\u00f3n, la memoria y el aprendizaje, hasta la formaci\u00f3n de conceptos y razonamiento l\u00f3gico. Las teor\u00eda descritas por esta rama han sido utilizados para desarrollar varios modelos de \nInteligencia Artificial\n y \nMachine Learning\n\n\nRamas de la Inteligencia artificial\n\n\nDentro de la \nInteligencia Artificial\n podemos encontrar distintas ramas, entre las que se destacan:\n\n\nMachine Learning\n\n\nEl \nMachine Learning\n es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del \nMachine Learning\n es \ngeneralizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada\n. El ejemplo m\u00e1s t\u00edpico donde podemos ver el uso del Machine Learning es en el filtrado de los correo basura o spam. Mediante la observaci\u00f3n de miles de correos electr\u00f3nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos. \n\n\nEl \nMachine Learning\n tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito, an\u00e1lisis del mercado de valores, clasificaci\u00f3n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob\u00f3tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distingir los distintos \ntipos de problemas de \nMachine Learning\n con los que nos podemos encontrar.\n\n\nAprendizaje supervisado\n\n\nEn los problemas de \naprendizaje supervisado\n se ense\u00f1a o entrena al algoritmo a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos, el algoritmo podr\u00e1 generalizar en una forma m\u00e1s precisa. Una vez concluido el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el algoritmo de aprendizaje utiliza la experiencia pasada que adquiri\u00f3 durante la etapa de entrenamiento para predecir un resultado.\n\n\nAprendizaje no supervisado\n\n\nEn los problemas de \naprendizaje no supervisado\n, el algoritmo es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al algoritmo lo que representan los datos. La idea es que el algoritmo pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos. \n\n\nAprendizaje por refuerzo\n\n\nEn los problemas de \naprendizaje por refuerzo\n, el algoritmo aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m\u00e1s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar\u00e1 creando una estrategia ganadora.\n\n\nDeep Learning\n\n\nEl \nDeep Learning\n constituye un conjunto particular de algoritmos de \nMachine Learning\n que utilizan estructuras profundas de \nredes neuronales\n para encontrar patrones en los datos. Estos tipos de algoritmos cuentan actualmente con un gran inter\u00e9s, ya que han demostrado ser sumamente exitosos para resolver determinados tipos de problemas; como por ejemplo, el reconocimiento de im\u00e1genes. Muchos consideran que este tipo de modelos son los que en el futuro nos llevaran a resolver definitivamente el problema de la \nInteligencia Artificial\n.\n\n\nRazonamiento probabil\u00edstico\n\n\nEl \nrazonamiento probabil\u00edstico\n se encarga de lidiar con la incertidumbre inherente de todo proceso de aprendizaje. El problema para crear una \nInteligencia Artificial\n entonces se convierte en encontrar la forma de trabajar con informaci\u00f3n ruidosa, incompleta e incluso muchas veces contradictoria. Estos algoritmos est\u00e1n sumamente ligados a la \nestad\u00edstica bayesiana\n; y la principal herramienta en la que se apoyan es en el \nteorema de Bayes\n.\n\n\nAlgortimos gen\u00e9ticos\n\n\nLos \nalgoritmos gen\u00e9ticos\n se basan en la idea de que la madre de todo aprendizaje es la \nselecci\u00f3n natural\n. Si la Naturaleza pudo crearnos, puede crear cualquier cosa; por tal motivo lo \u00fanico que deber\u00edamos hacer para alcanzar una \nInteligencia Artificial\n es simular sus mecanismos en una computadora. La idea de estos algoritmos es imitar a la Evoluci\u00f3n; funcionan seleccionando individuos de una poblaci\u00f3n de soluciones candidatas, y luego intentando producir nuevas generaciones de soluciones mejores que las anteriores una y otra vez hasta aproximarse a una soluci\u00f3n perfecta.\n\n\nAplicaciones de la Inteligencia artificial\n\n\nLas t\u00e9cnicas de la \nInteligencia Artificial\n pueden ser aplicadas en una gran variedad de industrias y situaciones, como ser:\n\n\nMedicina\n\n\nApoy\u00e1ndose en las herramientas que proporciona la \nInteligencia Artificial\n, los doctores podr\u00edan realizar diagn\u00f3sticos m\u00e1s certeros y oportunos, lo que llevar\u00eda a mejores tratamientos y m\u00e1s vidas salvadas. \n\n\nAutos aut\u00f3nomos\n\n\nUtilizando \nInteligencia Artificial\n podr\u00edamos crear autos aut\u00f3nomos que aprendan de los datos y experiencias de millones de otros autos, mejorando el tr\u00e1fico y haciendo mucho m\u00e1s segura la conducci\u00f3n.\n\n\nBancos\n\n\nUtilizando t\u00e9cnicas de \nMachine Learning\n los bancos pueden detectar fraudes antes de que ocurran por medio de analizar los patrones de comportamiento de gastos e identificando r\u00e1pidamente actividades sospechosas. \n\n\nAgricultura\n\n\nEn Agricultura se podr\u00eda optimizar el rendimiento de los cultivos por medio de la utilizaci\u00f3n de las t\u00e9cnicas de \nInteligencia Artificial\n para analizar los datos del suelo y del clima en tiempo real, logrando producir m\u00e1s alimentos incluso con climas perjudiciales. \n\n\nEducaci\u00f3n\n\n\nEn la Educaci\u00f3n se podr\u00edan utilizar las t\u00e9cnicas de la \nInteligencia Artificial\n para dise\u00f1ar programas de estudios personalizados basados en datos que mejoren el rendimiento y el ritmo de aprendizaje de los alumnos.\n\n\nLa \u00e9tica y los riesgos de desarrollar una Inteligencia Artificial\n\n\nActualmente tambi\u00e9n ha surgido un debate \u00e9tico alrededor de la \nInteligencia Artificial\n. Algunos de los pensadores m\u00e1s importantes del planeta han establecido su preocupaci\u00f3n sobre el progreso de la \nIA\n. Entre los problemas que puede traer aparejado el desarrollo de la \nInteligencia Artificial\n, podemos encontrar los siguientes:\n\n\n\n\nLas personas podr\u00edan perder sus trabajos por la automatizaci\u00f3n.\n\n\nLas personas podr\u00edan tener demasiado (o muy poco) tiempo de ocio.\n\n\nLas personas podr\u00edan perder el sentido de ser \u00fanicos.\n\n\nLas personas podr\u00edan perder algunos de sus derechos privados.\n\n\nLa utilizaci\u00f3n de los sistemas de \nIA\n podr\u00eda llevar a la p\u00e9rdida de responsabilidad.\n\n\nEl \u00e9xito de la \nIA\n podr\u00eda significar el fin de la raza humana.\n\n\n\n\nEl debate sobre los beneficios y riesgos del desarrollo de la \nInteligencia Artificial\n est\u00e1 todav\u00eda abierto.\n\n\n\u00bfC\u00f3mo iniciarse en el campo de la Inteligencia artificial?\n\n\nSi luego de leer esta introducci\u00f3n, te has quedado fascinado por el campo de la \nInteligencia Artificial\n y quieres incursionar en el mismo, aqu\u00ed te dejo algunas recomendaciones para iniciarse.\n\n\nIAAR\n\n\nIAAR\n es la comunidad argentina de inteligencia artificial. Agrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo \u00e9tico y humanitario de las tecnolog\u00edas cognitivas. Para comenzar a formar parte de la comunidad pueden inscribirse en los grupos de facebook: \nIAAR\n, \nDebates\n, \nProyectos\n, \nCapacitaci\u00f3n\n; y/o en el \nmeetup\n.\n\n\nProgramaci\u00f3n\n\n\nPara poder trabajar en problemas relacionados al campo de la \nInteligencia Artificial\n es necesario saber programar. Los principales lenguajes que se utilizan son \nPython\n y \nR\n. En los repositorios de \nAcademia de IAAR\n van a poder encontrar material sobre estos lenguajes.\n\n\nFrameworks\n\n\nExisten varios frameworks open source que nos facilitan el trabajar con modelos de \nDeep Learning\n, entre los que se destacan:\n\n\n\n\nTensorFlow\n: \nTensorFlow\n es un frameworks desarrollado por Google. Es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos que utiliza el lenguaje \nPython\n. \n\n\nPyTorch\n: \nPyTorch\n es un framework de \nDeep Learning\n que utiliza el lenguaje \nPython\n y cuenta con el apoyo de Facebook.\n\n\nCaffe\n: \nCaffe\n es un framework de \nDeep Learning\n hecho con expresi\u00f3n, velocidad y modularidad en mente, el cual es desarrollado por la universidad de Berkeley.\n\n\nCNTK\n: \nCNTK\n es un conjunto de herramientas, desarrolladas por Microsoft, f\u00e1ciles de usar, de c\u00f3digo abierto que entrena algoritmos de \nDeep Learning\n para aprender como el cerebro humano.\n\n\nTheano\n: \nTheano\n es una librer\u00eda de \nPython\n que permite definir, optimizar y evaluar expresiones matem\u00e1ticas que involucran tensores de manera eficiente. \n\n\nDeepLearning4j\n: \nDeepLearning4j\n Es una librer\u00eda open source para trabajar con modelos de \nDeep Learning\n distribuidos utilizando el lenguaje \nJava\n.\n\n\n\n\nBots\n\n\nUna de las ramas con mayor crecimiento y que m\u00e1s se ha beneficiado con el boom de la \nInteligencia Artificial\n es la de los \nBots\n. Generar peque\u00f1os \nBots\n que puedan tener conversaciones b\u00e1sicas con los usuarios es bastante simple. Pueden encontrar una gu\u00eda con una gran n\u00famero de herramientas en el \nblog de IAAR\n.",
            "title": "Inteligencia Artificial"
        },
        {
            "location": "/#introduccion-a-la-inteligencia-artificial",
            "text": "El cerebro es el \u00f3rgano m\u00e1s incre\u00edble del cuerpo humano. Establece la forma en que percibimos las im\u00e1genes, el sonido, los olores, los sabores y el tacto. Nos permite almacenar recuerdos, experimentar emociones e incluso so\u00f1ar. Sin el, ser\u00edamos organismos primitivos, incapaces de otra cosa que el m\u00e1s simple de los reflejos. El cerebro es, en definitiva, lo que nos hace inteligentes.\nDurante d\u00e9cadas hemos so\u00f1ado con construir m\u00e1quinas inteligentes con cerebros como los nuestros; asistentes robotizados para limpiar nuestras casas, coches que se conducen por s\u00ed mismos, microscopios que detecten enfermedades autom\u00e1ticamente. Pero construir estas m\u00e1quinas  artificialmente inteligentes  nos obliga a resolver algunos de los problemas computacionales m\u00e1s complejos que hemos tenido; problemas que nuestros cerebros ya pueden resolver en una fracci\u00f3n de segundos. La forma de atacar y resolver estos problemas, es el campo de estudio de la  Inteligencia Artificial .",
            "title": "Introducci\u00f3n a la Inteligencia Artificial"
        },
        {
            "location": "/#que-es-la-inteligencia-artificial",
            "text": "Definir el concepto de  Inteligencia Artificial  no es nada f\u00e1cil. Una definici\u00f3n sumamente general ser\u00eda que la  IA  es el estudio de la  infrom\u00e1tica  centr\u00e1ndose en el desarrollo de software o  m\u00e1quinas que exhiben una inteligencia humana .",
            "title": "\u00bfQu\u00e9 es la Inteligencia Artificial?"
        },
        {
            "location": "/#objetivos-de-la-inteligencia-artificial",
            "text": "Los objetivos principales de la  IA  incluyen la deducci\u00f3n y el razonamiento, la representaci\u00f3n del conocimiento, la planificaci\u00f3n, el procesamiento del lenguaje natural ( NLP ), el aprendizaje, la percepci\u00f3n y la capacidad de manipular y mover objetos. Los objetivos a largo plazo incluyen el logro de la Creatividad, la Inteligencia Social y la Inteligencia General (a nivel Humano).",
            "title": "Objetivos de la Inteligencia Artificial"
        },
        {
            "location": "/#cuatro-enfoques-distintos",
            "text": "Podemos distinguir cuatro enfoques distintos de abordar el problema de la  Inteligencia Artificial .    Sistemas que se comportan como humanos : Aqu\u00ed la idea es desarrollar m\u00e1quinas capaces de realizar funciones para las cuales se requerir\u00eda un humano inteligente. Dentro de este enfoque podemos encontrar la famosa  Prueba de Turing . Para poder superar esta prueba, la m\u00e1quina deber\u00eda poseer las siguientes capacidades:  Procesamiento de lenguaje natural , que le permita comunicarse satisfactoriamente.   Representaci\u00f3n del conocimiento , para almacenar lo que se conoce o se siente.  Razonamiento autom\u00e1tico , para utilizar la informaci\u00f3n almacenada para responder a preguntas y extraer nuevas conclusiones.  Aprendizaje autom\u00e1tico , para adaptarse a nuevas circunstancias y para detectar y extrapolar patrones.  Visi\u00f3n computacional , para percibir objetos.   Rob\u00f3tica , para manipular y mover objetos.      Sistemas que piensan como humanos : Aqu\u00ed la idea es hacer que las m\u00e1quinas piensen como humanos en el sentido m\u00e1s literal; es decir, que tengan capacidades cognitivas de toma de decisiones, resoluci\u00f3n de problemas, aprendizaje, etc. Dentro de este enfoque podemos encontrar al campo\ninterdisciplinario de la  ciencia cognitiva , en el cual convergen modelos computacionales de  IA  y\nt\u00e9cnicas experimentales de  psicolog\u00eda  intentando elaborar teor\u00edas precisas y verificables sobre el funcionamiento de la mente humana.    Sistemas que piensan racionalmente : Aqu\u00ed la idea es descubrir los c\u00e1lculos que hacen posible percibir, razonar y actuar; es decir, encontrar las  leyes  que rigen el pensamiento racional. Dentro de este enfoque podemos encontrar a la  L\u00f3gica , que intenta expresar las  leyes  que gobiernan la manera de operar de la mente.    Sistemas que se comportan racionalmente : Aqu\u00ed la idea es dise\u00f1ar  agentes  inteligentes. Dentro de este enfoque un  agente racional  es aquel que act\u00faa con la intenci\u00f3n de alcanzar el mejor resultado o, cuando hay incertidumbre, el mejor resultado esperado. Un elemento importante a tener en cuenta es que tarde o temprano uno se dar\u00e1 cuenta de que obtener una racionalidad perfecta (hacer siempre lo correcto) no es del todo posible en entornos complejos. La demanda computacional que esto implica es demasiado grande, por lo que debemos conformarnos con una racionalidad limitada. Como lo que se busca en este enfoque es realizar inferencias correctas, se necesitan las mismas habilidades que para la  Prueba de Turing , es decir, es necesario contar con la  capacidad para representar el conocimiento y razonar bas\u00e1ndonos en \u00e9l , porque ello permitir\u00e1 alcanzar decisiones correctas en una amplia gama de situaciones. Es necesario  ser capaz de generar sentencias comprensibles en lenguaje natural , ya que el enunciado de tales oraciones permite a los agentes desenvolverse en una sociedad compleja. El  aprendizaje  no se lleva a cabo por erudici\u00f3n exclusivamente, sino que  profundizar en el conocimiento  de c\u00f3mo funciona el mundo facilita la concepci\u00f3n de estrategias mejores para manejarse en \u00e9l.",
            "title": "Cuatro enfoques distintos"
        },
        {
            "location": "/#fundamentos-de-la-inteligencia-artificial",
            "text": "Existen varias disciplinas que han contribuido con ideas, puntos de vista y t\u00e9cnicas al desarrollo del campo de la  Inteligencia Artificial . Ellas son:",
            "title": "Fundamentos de la Inteligencia artificial"
        },
        {
            "location": "/#filosofia",
            "text": "Muchas han sido las contribuciones de la  Filosof\u00eda  a las ciencias. En el campo de la  Inteligencia Artificial  a contribuido con varios aportes entre los que se destacan los conceptos de  IA d\u00e9bil  y  IA fuerte .   La  IA d\u00e9bil  se define como la inteligencia artificial racional que se centra t\u00edpicamente en una tarea estrecha. La inteligencia de la  IA d\u00e9bil  es limitada, no hay autoconciencia o inteligencia genuina.  Siri  es un buen ejemplo de una  IA d\u00e9bil  que combina varias t\u00e9cnicas de  IA d\u00e9bil  para funcionar.  Siri  puede hacer un mont\u00f3n de cosas por nosotros, pero a medida que intentamos tener conversaciones con el asistente virtual, nos damos cuenta de cuan limitada es.  La  IA fuerte  es aquella  inteligencia artificial  que iguala o excede la inteligencia humana promedio. Este tipo de  AI   ser\u00e1 capaz de realizar todas las tareas que un ser humano podr\u00eda hacer. Hay mucha investigaci\u00f3n en este campo, pero todav\u00eda no han habido grandes avances.  Muchos son los debates filos\u00f3ficos alrededor de la  inteligencia artificial , para aquellos interesados en los aspectos filos\u00f3ficos les recomiendo inscribirse en nuestro  grupo de debate de IAAR",
            "title": "Filosof\u00eda"
        },
        {
            "location": "/#matematicas",
            "text": "Si de ciencias aplicadas se trata, no puede faltar el aporte de las  Matem\u00e1ticas . Para entender y desarrollar los principales  algoritmos  que se utilizan en el campo de la  Inteligencia Artificial , deber\u00edamos tener nociones de:",
            "title": "Matem\u00e1ticas"
        },
        {
            "location": "/#algebra-lineal",
            "text": "El  \u00e1lgebra lineal  es una rama de las matem\u00e1ticas que estudia conceptos tales como vectores, matrices, tensores, sistemas de ecuaciones lineales y en su enfoque de manera m\u00e1s formal, espacios vectoriales y sus transformaciones lineales. Una buena comprensi\u00f3n del  \u00e1lgebra lineal  es esencial para entender y trabajar con muchos algoritmos de  Machine Learning , y especialmente para los algoritmos de  Deep Learning .",
            "title": "\u00c1lgebra lineal"
        },
        {
            "location": "/#calculo",
            "text": "El  C\u00e1lculo  es el campo de la matem\u00e1tica que incluye el estudio de los l\u00edmites, derivadas, integrales y series infinitas, y m\u00e1s concretamente se puede decir que es el estudio del  cambio . Particularmente para el campo de la  Inteligencia Artificial  algunos conceptos que se deber\u00edan conocer incluyen:  C\u00e1lculo Diferencial e Integral ,  Derivadas Parciales , Funciones de Valores Vectoriales, y  Gradientes .",
            "title": "C\u00e1lculo"
        },
        {
            "location": "/#optimizacion-matematica",
            "text": "La  Optimizaci\u00f3n matem\u00e1tica  es la herramienta matem\u00e1tica que nos permite optimizar decisiones, es decir, seleccionar la mejor alternativa de un conjunto de criterios disponibles. Su comprensi\u00f3n es fundamental para poder entender la eficiencia computacional y la escalabilidad de los principales algoritmos de  Machine Learning  y  Deep Learning , los cuales suelen trabajar con matrices dispersas de gran tama\u00f1o.",
            "title": "Optimizaci\u00f3n matem\u00e1tica"
        },
        {
            "location": "/#probabilidad-y-estadistica",
            "text": "La  Probabilidad y estad\u00edstica  es la rama de la matem\u00e1tica que trata con la  incertidumbre , la  aleatoriedad  y la  inferencia . Sus conceptos son fundamentales para cualquier algoritmo de  Machine Learning  o  Deep Learning .  Una buena introducci\u00f3n a cada uno de estos campos de las matem\u00e1ticas que son fundamentales para la  Inteligencia Artificial , la pueden encontrar en mi  blog .",
            "title": "Probabilidad y estad\u00edstica"
        },
        {
            "location": "/#linguistica",
            "text": "La  Ling\u00fc\u00edstica  moderna y la  Inteligencia Artificial  nacieron al mismo tiempo y maduraron juntas, solap\u00e1ndose en un campo h\u00edbrido llamado ling\u00fc\u00edstica computacional o  procesamiento de lenguaje natural . El entendimiento del lenguaje requiere la comprensi\u00f3n de la materia bajo estudio y de su contexto, y no solamente el entendimiento de la estructura de las sentencias; lo que lo convierte en un problema bastante complejo de abordar.",
            "title": "Ling\u00fc\u00edstica"
        },
        {
            "location": "/#neurociencias",
            "text": "La  Neurociencia  es el estudio del sistema neurol\u00f3gico, y en especial del cerebro. La forma exacta en la que en un cerebro se genera el pensamiento es uno de los grandes misterios de la ciencia. El hecho de que una colecci\u00f3n de simples c\u00e9lulas puede llegar a generar razonamiento, acci\u00f3n, y conciencia es un enigma a resolver. Cerebros y computadores realizan tareas bastante diferentes y tienen propiedades muy distintas. Seg\u00fan los c\u00e1lculos de los expertos se estima que para el 2020 las computadoras igualaran la capacidad de procesamiento de los cerebros. Muchos modelos de  IA  fueron inspirados en la estructura y el funcionamiento de nuestro cerebro.",
            "title": "Neurociencias"
        },
        {
            "location": "/#psicologia",
            "text": "La  Psicolog\u00eda  trata sobre el estudio y an\u00e1lisis de la conducta y los procesos mentales de los individuos y grupos humanos. La rama que m\u00e1s influencia ha tenido para la  Inteligencia Artificial  es la de la  psicolog\u00eda cognitiva  que se encarga del estudio de la cognici\u00f3n; es decir, de los procesos mentales implicados en el conocimiento. Tiene como objeto de estudio los mecanismos b\u00e1sicos y profundos por los que se elabora el conocimiento, desde la percepci\u00f3n, la memoria y el aprendizaje, hasta la formaci\u00f3n de conceptos y razonamiento l\u00f3gico. Las teor\u00eda descritas por esta rama han sido utilizados para desarrollar varios modelos de  Inteligencia Artificial  y  Machine Learning",
            "title": "Psicolog\u00eda"
        },
        {
            "location": "/#ramas-de-la-inteligencia-artificial",
            "text": "Dentro de la  Inteligencia Artificial  podemos encontrar distintas ramas, entre las que se destacan:",
            "title": "Ramas de la Inteligencia artificial"
        },
        {
            "location": "/#machine-learning",
            "text": "El  Machine Learning  es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del  Machine Learning  es  generalizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada . El ejemplo m\u00e1s t\u00edpico donde podemos ver el uso del Machine Learning es en el filtrado de los correo basura o spam. Mediante la observaci\u00f3n de miles de correos electr\u00f3nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos.   El  Machine Learning  tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito, an\u00e1lisis del mercado de valores, clasificaci\u00f3n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob\u00f3tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distingir los distintos  tipos de problemas de  Machine Learning  con los que nos podemos encontrar.",
            "title": "Machine Learning"
        },
        {
            "location": "/#aprendizaje-supervisado",
            "text": "En los problemas de  aprendizaje supervisado  se ense\u00f1a o entrena al algoritmo a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos, el algoritmo podr\u00e1 generalizar en una forma m\u00e1s precisa. Una vez concluido el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el algoritmo de aprendizaje utiliza la experiencia pasada que adquiri\u00f3 durante la etapa de entrenamiento para predecir un resultado.",
            "title": "Aprendizaje supervisado"
        },
        {
            "location": "/#aprendizaje-no-supervisado",
            "text": "En los problemas de  aprendizaje no supervisado , el algoritmo es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al algoritmo lo que representan los datos. La idea es que el algoritmo pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos.",
            "title": "Aprendizaje no supervisado"
        },
        {
            "location": "/#aprendizaje-por-refuerzo",
            "text": "En los problemas de  aprendizaje por refuerzo , el algoritmo aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m\u00e1s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar\u00e1 creando una estrategia ganadora.",
            "title": "Aprendizaje por refuerzo"
        },
        {
            "location": "/#deep-learning",
            "text": "El  Deep Learning  constituye un conjunto particular de algoritmos de  Machine Learning  que utilizan estructuras profundas de  redes neuronales  para encontrar patrones en los datos. Estos tipos de algoritmos cuentan actualmente con un gran inter\u00e9s, ya que han demostrado ser sumamente exitosos para resolver determinados tipos de problemas; como por ejemplo, el reconocimiento de im\u00e1genes. Muchos consideran que este tipo de modelos son los que en el futuro nos llevaran a resolver definitivamente el problema de la  Inteligencia Artificial .",
            "title": "Deep Learning"
        },
        {
            "location": "/#razonamiento-probabilistico",
            "text": "El  razonamiento probabil\u00edstico  se encarga de lidiar con la incertidumbre inherente de todo proceso de aprendizaje. El problema para crear una  Inteligencia Artificial  entonces se convierte en encontrar la forma de trabajar con informaci\u00f3n ruidosa, incompleta e incluso muchas veces contradictoria. Estos algoritmos est\u00e1n sumamente ligados a la  estad\u00edstica bayesiana ; y la principal herramienta en la que se apoyan es en el  teorema de Bayes .",
            "title": "Razonamiento probabil\u00edstico"
        },
        {
            "location": "/#algortimos-geneticos",
            "text": "Los  algoritmos gen\u00e9ticos  se basan en la idea de que la madre de todo aprendizaje es la  selecci\u00f3n natural . Si la Naturaleza pudo crearnos, puede crear cualquier cosa; por tal motivo lo \u00fanico que deber\u00edamos hacer para alcanzar una  Inteligencia Artificial  es simular sus mecanismos en una computadora. La idea de estos algoritmos es imitar a la Evoluci\u00f3n; funcionan seleccionando individuos de una poblaci\u00f3n de soluciones candidatas, y luego intentando producir nuevas generaciones de soluciones mejores que las anteriores una y otra vez hasta aproximarse a una soluci\u00f3n perfecta.",
            "title": "Algortimos gen\u00e9ticos"
        },
        {
            "location": "/#aplicaciones-de-la-inteligencia-artificial",
            "text": "Las t\u00e9cnicas de la  Inteligencia Artificial  pueden ser aplicadas en una gran variedad de industrias y situaciones, como ser:",
            "title": "Aplicaciones de la Inteligencia artificial"
        },
        {
            "location": "/#medicina",
            "text": "Apoy\u00e1ndose en las herramientas que proporciona la  Inteligencia Artificial , los doctores podr\u00edan realizar diagn\u00f3sticos m\u00e1s certeros y oportunos, lo que llevar\u00eda a mejores tratamientos y m\u00e1s vidas salvadas.",
            "title": "Medicina"
        },
        {
            "location": "/#autos-autonomos",
            "text": "Utilizando  Inteligencia Artificial  podr\u00edamos crear autos aut\u00f3nomos que aprendan de los datos y experiencias de millones de otros autos, mejorando el tr\u00e1fico y haciendo mucho m\u00e1s segura la conducci\u00f3n.",
            "title": "Autos aut\u00f3nomos"
        },
        {
            "location": "/#bancos",
            "text": "Utilizando t\u00e9cnicas de  Machine Learning  los bancos pueden detectar fraudes antes de que ocurran por medio de analizar los patrones de comportamiento de gastos e identificando r\u00e1pidamente actividades sospechosas.",
            "title": "Bancos"
        },
        {
            "location": "/#agricultura",
            "text": "En Agricultura se podr\u00eda optimizar el rendimiento de los cultivos por medio de la utilizaci\u00f3n de las t\u00e9cnicas de  Inteligencia Artificial  para analizar los datos del suelo y del clima en tiempo real, logrando producir m\u00e1s alimentos incluso con climas perjudiciales.",
            "title": "Agricultura"
        },
        {
            "location": "/#educacion",
            "text": "En la Educaci\u00f3n se podr\u00edan utilizar las t\u00e9cnicas de la  Inteligencia Artificial  para dise\u00f1ar programas de estudios personalizados basados en datos que mejoren el rendimiento y el ritmo de aprendizaje de los alumnos.",
            "title": "Educaci\u00f3n"
        },
        {
            "location": "/#la-etica-y-los-riesgos-de-desarrollar-una-inteligencia-artificial",
            "text": "Actualmente tambi\u00e9n ha surgido un debate \u00e9tico alrededor de la  Inteligencia Artificial . Algunos de los pensadores m\u00e1s importantes del planeta han establecido su preocupaci\u00f3n sobre el progreso de la  IA . Entre los problemas que puede traer aparejado el desarrollo de la  Inteligencia Artificial , podemos encontrar los siguientes:   Las personas podr\u00edan perder sus trabajos por la automatizaci\u00f3n.  Las personas podr\u00edan tener demasiado (o muy poco) tiempo de ocio.  Las personas podr\u00edan perder el sentido de ser \u00fanicos.  Las personas podr\u00edan perder algunos de sus derechos privados.  La utilizaci\u00f3n de los sistemas de  IA  podr\u00eda llevar a la p\u00e9rdida de responsabilidad.  El \u00e9xito de la  IA  podr\u00eda significar el fin de la raza humana.   El debate sobre los beneficios y riesgos del desarrollo de la  Inteligencia Artificial  est\u00e1 todav\u00eda abierto.",
            "title": "La \u00e9tica y los riesgos de desarrollar una Inteligencia Artificial"
        },
        {
            "location": "/#como-iniciarse-en-el-campo-de-la-inteligencia-artificial",
            "text": "Si luego de leer esta introducci\u00f3n, te has quedado fascinado por el campo de la  Inteligencia Artificial  y quieres incursionar en el mismo, aqu\u00ed te dejo algunas recomendaciones para iniciarse.",
            "title": "\u00bfC\u00f3mo iniciarse en el campo de la Inteligencia artificial?"
        },
        {
            "location": "/#iaar",
            "text": "IAAR  es la comunidad argentina de inteligencia artificial. Agrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo \u00e9tico y humanitario de las tecnolog\u00edas cognitivas. Para comenzar a formar parte de la comunidad pueden inscribirse en los grupos de facebook:  IAAR ,  Debates ,  Proyectos ,  Capacitaci\u00f3n ; y/o en el  meetup .",
            "title": "IAAR"
        },
        {
            "location": "/#programacion",
            "text": "Para poder trabajar en problemas relacionados al campo de la  Inteligencia Artificial  es necesario saber programar. Los principales lenguajes que se utilizan son  Python  y  R . En los repositorios de  Academia de IAAR  van a poder encontrar material sobre estos lenguajes.",
            "title": "Programaci\u00f3n"
        },
        {
            "location": "/#frameworks",
            "text": "Existen varios frameworks open source que nos facilitan el trabajar con modelos de  Deep Learning , entre los que se destacan:   TensorFlow :  TensorFlow  es un frameworks desarrollado por Google. Es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos que utiliza el lenguaje  Python .   PyTorch :  PyTorch  es un framework de  Deep Learning  que utiliza el lenguaje  Python  y cuenta con el apoyo de Facebook.  Caffe :  Caffe  es un framework de  Deep Learning  hecho con expresi\u00f3n, velocidad y modularidad en mente, el cual es desarrollado por la universidad de Berkeley.  CNTK :  CNTK  es un conjunto de herramientas, desarrolladas por Microsoft, f\u00e1ciles de usar, de c\u00f3digo abierto que entrena algoritmos de  Deep Learning  para aprender como el cerebro humano.  Theano :  Theano  es una librer\u00eda de  Python  que permite definir, optimizar y evaluar expresiones matem\u00e1ticas que involucran tensores de manera eficiente.   DeepLearning4j :  DeepLearning4j  Es una librer\u00eda open source para trabajar con modelos de  Deep Learning  distribuidos utilizando el lenguaje  Java .",
            "title": "Frameworks"
        },
        {
            "location": "/#bots",
            "text": "Una de las ramas con mayor crecimiento y que m\u00e1s se ha beneficiado con el boom de la  Inteligencia Artificial  es la de los  Bots . Generar peque\u00f1os  Bots  que puedan tener conversaciones b\u00e1sicas con los usuarios es bastante simple. Pueden encontrar una gu\u00eda con una gran n\u00famero de herramientas en el  blog de IAAR .",
            "title": "Bots"
        },
        {
            "location": "/deeplearning/",
            "text": "Introducci\u00f3n al Deep Learning\n\n\n\n\nIntroducci\u00f3n \n\n\nEl \nDeep Learning\n es sin duda el \u00e1rea de investigaci\u00f3n m\u00e1s popular dentro del campo de la \ninteligencia artificial\n. La mayor\u00eda de las nuevas investigaciones que se realizan, trabajan con modelos basados en las t\u00e9cnicas de \nDeep Learning\n; ya que las mismas han logrado resultados sorprendes en campos como \nProcesamiento del lenguaje natural\n y \nVisi\u00f3n por computadora\n. Pero... \u00bfqu\u00e9 es este misterioso concepto que ha ganado tanta popularidad? y... \u00bfc\u00f3mo se relaciona con el campo de la \ninteligencia artificial\n y el \nMachine Learning\n?. \n\n\nInteligencia artificial, Machine learning y Deep learning \n\n\nEn general se suelen utilizar los t\u00e9rminos de \ninteligencia artificial\n, \nMachine Learning\n y \nDeep Learning\n en forma intercambiada. Sin embargo, \u00e9stos t\u00e9rminos no son los mismo y abarcan distintas cosas. \n\n\nInteligencia Artificial\n\n\nEl t\u00e9rmino \ninteligencia artificial\n es el m\u00e1s general y engloba a los campos de \nMachine Learning\n y \nDeep Learning\n junto con otras t\u00e9cnicas como los \nalgoritmos de b\u00fasqueda\n, el \nrazonamiento simb\u00f3lico\n, el \nrazonamiento l\u00f3gico\n y la \nestad\u00edstica\n. Naci\u00f3 en los a\u00f1os 1950s, cuando un grupo de pioneros de la computaci\u00f3n comenzaron a preguntarse si se pod\u00eda hacer que las computadoras \npensaran\n. Una definici\u00f3n concisa de la \ninteligencia artificial\n ser\u00eda: \nel esfuerzo para automatizar las tareas intelectuales que normalmente realizan los seres humanos\n. \n\n\nMachine Learning\n\n\nEl \nMachine Learning\n o \nAprendizaje autom\u00e1tico\n se refiere a un amplio conjunto de t\u00e9cnicas inform\u00e1ticas que nos permiten dar a las computadoras \nla capacidad de aprender sin ser expl\u00edcitamente programadas\n. Hay muchos tipos diferentes de algoritmos de \nAprendizaje autom\u00e1tico\n, entre los que se encuentran el \naprendizaje por refuerzo\n, los \nalgoritmos gen\u00e9ticos\n, el aprendizaje basado en \nreglas de asociaci\u00f3n\n, los \nalgoritmos de agrupamiento\n, los \n\u00e1rboles de decisi\u00f3n\n, las \nm\u00e1quinas de vectores de soporte\n y las \nredes neuronales\n. Actualmente, los algoritmos m\u00e1s populares dentro de este campo son los de \nDeep Learning\n.\n\n\nDeep Learning\n\n\nEl \nDeep Learning\n o \naprendizaje profundo\n es un subcampo dentro del \nMachine Learning\n, el cu\u00e1l utiliza distintas estructuras de \nredes neuronales\n para lograr el aprendizaje de sucesivas \ncapas de representaciones\n cada vez m\u00e1s significativas de los datos. El \nprofundo\n o \ndeep\n en \nDeep Learning\n hace referencia a la cantidad de \ncapas de representaciones\n que se utilizan en el modelo; en general se suelen utilizar decenas o incluso cientos de \ncapas de representaci\u00f3n\n. las cuales \naprenden\n automaticamente a medida que el modelo es entrenado con los datos.\n\n\n\n\n\u00bfQu\u00e9 es el Deep Learning? \n\n\nAntes de poder entender que es el \nDeep Learning\n, debemos en primer lugar conocer dos conceptos fundamentales: las \nredes neuronales artificiales\n y la \nPropagaci\u00f3n hacia atr\u00e1s\n.\n\n\nRedes Neuronales\n\n\nLas \nredes neuronales\n son un modelo computacional basado en un gran conjunto de unidades neuronales simples (\nneuronas artificiales\n), de forma aproximadamente an\u00e1loga al comportamiento observado en los axones de las neuronas en los cerebros biol\u00f3gicos. \n\n\nCada una de estas neuronas simples, va a tener una forma similar al siguiente diagrama:\n\n\n\n\nEn donde sus componentes son:\n\n\n\n\n\n\n$x_1, x_2, \\dots, x_n$: Los datos de entrada en la neurona, los cuales tambi\u00e9n puede ser que sean producto de la salida de otra neurona de la red.\n\n\n\n\n\n\n$x_0$: La unidad de sesgo; un valor constante que se le suma a la entrada de la funci\u00f3n de activaci\u00f3n de la neurona. Generalmente tiene el valor 1. Este valor va a permitir cambiar la funci\u00f3n de activaci\u00f3n hacia la derecha o izquierda, otorg\u00e1ndole m\u00e1s flexibilidad para aprender a la neurona.\n\n\n\n\n\n\n$w_0, w_1, w_2, \\dots, w_n$: Los pesos relativos de cada entrada. Tener en cuenta que incluso la unidad de sesgo tiene un peso.\n\n\n\n\n\n\na: La salida de la neurona. Que va a ser calculada de la siguiente forma:\n\n\n\n\n\n\n$$a = f\\left(\\sum_{i=0}^n w_i \\cdot x_i \\right)$$\n\n\nAqu\u00ed $f$ es la \nfunci\u00f3n de activaci\u00f3n\n de la neurona. Esta funci\u00f3n es la que le otorga tanta flexibilidad a las \nredes neuronales\n y le permite estimar complejas relaciones no lineales en los datos. Puede ser tanto una \nfunci\u00f3n lineal\n, una \nfunci\u00f3n log\u00edstica\n, \nhiperb\u00f3lica\n, etc.\n\n\nCada unidad neuronal est\u00e1 conectada con muchas otras y los enlaces entre ellas pueden incrementar o inhibir el estado de activaci\u00f3n de las neuronas adyacentes. Estos sistemas aprenden y se forman a s\u00ed mismos, en lugar de ser programados de forma expl\u00edcita, y sobresalen en \u00e1reas donde la detecci\u00f3n de soluciones o caracter\u00edsticas es dif\u00edcil de expresar con la programaci\u00f3n convencional.\n\n\n\n\nPropagaci\u00f3n hacia atr\u00e1s\n\n\nLa \npropagaci\u00f3n hacia atr\u00e1s\n o \nbackpropagation\n es un algoritmo que funciona mediante la determinaci\u00f3n de la p\u00e9rdida (o error) en la salida y luego propag\u00e1ndolo de nuevo hacia atr\u00e1s en la red. De esta forma los pesos se van actualizando para minimizar el error resultante de cada neurona. Este algoritmo es lo que les permite a las \nredes neuronales\n aprender.\n\n\n\n\n\u00bfC\u00f3mo funciona el Deep Learning?  \n\n\nEn general, cualquier t\u00e9cnica de \nMachine Learning\n trata de realizar la asignaci\u00f3n de entradas (por ejemplo, im\u00e1genes) a salidas objetivo (Por ejemplo, la etiqueta \"gato\"), mediante la observaci\u00f3n de un gran n\u00famero de ejemplos de entradas y salidas. El \nDeep Learning\n realiza este mapeo de entrada-a-objetivo por medio de una \nred neuronal artificial\n que est\u00e1 compuesta de un n\u00famero grande de \ncapas\n dispuestas en forma de jerarqu\u00eda. La \nred\n aprende algo simple en la capa inicial de la jerarqu\u00eda y luego env\u00eda esta informaci\u00f3n a la siguiente capa. La siguiente capa toma esta informaci\u00f3n simple, lo combina en algo que es un poco m\u00e1s complejo, y lo pasa a la tercer capa. Este proceso contin\u00faa de forma tal que cada capa de la jerarqu\u00eda construye algo m\u00e1s complejo de la entrada que recibi\u00f3 de la capa anterior. De esta forma, la \nred\n ir\u00e1 \naprendiendo\n por medio de la exposici\u00f3n a los datos de ejemplo.\n\n\nLa especificaci\u00f3n de lo que cada \ncapa\n hace a la entrada que recibe es almacenada en los \npesos\n de la capa, que en esencia, no son m\u00e1s que n\u00fameros. Utilizando terminolog\u00eda m\u00e1s t\u00e9cnica podemos decir que la transformaci\u00f3n de datos que se produce en la \ncapa\n es \nparametrizada\n por sus \npesos\n. Para que la \nred\n aprenda debemos encontrar los \npesos\n de todas las \ncapas\n de forma tal que la \nred\n realice un mapeo perfecto entre los ejemplos de entrada con sus respectivas salidas objetivo. Pero el problema reside en que una \nred\n de \nDeep Learning\n puede tener millones de \npar\u00e1metros\n, por lo que encontrar el valor correcto de todos ellos puede ser una tarea realmente muy dif\u00edcil, especialmente si la modificaci\u00f3n del valor de uno de ellos afecta a todos los dem\u00e1s.\n\n\n\n\nPara poder controlar algo, en primer lugar debemos poder observarlo. En este sentido, para controlar la salida de la \nred neuronal\n, deber\u00edamos poder medir cuan lejos esta la salida que obtuvimos de la que se esperaba obtener. Este es el trabajo de la \nfunci\u00f3n de p\u00e9rdida\n de la \nred\n. Esta funci\u00f3n toma las predicciones que realiza el modelo y los valores objetivos (lo que realmente esperamos que la \nred\n produzca), y calcula cu\u00e1n lejos estamos de ese valor, de esta manera, podemos capturar que tan bien esta funcionando el modelo para el ejemplo especificado. El truco fundamental del \nDeep Learning\n es utilizar el valor que nos devuelve esta  \nfunci\u00f3n de p\u00e9rdida\n para retroalimentar la  \nred\n y ajustar los \npesos\n en la direcci\u00f3n que vayan reduciendo la \np\u00e9rdida\n del modelo para cada ejemplo. Este ajuste, es el trabajo del \noptimizador\n, el cu\u00e1l implementa la \npropagaci\u00f3n hacia atr\u00e1s\n. \n\n\n\n\nResumiendo, el funcionamiento ser\u00eda el siguiente: inicialmente, los \npesos\n de cada \ncapa\n son asignados en forma aleatoria, por lo que la \nred\n simplemente implementa una serie de transformaciones aleatorias. En este primer paso, obviamente la salida del modelo dista bastante del ideal que deseamos obtener, por lo que el valor de la \nfunci\u00f3n de p\u00e9rdida\n va a ser bastante alto. Pero a medida que la \nred\n va procesando nuevos casos, los \npesos\n se van ajustando de forma tal de ir reduciendo cada vez m\u00e1s el valor de la \nfunci\u00f3n de p\u00e9rdida\n. Este proceso es el que se conoce como \nentrenamiento\n de la \nred\n, el cual repetido una suficiente cantidad de veces, generalmente 10 iteraciones de miles de ejemplos, logra que los \npesos\n se ajusten a los que minimizan la \nfunci\u00f3n de p\u00e9rdida\n. Una \nred\n que ha minimizado la \np\u00e9rdida\n es la que logra los resultados que mejor se ajustan a las salidas objetivo, es decir, que el modelo se encuentra \nentrenado\n. \n\n\nArquitecturas de Deep Learning \n\n\nLa estructura de datos fundamental de una \nred neuronal\n est\u00e1 vagamente inspirada en el cerebro humano. Cada una de nuestras c\u00e9lulas cerebrales (neuronas) est\u00e1 conectada a muchas otras neuronas por sinapsis. A medida que experimentamos e interactuamos con el mundo, nuestro cerebro crea nuevas conexiones, refuerza algunas conexiones y debilita a los dem\u00e1s. De esta forma, en nuestro cerebro se desarrollan ciertas regiones que se especializan en el procesamiento de determinadas \nentradas\n. As\u00ed vamos a tener un \u00e1rea especializada en la visi\u00f3n, otra que se especializa en la audici\u00f3n, otra para el lenguaje, etc. De forma similar, dependiendo del tipo de \nentradas\n con las que trabajemos, van a existir distintas \narquitecturas\n de \nredes neuronales\n que mejor se adaptan para procesar esa informaci\u00f3n. Algunas de las arquitecturas m\u00e1s populares son:\n\n\nRedes neuronales prealimentadas\n\n\nLas \nRedes neuronales prealimentadas\n fueron las primeras que se desarrollaron y son el modelo m\u00e1s sencillo. En estas redes la informaci\u00f3n se mueve en una sola direcci\u00f3n: hacia adelante. Los principales exponentes de este tipo de arquitectura son el \nperceptr\u00f3n\n y el \nperceptr\u00f3n multicapa\n. Se suelen utilizar en problemas de clasificaci\u00f3n simples. \n\n\n\n\nRedes neuronales convolucionales\n\n\nLas \nredes neuronales convolucionales\n son muy similares a las \nredes neuronales\n ordinarias como el \nperceptron multicapa\n; se componen de \nneuronas\n que tienen \npesos\n y \nsesgos\n que pueden aprender. Cada \nneurona\n recibe algunas entradas, realiza un \nproducto escalar\n y luego aplica una funci\u00f3n de activaci\u00f3n. Al igual que en el \nperceptron multicapa\n tambi\u00e9n vamos a tener una \nfunci\u00f3n de p\u00e9rdida o costo\n sobre la \u00faltima capa, la cual estar\u00e1 totalmente conectada. Lo que diferencia a las \nredes neuronales convolucionales\n es que suponen expl\u00edcitamente que las entradas son im\u00e1genes, lo que nos permite codificar ciertas propiedades en la arquitectura; permitiendo ganar en eficiencia y reducir la cantidad de par\u00e1metros en la red. \n\n\nEn general, las \nredes neuronales convolucionales\n van a estar construidas con una estructura que contendr\u00e1 3 tipos distintos de capas:\n\n\n\n\nUna capa \nconvolucional\n, que es la que le da le nombre a la red.\n\n\nUna capa de reducci\u00f3n o de \npooling\n, la cual va a reducir la cantidad de par\u00e1metros al quedarse con las caracter\u00edsticas m\u00e1s comunes.\n\n\nUna capa clasificadora totalmente conectada, la cual nos va dar el resultado final de la red.\n\n\n\n\nAlgunas implementaciones espec\u00edficas que podemos encontrar sobre este tipo de redes son: \ninception v3\n, \nResNet\n, \nVGG16\n y \nxception\n, entre otras. Todas ellas han logrado excelentes resultados.\n\n\n\n\nRedes neuronales recurrentes\n\n\nLos seres humanos no comenzamos nuestro pensamiento desde cero cada segundo, sino que los mismos tienen una persistencia. Las \nRedes neuronales prealimentadas\n tradicionales no cuentan con esta persistencia, y esto parece una deficiencia importante. Las \nRedes neuronales recurrentes\n abordan este problema. Son redes con bucles de retroalimentaci\u00f3n, que permiten que la informaci\u00f3n persista.\n\n\nUna \nRed neural recurrente\n puede ser pensada como una red con m\u00faltiples copias de ella misma, en las que cada una de ellas pasa un mensaje a su sucesor. Esta naturaleza en forma de cadena revela que las \nRedes neurales recurrentes\n est\u00e1n \u00edntimamente relacionadas con las secuencias y listas; por lo que son ideales para trabajar con este tipo de datos. En los \u00faltimos a\u00f1os, ha habido un \u00e9xito incre\u00edble aplicando \nRedes neurales recurrentes\n  a una variedad de problemas como: reconocimiento de voz, modelado de lenguaje, traducci\u00f3n, subt\u00edtulos de im\u00e1genes y la lista contin\u00faa.\n\n\nLas \nredes de memoria de largo plazo a corto plazo\n - generalmente llamadas \nLSTMs\n - son un tipo especial de \nRedes neurales recurrentes\n, capaces de aprender dependencias a largo plazo. Ellas tambi\u00e9n tienen una estructura como cadena, pero el m\u00f3dulo de repetici\u00f3n tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, tiene cuatro, que interact\u00faan de una manera especial permitiendo tener una memoria a m\u00e1s largo plazo.\n\n\n\n\nPara m\u00e1s informaci\u00f3n sobre diferentes arquitecturas de \nredes neuronales\n pueden visitar el siguiente art\u00edculo de \nwikipedia\n.\n\n\nLogros del Deep Learning \n\n\nEn los \u00faltimos a\u00f1os el \nDeep Learning\n ha producido toda una revoluci\u00f3n en el campo del \nMachine Learning\n, con resultados notables en todos los problemas de \npercepci\u00f3n\n, como \nver\n y \nescuchar\n, problemas que implican habilidades que parecen muy naturales e intuitivas para los seres humanos, pero que desde hace tiempo se han mostrado dif\u00edciles para las m\u00e1quinas. En particular, el \nDeep Learning\n ha logrado los siguientes avances, todos ellos en \u00e1reas hist\u00f3ricamente dif\u00edciles del \nMachine Learning\n.\n\n\n\n\nUn nivel casi humano para la clasificaci\u00f3n de im\u00e1genes.\n\n\nUn nivel casi humano para el reconocimiento del lenguaje hablado.\n\n\nUn nivel casi humano en el reconocimiento de escritura.\n\n\nGrandes mejoras en traducciones de lenguas.\n\n\nGrandes mejoras en conversaciones \ntext-to-speech\n.\n\n\nAsistentes digitales como Google Now o Siri.\n\n\nUn nivel casi humano en autos aut\u00f3nomos.\n\n\nMejores resultados de b\u00fasqueda en la web.\n\n\nGrandes mejoras para responder preguntas en lenguaje natural.\n\n\nAlcanzado Nivel maestro (superior al humano) en varios juegos.\n\n\n\n\nEn muchos sentidos, el \nDeep Learning\n todav\u00eda sigue siendo un campo misterioso para explorar, por lo que seguramente veremos nuevos avances en nuevas \u00e1reas utilizando estas t\u00e9cnicas. Tal vez alg\u00fan d\u00eda el \nDeep Learning\n ayuda a los seres humanos a hacer ciencia, desarrollar software y mucho m\u00e1s.\n\n\n\u00bfPor qu\u00e9 estos sorprendentes resultados surgen ahora?  \n\n\nMuchos de los conceptos del \nDeep Learning\n se desarrollaron en los a\u00f1os 80s y 90s, algunos incluso mucho antes. Sin embargo, los primeros resultados exitosos del \nDeep Learning\n surgieron en los \u00faltimos 5 a\u00f1os. \u00bfqu\u00e9 fue lo que cambio para lograr la popularidad y \u00e9xito de los modelos basados en \nDeep Learning\n en estos \u00faltimos a\u00f1os? \n\n\nSi bien existen m\u00faltiples factores para explicar esta \nrevoluci\u00f3n\n del \nDeep Learning\n, los dos principales componentes parecen ser la \ndisponibilidad de masivos vol\u00famenes de datos\n, lo que actualmente se conoce bajo el nombre de \nBig Data\n; y el \nprogreso en el poder de computo\n, especialmente gracias a los \nGPUs\n. Entonces, dentro de los factores que explican esta popularidad de los modelos de \nDeep Learning\n podemos encontrar:\n\n\n\n\n\n\nLa disponibilidad de conjuntos de datos enormes y de buena calidad\n. Gracias a la  revoluci\u00f3n digital en que nos encontramos, podemos generar conjuntos de datos enormes con los cuales alimentar a los algoritmos de \nDeep Learning\n, los cuales necesitan de muchos datos para poder \ngeneralizar\n.\n\n\n\n\n\n\nComputaci\u00f3n paralela masiva con \nGPUs\n. En l\u00edneas generales, los modelos de \nredes neuronales\n no son m\u00e1s que complicados c\u00e1lculos num\u00e9ricos que se realizan en paralelo. Gracias al desarrollo de los \nGPUs\n estos c\u00e1lculos ahora se pueden realizar en forma mucho m\u00e1s r\u00e1pida, permitiendo que podamos entrenar modelos m\u00e1s profundos y grandes. \n\n\n\n\n\n\nFunciones de activaci\u00f3n amigables para \nBackpropagation\n. La \nprogaci\u00f3n hacia atr\u00e1s\n o \nBackpropagation\n es el algoritmo fundamental que hace funcionar a las \nredes neuronales\n; pero la forma en que trabaja implica c\u00e1lculos realmente complicados. La transici\u00f3n desde funciones de activaci\u00f3n como \ntanh\n o \nsigmoid\n a funciones como \nReLU\n o \nSELU\n han simplificado estos problemas. \n\n\n\n\n\n\nNuevas arquitecturas\n. Arquitecturas como \nResnets\n, \ninception\n y \nGAN\n mantienen el campo actualizado y contin\u00faan aumentando las flexibilidad de los modelos. \n\n\n\n\n\n\nNuevas t\u00e9cnicas de \nregularizaci\u00f3n\n. T\u00e9cnicas como \ndropout\n, \nbatch normalization\n y \ndata-augmentation\n nos permiten entrenar redes m\u00e1s grandes con menos peligro de \nsobreajuste\n.\n\n\n\n\n\n\nOptimizadores m\u00e1s robustos\n. La \noptimizaci\u00f3n\n es fundamental para el funcionamiento de las \nredes neuronales\n. Mejoras sobre el tradicional procedimiento de \nSGD\n, como \nADAM\n han ayudado a mejorar el rendimiento de los modelos.\n\n\n\n\n\n\nPlataformas de software\n. Herramientas como \nTensorFlow\n, \nTheano\n, \nKeras\n, \nCNTK\n, \nPyTorch\n, \nChainer\n, y \nmxnet\n nos permiten crear prototipos en forma m\u00e1s r\u00e1pida y trabajar con \nGPUs\n sin tantas complicaciones. Nos permiten enfocarnos en la estructura del modelo sin tener que preocuparnos por los detalles de m\u00e1s bajo nivel.\n\n\n\n\n\n\nOtra raz\u00f3n por la que el \nDeep Learning\n ha tenido tanta repercusi\u00f3n \u00faltimamente adem\u00e1s de ofrecer un mejor rendimiento en muchos problemas; es que el \nDeep Learning\n esta haciendo la resoluci\u00f3n de problemas mucho m\u00e1s f\u00e1cil, ya que automatiza completamente lo que sol\u00eda ser uno de los pasos m\u00e1s dif\u00edciles y cruciales en el flujo de trabajo de \nMachine Learning\n: la \ningenier\u00eda de atributos\n. Antes del \nDeep Learning\n, para poder entrenar un modelo, primero deb\u00edamos refinar las \nentradas\n para adaptarlas al tipo de transformaci\u00f3n del modelo; ten\u00edamos que cuidadosamente \nseleccionar los atributos\n m\u00e1s representativos y desechar los poco informativos. El \nDeep Learning\n, en cambio, automatiza este proceso; aprendemos todos los atributos de una sola pasada y el mismo modelo se encarga de adaptarse y quedarse con lo m\u00e1s representativo.  \n\n\n\u00bfC\u00f3mo mantenerse actualizado en el campo de Deep Learning? \n\n\nEl campo del \nDeep Learning\n se mueve muy rapidamente, con varios \npapers\n que se publican por mes; por tal motivo, mantenerse actualizado con las \u00faltimas tendencias del campo puede ser bastante complicado. Algunos consejos pueden ser:\n\n\n\n\n\n\nEstarse atento a las publicaciones en \narxiv\n, especialmente a la secci\u00f3n de \nmachine learning\n. La mayor\u00eda de los \npapers\n m\u00e1s relevantes, los vamos a poder encontrar en esa plataforma.\n\n\n\n\n\n\nSeguir el blog de \nkeras\n en el cual podemos encontrar como implementar varios modelos utilizando esta genial librer\u00eda.\n\n\n\n\n\n\nSeguir el blog de \nopenai\n en d\u00f3nde detallan las investigaciones que van realizando, especialmente trabajando con \nGANs\n.\n\n\n\n\n\n\nSeguir el blog de \nGoogle research\n; en d\u00f3nde se viene haciendo bastante foco en los modelos de \nDeep Learning\n.\n\n\n\n\n\n\nUtilizar la secci\u00f3n de Machine Learning de \nreddit\n.\n\n\n\n\n\n\nSuscribirse al podcast \nTalking machines\n; en d\u00f3nde se entrevista a los principales exponentes del campo de la \ninteligencia artificial\n.\n\n\n\n\n\n\nPor \u00faltimo, obviamente estar atentos a las \npublicaciones que se realizan en \nIAAR\n.",
            "title": "Deep Learning"
        },
        {
            "location": "/deeplearning/#introduccion-al-deep-learning",
            "text": "",
            "title": "Introducci\u00f3n al Deep Learning"
        },
        {
            "location": "/deeplearning/#introduccion",
            "text": "El  Deep Learning  es sin duda el \u00e1rea de investigaci\u00f3n m\u00e1s popular dentro del campo de la  inteligencia artificial . La mayor\u00eda de las nuevas investigaciones que se realizan, trabajan con modelos basados en las t\u00e9cnicas de  Deep Learning ; ya que las mismas han logrado resultados sorprendes en campos como  Procesamiento del lenguaje natural  y  Visi\u00f3n por computadora . Pero... \u00bfqu\u00e9 es este misterioso concepto que ha ganado tanta popularidad? y... \u00bfc\u00f3mo se relaciona con el campo de la  inteligencia artificial  y el  Machine Learning ?.",
            "title": "Introducci\u00f3n "
        },
        {
            "location": "/deeplearning/#inteligencia-artificial-machine-learning-y-deep-learning",
            "text": "En general se suelen utilizar los t\u00e9rminos de  inteligencia artificial ,  Machine Learning  y  Deep Learning  en forma intercambiada. Sin embargo, \u00e9stos t\u00e9rminos no son los mismo y abarcan distintas cosas.",
            "title": "Inteligencia artificial, Machine learning y Deep learning "
        },
        {
            "location": "/deeplearning/#inteligencia-artificial",
            "text": "El t\u00e9rmino  inteligencia artificial  es el m\u00e1s general y engloba a los campos de  Machine Learning  y  Deep Learning  junto con otras t\u00e9cnicas como los  algoritmos de b\u00fasqueda , el  razonamiento simb\u00f3lico , el  razonamiento l\u00f3gico  y la  estad\u00edstica . Naci\u00f3 en los a\u00f1os 1950s, cuando un grupo de pioneros de la computaci\u00f3n comenzaron a preguntarse si se pod\u00eda hacer que las computadoras  pensaran . Una definici\u00f3n concisa de la  inteligencia artificial  ser\u00eda:  el esfuerzo para automatizar las tareas intelectuales que normalmente realizan los seres humanos .",
            "title": "Inteligencia Artificial"
        },
        {
            "location": "/deeplearning/#machine-learning",
            "text": "El  Machine Learning  o  Aprendizaje autom\u00e1tico  se refiere a un amplio conjunto de t\u00e9cnicas inform\u00e1ticas que nos permiten dar a las computadoras  la capacidad de aprender sin ser expl\u00edcitamente programadas . Hay muchos tipos diferentes de algoritmos de  Aprendizaje autom\u00e1tico , entre los que se encuentran el  aprendizaje por refuerzo , los  algoritmos gen\u00e9ticos , el aprendizaje basado en  reglas de asociaci\u00f3n , los  algoritmos de agrupamiento , los  \u00e1rboles de decisi\u00f3n , las  m\u00e1quinas de vectores de soporte  y las  redes neuronales . Actualmente, los algoritmos m\u00e1s populares dentro de este campo son los de  Deep Learning .",
            "title": "Machine Learning"
        },
        {
            "location": "/deeplearning/#deep-learning",
            "text": "El  Deep Learning  o  aprendizaje profundo  es un subcampo dentro del  Machine Learning , el cu\u00e1l utiliza distintas estructuras de  redes neuronales  para lograr el aprendizaje de sucesivas  capas de representaciones  cada vez m\u00e1s significativas de los datos. El  profundo  o  deep  en  Deep Learning  hace referencia a la cantidad de  capas de representaciones  que se utilizan en el modelo; en general se suelen utilizar decenas o incluso cientos de  capas de representaci\u00f3n . las cuales  aprenden  automaticamente a medida que el modelo es entrenado con los datos.",
            "title": "Deep Learning"
        },
        {
            "location": "/deeplearning/#que-es-el-deep-learning",
            "text": "Antes de poder entender que es el  Deep Learning , debemos en primer lugar conocer dos conceptos fundamentales: las  redes neuronales artificiales  y la  Propagaci\u00f3n hacia atr\u00e1s .",
            "title": "\u00bfQu\u00e9 es el Deep Learning? "
        },
        {
            "location": "/deeplearning/#redes-neuronales",
            "text": "Las  redes neuronales  son un modelo computacional basado en un gran conjunto de unidades neuronales simples ( neuronas artificiales ), de forma aproximadamente an\u00e1loga al comportamiento observado en los axones de las neuronas en los cerebros biol\u00f3gicos.   Cada una de estas neuronas simples, va a tener una forma similar al siguiente diagrama:   En donde sus componentes son:    $x_1, x_2, \\dots, x_n$: Los datos de entrada en la neurona, los cuales tambi\u00e9n puede ser que sean producto de la salida de otra neurona de la red.    $x_0$: La unidad de sesgo; un valor constante que se le suma a la entrada de la funci\u00f3n de activaci\u00f3n de la neurona. Generalmente tiene el valor 1. Este valor va a permitir cambiar la funci\u00f3n de activaci\u00f3n hacia la derecha o izquierda, otorg\u00e1ndole m\u00e1s flexibilidad para aprender a la neurona.    $w_0, w_1, w_2, \\dots, w_n$: Los pesos relativos de cada entrada. Tener en cuenta que incluso la unidad de sesgo tiene un peso.    a: La salida de la neurona. Que va a ser calculada de la siguiente forma:    $$a = f\\left(\\sum_{i=0}^n w_i \\cdot x_i \\right)$$  Aqu\u00ed $f$ es la  funci\u00f3n de activaci\u00f3n  de la neurona. Esta funci\u00f3n es la que le otorga tanta flexibilidad a las  redes neuronales  y le permite estimar complejas relaciones no lineales en los datos. Puede ser tanto una  funci\u00f3n lineal , una  funci\u00f3n log\u00edstica ,  hiperb\u00f3lica , etc.  Cada unidad neuronal est\u00e1 conectada con muchas otras y los enlaces entre ellas pueden incrementar o inhibir el estado de activaci\u00f3n de las neuronas adyacentes. Estos sistemas aprenden y se forman a s\u00ed mismos, en lugar de ser programados de forma expl\u00edcita, y sobresalen en \u00e1reas donde la detecci\u00f3n de soluciones o caracter\u00edsticas es dif\u00edcil de expresar con la programaci\u00f3n convencional.",
            "title": "Redes Neuronales"
        },
        {
            "location": "/deeplearning/#propagacion-hacia-atras",
            "text": "La  propagaci\u00f3n hacia atr\u00e1s  o  backpropagation  es un algoritmo que funciona mediante la determinaci\u00f3n de la p\u00e9rdida (o error) en la salida y luego propag\u00e1ndolo de nuevo hacia atr\u00e1s en la red. De esta forma los pesos se van actualizando para minimizar el error resultante de cada neurona. Este algoritmo es lo que les permite a las  redes neuronales  aprender.",
            "title": "Propagaci\u00f3n hacia atr\u00e1s"
        },
        {
            "location": "/deeplearning/#como-funciona-el-deep-learning",
            "text": "En general, cualquier t\u00e9cnica de  Machine Learning  trata de realizar la asignaci\u00f3n de entradas (por ejemplo, im\u00e1genes) a salidas objetivo (Por ejemplo, la etiqueta \"gato\"), mediante la observaci\u00f3n de un gran n\u00famero de ejemplos de entradas y salidas. El  Deep Learning  realiza este mapeo de entrada-a-objetivo por medio de una  red neuronal artificial  que est\u00e1 compuesta de un n\u00famero grande de  capas  dispuestas en forma de jerarqu\u00eda. La  red  aprende algo simple en la capa inicial de la jerarqu\u00eda y luego env\u00eda esta informaci\u00f3n a la siguiente capa. La siguiente capa toma esta informaci\u00f3n simple, lo combina en algo que es un poco m\u00e1s complejo, y lo pasa a la tercer capa. Este proceso contin\u00faa de forma tal que cada capa de la jerarqu\u00eda construye algo m\u00e1s complejo de la entrada que recibi\u00f3 de la capa anterior. De esta forma, la  red  ir\u00e1  aprendiendo  por medio de la exposici\u00f3n a los datos de ejemplo.  La especificaci\u00f3n de lo que cada  capa  hace a la entrada que recibe es almacenada en los  pesos  de la capa, que en esencia, no son m\u00e1s que n\u00fameros. Utilizando terminolog\u00eda m\u00e1s t\u00e9cnica podemos decir que la transformaci\u00f3n de datos que se produce en la  capa  es  parametrizada  por sus  pesos . Para que la  red  aprenda debemos encontrar los  pesos  de todas las  capas  de forma tal que la  red  realice un mapeo perfecto entre los ejemplos de entrada con sus respectivas salidas objetivo. Pero el problema reside en que una  red  de  Deep Learning  puede tener millones de  par\u00e1metros , por lo que encontrar el valor correcto de todos ellos puede ser una tarea realmente muy dif\u00edcil, especialmente si la modificaci\u00f3n del valor de uno de ellos afecta a todos los dem\u00e1s.   Para poder controlar algo, en primer lugar debemos poder observarlo. En este sentido, para controlar la salida de la  red neuronal , deber\u00edamos poder medir cuan lejos esta la salida que obtuvimos de la que se esperaba obtener. Este es el trabajo de la  funci\u00f3n de p\u00e9rdida  de la  red . Esta funci\u00f3n toma las predicciones que realiza el modelo y los valores objetivos (lo que realmente esperamos que la  red  produzca), y calcula cu\u00e1n lejos estamos de ese valor, de esta manera, podemos capturar que tan bien esta funcionando el modelo para el ejemplo especificado. El truco fundamental del  Deep Learning  es utilizar el valor que nos devuelve esta   funci\u00f3n de p\u00e9rdida  para retroalimentar la   red  y ajustar los  pesos  en la direcci\u00f3n que vayan reduciendo la  p\u00e9rdida  del modelo para cada ejemplo. Este ajuste, es el trabajo del  optimizador , el cu\u00e1l implementa la  propagaci\u00f3n hacia atr\u00e1s .    Resumiendo, el funcionamiento ser\u00eda el siguiente: inicialmente, los  pesos  de cada  capa  son asignados en forma aleatoria, por lo que la  red  simplemente implementa una serie de transformaciones aleatorias. En este primer paso, obviamente la salida del modelo dista bastante del ideal que deseamos obtener, por lo que el valor de la  funci\u00f3n de p\u00e9rdida  va a ser bastante alto. Pero a medida que la  red  va procesando nuevos casos, los  pesos  se van ajustando de forma tal de ir reduciendo cada vez m\u00e1s el valor de la  funci\u00f3n de p\u00e9rdida . Este proceso es el que se conoce como  entrenamiento  de la  red , el cual repetido una suficiente cantidad de veces, generalmente 10 iteraciones de miles de ejemplos, logra que los  pesos  se ajusten a los que minimizan la  funci\u00f3n de p\u00e9rdida . Una  red  que ha minimizado la  p\u00e9rdida  es la que logra los resultados que mejor se ajustan a las salidas objetivo, es decir, que el modelo se encuentra  entrenado .",
            "title": "\u00bfC\u00f3mo funciona el Deep Learning?  "
        },
        {
            "location": "/deeplearning/#arquitecturas-de-deep-learning",
            "text": "La estructura de datos fundamental de una  red neuronal  est\u00e1 vagamente inspirada en el cerebro humano. Cada una de nuestras c\u00e9lulas cerebrales (neuronas) est\u00e1 conectada a muchas otras neuronas por sinapsis. A medida que experimentamos e interactuamos con el mundo, nuestro cerebro crea nuevas conexiones, refuerza algunas conexiones y debilita a los dem\u00e1s. De esta forma, en nuestro cerebro se desarrollan ciertas regiones que se especializan en el procesamiento de determinadas  entradas . As\u00ed vamos a tener un \u00e1rea especializada en la visi\u00f3n, otra que se especializa en la audici\u00f3n, otra para el lenguaje, etc. De forma similar, dependiendo del tipo de  entradas  con las que trabajemos, van a existir distintas  arquitecturas  de  redes neuronales  que mejor se adaptan para procesar esa informaci\u00f3n. Algunas de las arquitecturas m\u00e1s populares son:",
            "title": "Arquitecturas de Deep Learning "
        },
        {
            "location": "/deeplearning/#redes-neuronales-prealimentadas",
            "text": "Las  Redes neuronales prealimentadas  fueron las primeras que se desarrollaron y son el modelo m\u00e1s sencillo. En estas redes la informaci\u00f3n se mueve en una sola direcci\u00f3n: hacia adelante. Los principales exponentes de este tipo de arquitectura son el  perceptr\u00f3n  y el  perceptr\u00f3n multicapa . Se suelen utilizar en problemas de clasificaci\u00f3n simples.",
            "title": "Redes neuronales prealimentadas"
        },
        {
            "location": "/deeplearning/#redes-neuronales-convolucionales",
            "text": "Las  redes neuronales convolucionales  son muy similares a las  redes neuronales  ordinarias como el  perceptron multicapa ; se componen de  neuronas  que tienen  pesos  y  sesgos  que pueden aprender. Cada  neurona  recibe algunas entradas, realiza un  producto escalar  y luego aplica una funci\u00f3n de activaci\u00f3n. Al igual que en el  perceptron multicapa  tambi\u00e9n vamos a tener una  funci\u00f3n de p\u00e9rdida o costo  sobre la \u00faltima capa, la cual estar\u00e1 totalmente conectada. Lo que diferencia a las  redes neuronales convolucionales  es que suponen expl\u00edcitamente que las entradas son im\u00e1genes, lo que nos permite codificar ciertas propiedades en la arquitectura; permitiendo ganar en eficiencia y reducir la cantidad de par\u00e1metros en la red.   En general, las  redes neuronales convolucionales  van a estar construidas con una estructura que contendr\u00e1 3 tipos distintos de capas:   Una capa  convolucional , que es la que le da le nombre a la red.  Una capa de reducci\u00f3n o de  pooling , la cual va a reducir la cantidad de par\u00e1metros al quedarse con las caracter\u00edsticas m\u00e1s comunes.  Una capa clasificadora totalmente conectada, la cual nos va dar el resultado final de la red.   Algunas implementaciones espec\u00edficas que podemos encontrar sobre este tipo de redes son:  inception v3 ,  ResNet ,  VGG16  y  xception , entre otras. Todas ellas han logrado excelentes resultados.",
            "title": "Redes neuronales convolucionales"
        },
        {
            "location": "/deeplearning/#redes-neuronales-recurrentes",
            "text": "Los seres humanos no comenzamos nuestro pensamiento desde cero cada segundo, sino que los mismos tienen una persistencia. Las  Redes neuronales prealimentadas  tradicionales no cuentan con esta persistencia, y esto parece una deficiencia importante. Las  Redes neuronales recurrentes  abordan este problema. Son redes con bucles de retroalimentaci\u00f3n, que permiten que la informaci\u00f3n persista.  Una  Red neural recurrente  puede ser pensada como una red con m\u00faltiples copias de ella misma, en las que cada una de ellas pasa un mensaje a su sucesor. Esta naturaleza en forma de cadena revela que las  Redes neurales recurrentes  est\u00e1n \u00edntimamente relacionadas con las secuencias y listas; por lo que son ideales para trabajar con este tipo de datos. En los \u00faltimos a\u00f1os, ha habido un \u00e9xito incre\u00edble aplicando  Redes neurales recurrentes   a una variedad de problemas como: reconocimiento de voz, modelado de lenguaje, traducci\u00f3n, subt\u00edtulos de im\u00e1genes y la lista contin\u00faa.  Las  redes de memoria de largo plazo a corto plazo  - generalmente llamadas  LSTMs  - son un tipo especial de  Redes neurales recurrentes , capaces de aprender dependencias a largo plazo. Ellas tambi\u00e9n tienen una estructura como cadena, pero el m\u00f3dulo de repetici\u00f3n tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, tiene cuatro, que interact\u00faan de una manera especial permitiendo tener una memoria a m\u00e1s largo plazo.   Para m\u00e1s informaci\u00f3n sobre diferentes arquitecturas de  redes neuronales  pueden visitar el siguiente art\u00edculo de  wikipedia .",
            "title": "Redes neuronales recurrentes"
        },
        {
            "location": "/deeplearning/#logros-del-deep-learning",
            "text": "En los \u00faltimos a\u00f1os el  Deep Learning  ha producido toda una revoluci\u00f3n en el campo del  Machine Learning , con resultados notables en todos los problemas de  percepci\u00f3n , como  ver  y  escuchar , problemas que implican habilidades que parecen muy naturales e intuitivas para los seres humanos, pero que desde hace tiempo se han mostrado dif\u00edciles para las m\u00e1quinas. En particular, el  Deep Learning  ha logrado los siguientes avances, todos ellos en \u00e1reas hist\u00f3ricamente dif\u00edciles del  Machine Learning .   Un nivel casi humano para la clasificaci\u00f3n de im\u00e1genes.  Un nivel casi humano para el reconocimiento del lenguaje hablado.  Un nivel casi humano en el reconocimiento de escritura.  Grandes mejoras en traducciones de lenguas.  Grandes mejoras en conversaciones  text-to-speech .  Asistentes digitales como Google Now o Siri.  Un nivel casi humano en autos aut\u00f3nomos.  Mejores resultados de b\u00fasqueda en la web.  Grandes mejoras para responder preguntas en lenguaje natural.  Alcanzado Nivel maestro (superior al humano) en varios juegos.   En muchos sentidos, el  Deep Learning  todav\u00eda sigue siendo un campo misterioso para explorar, por lo que seguramente veremos nuevos avances en nuevas \u00e1reas utilizando estas t\u00e9cnicas. Tal vez alg\u00fan d\u00eda el  Deep Learning  ayuda a los seres humanos a hacer ciencia, desarrollar software y mucho m\u00e1s.",
            "title": "Logros del Deep Learning "
        },
        {
            "location": "/deeplearning/#por-que-estos-sorprendentes-resultados-surgen-ahora",
            "text": "Muchos de los conceptos del  Deep Learning  se desarrollaron en los a\u00f1os 80s y 90s, algunos incluso mucho antes. Sin embargo, los primeros resultados exitosos del  Deep Learning  surgieron en los \u00faltimos 5 a\u00f1os. \u00bfqu\u00e9 fue lo que cambio para lograr la popularidad y \u00e9xito de los modelos basados en  Deep Learning  en estos \u00faltimos a\u00f1os?   Si bien existen m\u00faltiples factores para explicar esta  revoluci\u00f3n  del  Deep Learning , los dos principales componentes parecen ser la  disponibilidad de masivos vol\u00famenes de datos , lo que actualmente se conoce bajo el nombre de  Big Data ; y el  progreso en el poder de computo , especialmente gracias a los  GPUs . Entonces, dentro de los factores que explican esta popularidad de los modelos de  Deep Learning  podemos encontrar:    La disponibilidad de conjuntos de datos enormes y de buena calidad . Gracias a la  revoluci\u00f3n digital en que nos encontramos, podemos generar conjuntos de datos enormes con los cuales alimentar a los algoritmos de  Deep Learning , los cuales necesitan de muchos datos para poder  generalizar .    Computaci\u00f3n paralela masiva con  GPUs . En l\u00edneas generales, los modelos de  redes neuronales  no son m\u00e1s que complicados c\u00e1lculos num\u00e9ricos que se realizan en paralelo. Gracias al desarrollo de los  GPUs  estos c\u00e1lculos ahora se pueden realizar en forma mucho m\u00e1s r\u00e1pida, permitiendo que podamos entrenar modelos m\u00e1s profundos y grandes.     Funciones de activaci\u00f3n amigables para  Backpropagation . La  progaci\u00f3n hacia atr\u00e1s  o  Backpropagation  es el algoritmo fundamental que hace funcionar a las  redes neuronales ; pero la forma en que trabaja implica c\u00e1lculos realmente complicados. La transici\u00f3n desde funciones de activaci\u00f3n como  tanh  o  sigmoid  a funciones como  ReLU  o  SELU  han simplificado estos problemas.     Nuevas arquitecturas . Arquitecturas como  Resnets ,  inception  y  GAN  mantienen el campo actualizado y contin\u00faan aumentando las flexibilidad de los modelos.     Nuevas t\u00e9cnicas de  regularizaci\u00f3n . T\u00e9cnicas como  dropout ,  batch normalization  y  data-augmentation  nos permiten entrenar redes m\u00e1s grandes con menos peligro de  sobreajuste .    Optimizadores m\u00e1s robustos . La  optimizaci\u00f3n  es fundamental para el funcionamiento de las  redes neuronales . Mejoras sobre el tradicional procedimiento de  SGD , como  ADAM  han ayudado a mejorar el rendimiento de los modelos.    Plataformas de software . Herramientas como  TensorFlow ,  Theano ,  Keras ,  CNTK ,  PyTorch ,  Chainer , y  mxnet  nos permiten crear prototipos en forma m\u00e1s r\u00e1pida y trabajar con  GPUs  sin tantas complicaciones. Nos permiten enfocarnos en la estructura del modelo sin tener que preocuparnos por los detalles de m\u00e1s bajo nivel.    Otra raz\u00f3n por la que el  Deep Learning  ha tenido tanta repercusi\u00f3n \u00faltimamente adem\u00e1s de ofrecer un mejor rendimiento en muchos problemas; es que el  Deep Learning  esta haciendo la resoluci\u00f3n de problemas mucho m\u00e1s f\u00e1cil, ya que automatiza completamente lo que sol\u00eda ser uno de los pasos m\u00e1s dif\u00edciles y cruciales en el flujo de trabajo de  Machine Learning : la  ingenier\u00eda de atributos . Antes del  Deep Learning , para poder entrenar un modelo, primero deb\u00edamos refinar las  entradas  para adaptarlas al tipo de transformaci\u00f3n del modelo; ten\u00edamos que cuidadosamente  seleccionar los atributos  m\u00e1s representativos y desechar los poco informativos. El  Deep Learning , en cambio, automatiza este proceso; aprendemos todos los atributos de una sola pasada y el mismo modelo se encarga de adaptarse y quedarse con lo m\u00e1s representativo.",
            "title": "\u00bfPor qu\u00e9 estos sorprendentes resultados surgen ahora?  "
        },
        {
            "location": "/deeplearning/#como-mantenerse-actualizado-en-el-campo-de-deep-learning",
            "text": "El campo del  Deep Learning  se mueve muy rapidamente, con varios  papers  que se publican por mes; por tal motivo, mantenerse actualizado con las \u00faltimas tendencias del campo puede ser bastante complicado. Algunos consejos pueden ser:    Estarse atento a las publicaciones en  arxiv , especialmente a la secci\u00f3n de  machine learning . La mayor\u00eda de los  papers  m\u00e1s relevantes, los vamos a poder encontrar en esa plataforma.    Seguir el blog de  keras  en el cual podemos encontrar como implementar varios modelos utilizando esta genial librer\u00eda.    Seguir el blog de  openai  en d\u00f3nde detallan las investigaciones que van realizando, especialmente trabajando con  GANs .    Seguir el blog de  Google research ; en d\u00f3nde se viene haciendo bastante foco en los modelos de  Deep Learning .    Utilizar la secci\u00f3n de Machine Learning de  reddit .    Suscribirse al podcast  Talking machines ; en d\u00f3nde se entrevista a los principales exponentes del campo de la  inteligencia artificial .    Por \u00faltimo, obviamente estar atentos a las  publicaciones que se realizan en  IAAR .",
            "title": "\u00bfC\u00f3mo mantenerse actualizado en el campo de Deep Learning? "
        },
        {
            "location": "/ML/",
            "text": "Introducci\u00f3n al Machine Learning\n\n\n\n\nUna de las ramas de estudio que cada vez esta ganando m\u00e1s popularidad dentro de las \nciencias de la computaci\u00f3n\n es el \naprendizaje autom\u00e1tico\n o \nMachine Learning\n. Muchos de los servicios que utilizamos en nuestro d\u00eda a d\u00eda como google, gmail, netflix, spotify o amazon se valen de las herramientas que les brinda el \nMachine Learning\n para alcanzar un servicio cada vez m\u00e1s personalizado y lograr as\u00ed ventajas competitivas sobre sus rivales. \n\n\n\u00bfQu\u00e9 es Machine Learning?\n\n\nPero, \u00bfqu\u00e9 es exactamente \nMachine Learning\n?. El \nMachine Learning\n es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del \nMachine Learning\n es \ngeneralizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada\n. El ejemplo m\u00e1s t\u00edpico donde podemos ver el uso del \nMachine Learning\n es en el filtrado de los correo basura o spam. Mediante la observaci\u00f3n de miles de correos electr\u00f3nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos. El \nMachine Learning\n combina conceptos y t\u00e9cnicas de diferentes \u00e1reas del conocimiento, como las \nmatem\u00e1ticas\n, \nestad\u00edsticas\n y las \nciencias de la computaci\u00f3n\n; por tal motivo, hay muchas maneras de aprender la disciplina.\n\n\nTipos de Machine Learning\n\n\nEl \nMachine Learning\n tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito, an\u00e1lisis del mercado de valores, clasificaci\u00f3n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob\u00f3tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distingir los distintos tipos de problemas de \nMachine Learning\n con los que nos podemos encontrar.\n\n\nAprendizaje supervisado\n\n\nEn los problemas de \naprendizaje supervisado\n se ense\u00f1a o entrena al \nalgoritmo\n a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos m\u00e1s el \nalgoritmo\n puede aprender sobre el tema. Una vez conclu\u00eddo el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el \nalgoritmo\n de aprendizaje utiliza la experiencia pasada que adquiri\u00f3 durante la etapa de entrenamiento para predecir un resultado. Esto es similar al m\u00e9todo de aprendizaje que se utiliza en las escuelas, donde se nos ense\u00f1an problemas y las formas de resolverlos, para que luego podamos aplicar los mismos m\u00e9todos en situaciones similares.\n\n\nAprendizaje no supervisado\n\n\nEn los problemas de \naprendizaje no supervisado\n el \nalgoritmo\n es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al \nalgoritmo\n lo que representan los datos. La idea es que el \nalgoritmo\n pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos. El \naprendizaje no supervisado\n es similar al m\u00e9todo que utilizamos para aprender a hablar cuando somos bebes, en un principio escuchamos hablar a nuestros padres y no entendemos nada; pero a medida que vamos escuchando miles de conversaciones, nuestro cerebro comenzar\u00e1 a formar un modelo sobre c\u00f3mo funciona el lenguaje y comenzaremos a reconocer patrones y a esperar ciertos sonidos. \n\n\nAprendizaje por refuerzo\n\n\nEn los problemas de aprendizaje por refuerzo, el \nalgoritmo\n aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m\u00e1s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar\u00e1 creando una estrategia ganadora.\n\n\nSobreajuste\n\n\nComo mencionamos cuando definimos al \nMachine Learning\n, la idea fundamental es encontrar patrones que podamos generalizar para luego poder aplicar esta generalizaci\u00f3n sobre los casos que todav\u00eda no hemos observado y realizar predicciones. Pero tambi\u00e9n puede ocurrir que durante el entrenamiento solo descubramos casualidades en los datos que se parecen a patrones interesantes, pero que no generalicen. Esto es lo que se conoce con el nombre de \nsobreajuste\n o sobreentrenamiento\n.\n\n\nEl \nsobreajuste\n es la tendencia que tienen la mayor\u00eda de los \nalgoritmos\n de \nMachine Learning\n a ajustarse a unas caracter\u00edsticas muy espec\u00edficas de los datos de entrenamiento que no tienen relaci\u00f3n causal con la \nfunci\u00f3n objetivo\n que estamos buscando para generalizar. El ejemplo m\u00e1s extremo de un modelo \nsobreajustado\n es un modelo que solo memoriza las respuestas correctas; este modelo al ser utilizado con datos que nunca antes ha visto va a tener un rendimiento azaroso, ya que nunca logr\u00f3 generalizar un patr\u00f3n para predecir.\n\n\nComo evitar el sobreajuste\n\n\nComo mencionamos anteriormente, todos los modelos de \nMachine Learning\n tienen tendencia al \nsobreajuste\n; es por esto que debemos aprender a convivir con el mismo y tratar de tomar medidas preventivas para reducirlo lo m\u00e1s posible. Las dos principales estrategias para lidiar son el \nsobreajuste\n son: la \nretenci\u00f3n de datos\n y la \nvalidaci\u00f3n cruzada\n.\n\n\nEn el primer caso, la idea es dividir nuestro \nconjunto de datos\n, en uno o varios conjuntos de entrenamiento y otro/s conjuntos de evaluaci\u00f3n. Es decir, que no le vamos a pasar todos nuestros datos al \nalgoritmo\n durante el entrenamiento, sino que vamos a \nretener\n una parte de los datos de entrenamiento para realizar una evaluaci\u00f3n de la efectividad del modelo. Con esto lo que buscamos es evitar que los mismos datos que usamos para entrenar sean los mismos que utilizamos para evaluar. De esta forma vamos a poder analizar con m\u00e1s precisi\u00f3n como el modelo se va comportando a medida que m\u00e1s lo vamos entrenando y poder detectar el punto cr\u00edtico en el que el modelo deja de generalizar y comienza a \nsobreajustarse\n a los datos de entrenamiento.\n\n\nLa \nvalidaci\u00f3n cruzada\n es un procedimiento m\u00e1s sofisticado que el anterior. En lugar de solo obtener una simple estimaci\u00f3n de la efectividad de la \ngeneralizaci\u00f3n\n; la idea es realizar un an\u00e1lisis estad\u00edstico para obtener otras medidas del rendimiento estimado, como la media y la varianza, y as\u00ed poder entender c\u00f3mo se espera que el rendimiento var\u00ede a trav\u00e9s de los distintos conjuntos de datos. Esta variaci\u00f3n es fundamental para la evaluaci\u00f3n de la confianza en la estimaci\u00f3n del rendimiento.\nLa \nvalidaci\u00f3n cruzada\n tambi\u00e9n hace un mejor uso de un conjunto de datos limitado; ya que a diferencia de la simple divisi\u00f3n de los datos en uno el entrenamiento y otro de evaluaci\u00f3n; la \nvalidaci\u00f3n cruzada\n calcula sus estimaciones sobre todo el \nconjunto de datos\n mediante la realizaci\u00f3n de m\u00faltiples divisiones e intercambios sistem\u00e1ticos entre datos de entrenamiento y datos de evaluaci\u00f3n.\n\n\nPasos para construir un modelo de machine learning\n\n\nConstruir un modelo de \nMachine Learning\n, no se reduce solo a utilizar un \nalgoritmo\n de aprendizaje o utilizar una librer\u00eda de \nMachine Learning\n; sino que es todo un proceso que suele involucrar los siguientes pasos:\n\n\n\n\n\n\nRecolectar los datos\n. Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una \nAPI\n o desde una base de datos. Podemos tambi\u00e9n utilizar otros dispositivos que recolectan los datos por nosotros; o utilizar datos que son de dominio p\u00fablico. El n\u00famero de opciones que tenemos para recolectar datos no tiene fin!. Este paso parece obvio, pero es uno de los que m\u00e1s complicaciones trae y m\u00e1s tiempo consume. \n\n\n\n\n\n\nPreprocesar los datos\n. Una vez que tenemos los datos, tenemos que asegurarnos que tiene el formato correcto para nutrir nuestro \nalgoritmo\n de aprendizaje. Es pr\u00e1cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m\u00e1s sencillo que el paso anterior.\n\n\n\n\n\n\nExplorar los datos\n. Una vez que ya tenemos los datos y est\u00e1n con el formato correcto, podemos realizar un pre an\u00e1lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg\u00fan patr\u00f3n en los mismos que nos facilite la construcci\u00f3n del modelo. En esta etapa suelen ser de mucha utilidad las medidas estad\u00edsticas y los gr\u00e1ficos en 2 y 3 dimensiones para tener una idea visual de como se comportan nuestros datos. En este punto podemos detectar \nvalores at\u00edpicos\n que debamos descartar; o encontrar las caracter\u00edsticas que m\u00e1s influencia tienen para realizar una predicci\u00f3n.\n\n\n\n\n\n\nEntrenar el \nalgoritmo\n. Aqu\u00ed es donde comenzamos a utilizar las t\u00e9cnicas de \nMachine Learning\n realmente. En esta etapa nutrimos al o los \nalgoritmos\n de aprendizaje con los datos que venimos procesando en las etapas anteriores. La idea es que los \nalgoritmos\n puedan extraer informaci\u00f3n \u00fatil de los datos que le pasamos para luego poder hacer predicciones. \n\n\n\n\n\n\nEvaluar el \nalgoritmo\n. En esta etapa ponemos a prueba la informaci\u00f3n o conocimiento que el \nalgoritmo\n obtuvo del entrenamiento del paso anterior. Evaluamos que tan preciso es el algoritmo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el \nalgoritmo\n cambiando algunos par\u00e1metros hasta lograr un rendimiento aceptable.  \n\n\n\n\n\n\nUtilizar el modelo\n. En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu\u00ed tambi\u00e9n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores. \n\n\n\n\n\n\nLibrer\u00edas de Python para machine learning\n\n\nComo siempre me gusta comentar, una de las grandes ventajas que ofrece \nPython\n sobre otros lenguajes de programaci\u00f3n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer\u00edas de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de \nMachine Learning\n, las principales librer\u00edas que podemos utilizar son: \n\n\nScikit-Learn\n\n\nScikit-learn\n es la principal librer\u00eda que existe para trabajar con \nMachine Learning\n, incluye la implementaci\u00f3n de un gran n\u00famero de \nalgoritmos\n de aprendizaje. La podemos utilizar para \nclasificaciones\n, \nextraccion de caracter\u00edsticas\n, \nregresiones\n, \nagrupaciones\n, \nreducci\u00f3n de dimensiones\n, \nselecci\u00f3n de modelos\n, o \npreprocesamiento\n. Posee una \nAPI\n que es consistente en todos los modelos y se integra muy bien con el resto de los paquetes cient\u00edficos que ofrece \nPython\n. Esta librer\u00eda tambi\u00e9n nos facilita las tareas de evaluaci\u00f3n, diagnostico y \nvalidaciones cruzadas\n ya que nos proporciona varios m\u00e9todos de f\u00e1brica para poder realizar estas tareas en forma muy simple. \n\n\nStatsmodels\n\n\nStatsmodels\n es otra gran librer\u00eda que hace foco en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios. Al igual que \nScikit-learn\n, tambi\u00e9n se integra muy bien con el resto de los paquetes cientificos de \nPython\n. Si deseamos ajustar modelos lineales, hacer una an\u00e1lisis estad\u00edstico, o tal vez un poco de modelado predictivo, entonces \nStatsmodels\n es la librer\u00eda ideal. Las pruebas estad\u00edsticas que ofrece son bastante amplias y abarcan tareas de validaci\u00f3n para la mayor\u00eda de los casos. \n\n\nPyMC\n\n\npyMC\n es un m\u00f3dulo de \nPython\n que implementa modelos estad\u00edsticos bayesianos, incluyendo la \ncadena de Markov Monte Carlo(MCMC)\n. \npyMC\n  ofrece funcionalidades para hacer el an\u00e1lisis bayesiano lo mas simple posible. Incluye los modelos \nbayesianos\n,  distribuciones estad\u00edsticas y herramientas de diagnostico  para la covarianza de los modelos. Si queremos realizar un an\u00e1lisis \nbayesiano\n esta es sin duda la librer\u00eda a utilizar.\n\n\nNTLK\n\n\nNLTK\n es la librer\u00eda l\u00edder para el procesamiento del lenguaje natural o \nNLP\n por sus siglas en ingl\u00e9s. Proporciona interfaces f\u00e1ciles de usar a m\u00e1s de 50 cuerpos y recursos l\u00e9xicos, como \nWordNet\n, junto con un conjunto de bibliotecas de procesamiento de texto para la clasificaci\u00f3n, tokenizaci\u00f3n, el etiquetado, el an\u00e1lisis y el razonamiento sem\u00e1ntico. \n\n\nObviamente, aqu\u00ed solo estoy listando unas pocas de las muchas librer\u00edas que existen en \nPython\n para trabajar con problemas de \nMachine Learning\n, los invito a realizar su propia investigaci\u00f3n sobre el tema.\n\n\nAlgoritmos m\u00e1s utilizados\n\n\nLos \nalgoritmos\n  que m\u00e1s se suelen utilizar en los problemas de \nMachine Learning\n son los siguientes:\n\n\n\n\nRegresi\u00f3n Lineal\n\n\nRegresi\u00f3n Log\u00edstica\n\n\nArboles de Decision\n\n\nRandom Forest\n\n\nSVM\n o M\u00e1quinas de vectores de soporte.\n\n\nKNN\n o K vecinos m\u00e1s cercanos.\n\n\nK-means\n\n\n\n\nTodos ellos se pueden aplicar a casi cualquier problema de datos y obviamente estan todos implementados por la excelente librer\u00eda de \nPython\n, \nScikit-learn\n. Veamos algunos ejemplos de ellos.\n\n\nRegresi\u00f3n Lineal\n\n\nSe utiliza para estimar los valores reales (costo de las viviendas, el n\u00famero de llamadas, ventas totales, etc.) basados en variables continuas. La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar una mejor l\u00ednea recta con respecto a los puntos. Esta l\u00ednea de mejor ajuste se conoce como l\u00ednea de regresi\u00f3n y esta representada por la siguiente ecuaci\u00f3n lineal:\n\n\n$$Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{n}X_{n}$$\n\n\nVeamos un peque\u00f1o ejemplo de como se implementa en \nPython\n. En este ejemplo voy a utilizar el dataset Boston que ya viene junto con \nScikit-learn\n y es ideal para practicar con \nRegresiones Lineales\n; el mismo contiene precios de casas de varias \u00e1reas de la ciudad de Boston. \n\n\n# importando pandas, numpy y matplotlib\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# importando los datasets de sklearn\nfrom sklearn import datasets\n\nboston = datasets.load_boston()\nboston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\nboston_df['TARGET'] = boston.target\nboston_df.head() # estructura de nuestro dataset.\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \nTARGET\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0.00632\n\n      \n18.0\n\n      \n2.31\n\n      \n0.0\n\n      \n0.538\n\n      \n6.575\n\n      \n65.2\n\n      \n4.0900\n\n      \n1.0\n\n      \n296.0\n\n      \n15.3\n\n      \n396.90\n\n      \n4.98\n\n      \n24.0\n\n    \n\n    \n\n      \n1\n\n      \n0.02731\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n6.421\n\n      \n78.9\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n396.90\n\n      \n9.14\n\n      \n21.6\n\n    \n\n    \n\n      \n2\n\n      \n0.02729\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n7.185\n\n      \n61.1\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n392.83\n\n      \n4.03\n\n      \n34.7\n\n    \n\n    \n\n      \n3\n\n      \n0.03237\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n6.998\n\n      \n45.8\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n394.63\n\n      \n2.94\n\n      \n33.4\n\n    \n\n    \n\n      \n4\n\n      \n0.06905\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n7.147\n\n      \n54.2\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n396.90\n\n      \n5.33\n\n      \n36.2\n\n    \n\n  \n\n\n\n\n\n# importando el modelo de regresi\u00f3n lineal\nfrom sklearn.linear_model import LinearRegression\n\nrl = LinearRegression() # Creando el modelo.\nrl.fit(boston.data, boston.target) # ajustando el modelo\n\n# haciendo las predicciones\npredicciones = rl.predict(boston.data)\npredicciones_df = pd.DataFrame(predicciones, columns=['Pred'])\npredicciones_df.head() # predicciones de las primeras 5 lineas\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPred\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n30.008213\n\n    \n\n    \n\n      \n1\n\n      \n25.029861\n\n    \n\n    \n\n      \n2\n\n      \n30.570232\n\n    \n\n    \n\n      \n3\n\n      \n28.608141\n\n    \n\n    \n\n      \n4\n\n      \n27.942882\n\n    \n\n  \n\n\n\n\n\n# Calculando el desvio\nnp.mean(boston.target - predicciones)\n\n\n\n\nComo podemos ver, el desv\u00edo del modelo es peque\u00f1o, por lo que sus resultados para este ejemplo son bastante confiables.\n\n\nRegresi\u00f3n Log\u00edstica\n\n\nLos modelos lineales, tambi\u00e9n pueden ser utilizados para clasificaciones; es decir, que primero ajustamos el modelo lineal a la probabilidad de que una cierta clase o categor\u00eda ocurra y, a luego, utilizamos una funci\u00f3n para crear un umbral en el cual especificamos el resultado de una de estas clases o categor\u00edas. La funci\u00f3n que utiliza este modelo, no es ni m\u00e1s ni menos que la funci\u00f3n log\u00edstica.\n\n\n$$f(x) = \\frac{1}{1 + e^{-1}}$$\n\n\nVeamos, aqu\u00ed tambi\u00e9n un peque\u00f1o ejemplo en \nPython\n.\n\n\n# Creando un dataset de ejemplo \nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1000, n_features=4)\n\n# Importando el modelo\nfrom sklearn.linear_model import LogisticRegression\n\nrlog = LogisticRegression() # Creando el modelo\n\n# Dividiendo el dataset en entrenamiento y evaluacion\nX_entrenamiento = X[:-200]\nX_evaluacion = X[-200:]\ny_entrenamiento = y[:-200]\ny_evaluacion = y[-200:]\n\nrlog.fit(X_entrenamiento, y_entrenamiento) #ajustando el modelo\n\n# Realizando las predicciones\ny_predic_entrenamiento = rlog.predict(X_entrenamiento) \ny_predic_evaluacion = rlog.predict(X_evaluacion)\n\n# Verificando la exactitud del modelo\nentrenamiento = (y_predic_entrenamiento == y_entrenamiento).sum().astype(float) / y_entrenamiento.shape[0]\nprint(\"sobre datos de entrenamiento: {0:.2f}\".format(entrenamiento))\nevaluacion = (y_predic_evaluacion == y_evaluacion).sum().astype(float) / y_evaluacion.shape[0]\nprint(\"sobre datos de evaluaci\u00f3n: {0:.2f}\".format(evaluacion))\n\nsobre datos de entrenamiento: 0.95\nsobre datos de evaluaci\u00f3n: 0.94\n\n\n\n\nComo podemos ver en este ejemplo tambi\u00e9n nuestro modelo tiene bastante precisi\u00f3n clasificando las categor\u00edas de nuestro dataset.\n\n\nArboles de decisi\u00f3n\n\n\nLos \nArboles de Decision\n son diagramas con construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema.\nLos \nArboles de Decision\n est\u00e1n compuestos por nodos interiores, nodos terminales y ramas que emanan de los nodos interiores. Cada nodo interior en el \u00e1rbol contiene una prueba de un atributo, y cada rama representa un valor distinto del atributo. Siguiendo las ramas desde el nodo ra\u00edz hacia abajo, cada ruta finalmente termina en un nodo terminal creando una segmentaci\u00f3n de los datos. Veamos aqu\u00ed tambi\u00e9n un peque\u00f1o ejemplo en \nPython\n.\n\n\n# Creando un dataset de ejemplo\nX, y = datasets.make_classification(1000, 20, n_informative=3)\n\n# Importando el arbol de decisi\u00f3n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nad = DecisionTreeClassifier(criterion='entropy', max_depth=5) # Creando el modelo\nad.fit(X, y) # Ajustando el modelo\n\n#generando archivo para graficar el arbol\nwith open(\"mi_arbol.dot\", 'w') as archivo_dot:\n    tree.export_graphviz(ad, out_file = archivo_dot)\n\n# utilizando el lenguaje dot para graficar el arbol.\n!dot -Tjpeg mi_arbol.dot -o arbol_decision.jpeg\n\n\n\n\nLuego de usar el lenguaje \ndot\n para convertir nuestro arbol a formato jpeg, ya podemos ver la imagen del mismo.\n\n\n\n\n# verificando la precisi\u00f3n\nprint(\"precisi\u00f3n del modelo: {0: .2f}\".format((y == ad.predict(X)).mean()))\n\nprecisi\u00f3n del modelo:  0.94\n\n\n\n\nEn este ejemplo, nuestro \u00e1rbol tiene una precisi\u00f3n del 89%. Tener en cuenta que los \nArboles de Decision\n tienen tendencia al \nsobreajuste\n.\n\n\nRandom Forest\n\n\nEn lugar de utilizar solo un arbol para decidir, \u00bfpor qu\u00e9 no utilizar todo un bosque?!!. Esta es la idea central detr\u00e1s del \nalgoritmo\n de \nRandom Forest\n. Tarbaja construyendo una gran cantidad de \narboles de decision\n muy poco profundos, y luego toma la clase que\ncada \u00e1rbol eligi\u00f3. Esta idea es muy poderosa en \nMachine Learning\n. Si tenemos en cuenta que un sencillo clasificador entrenado podr\u00eda tener s\u00f3lo el 60 por ciento de precisi\u00f3n, podemos entrenar un mont\u00f3n de clasificadores que sean por lo general acertados y luego podemos utilizar la sabidur\u00eda de todos los aprendices juntos.\nCon \nPython\n los podemos utilizar de la siguiente manera:\n\n\n# Creando un dataset de ejemplo\nX, y = datasets.make_classification(1000)\n\n# Importando el random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier() # Creando el modelo\nrf.fit(X, y) # Ajustando el modelo\n\n# verificando la precisi\u00f3n\nprint(\"precisi\u00f3n del modelo: {0: .2f}\".format((y == rf.predict(X)).mean()))\n\nprecisi\u00f3n del modelo:  1.00\n\n\n\n\nSVM o M\u00e1quinas de vectores de soporte\n\n\nLa idea detr\u00e1s de \nSVM\n es encontrar un plano que separe los grupos dentro de los datos de la mejor forma posible. Aqu\u00ed, la separaci\u00f3n significa que la elecci\u00f3n\ndel plano maximiza el margen entre los puntos m\u00e1s cercanos en el plano; \u00e9stos puntos se denominan vectores de soporte. Pasemos al ejemplo.\n\n\n# importanto SVM\nfrom sklearn import svm\n\n# importando el dataset iris\niris = datasets.load_iris()\nX = iris.data[:, :2]  # solo tomamos las primeras 2 caracter\u00edsticas\ny = iris.target\n\nh = .02  # tama\u00f1o de la malla del grafico\n\n# Creando el SVM con sus diferentes m\u00e9todos\nC = 1.0  # parametro de regulacion SVM \nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\nlin_svc = svm.LinearSVC(C=C).fit(X, y)\n\n# crear el area para graficar\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# titulos de los graficos\ntitles = ['SVC con el motor lineal',\n          'LinearSVC',\n          'SVC con el motor RBF',\n          'SVC con el motor polinomial']\n\n\nfor i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n    # Realizando el gr\u00e1fico, se le asigna un color a cada punto\n    plt.subplot(2, 2, i + 1)\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n    # Graficando tambien los puntos de datos\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('largo del petalo')\n    plt.ylabel('ancho del petalo')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(titles[i])\n\nplt.show()\n\n\n\n\n\n\nKNN o k vecinos m\u00e1s cercanos\n\n\nEste es un m\u00e9todo de clasificaci\u00f3n no param\u00e9trico, que estima el valor de la probabilidad a posteriori de que un elemento $x$ pertenezca a una clase en particular a partir de la informaci\u00f3n proporcionada por el conjunto de prototipos.\nLa regresi\u00f3n \nKNN\n se calcula simplemente tomando el promedio del punto k m\u00e1s cercano al punto que se est\u00e1 probando. \n\n\n\n# Creando el dataset iris\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# importando KNN \nfrom sklearn.neighbors import KNeighborsRegressor\n\nknnr = KNeighborsRegressor(n_neighbors=10) # Creando el modelo con 10 vecinos\nknnr.fit(X, y) # Ajustando el modelo\n\n# Verificando el error medio del modelo\nprint(\"El error medio del modelo es: {:.2f}\".format(np.power(y - knnr.predict(X),\n2).mean()))\n\nEl error medio del modelo es: 0.02\n\n\n\n\nK-means\n\n\nK-means\n es probablemente uno de los algoritmos de agrupamiento m\u00e1s conocidos y, en un sentido m\u00e1s amplio, una de las t\u00e9cnicas de aprendizaje no supervisado m\u00e1s conocidas.\n\nK-means\n es en realidad un \nalgoritmo\n muy simple que funciona para reducir al m\u00ednimo la suma de las distancias cuadradas desde la media dentro del agrupamiento. Para hacer esto establece primero un n\u00famero previamente especificado de conglomerados, K, y luego va asignando cada observaci\u00f3n a la agrupaci\u00f3n m\u00e1s cercana de acuerdo a su media. Veamos el ejemplo\n\n\n# Creando el dataset\ngrupos, pos_correcta = datasets.make_blobs(1000, centers=3,\ncluster_std=1.75)\n\n# Graficando los grupos de datos\nf, ax = plt.subplots(figsize=(7, 5))\ncolores = ['r', 'g', 'b']\n\nfor i in range(3):\n    p = grupos[pos_correcta == i]\n    ax.scatter(p[:,0], p[:,1], c=colores[i],\n               label=\"Grupo {}\".format(i))\n\nax.set_title(\"Agrupamiento perfecto\")\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n# importando KMeans\nfrom sklearn.cluster import KMeans\n\n# Creando el modelo\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(grupos) # Ajustando el modelo\n\n# verificando los centros de los grupos\nkmeans.cluster_centers_\n\n# Graficando segun modelo\nf, ax = plt.subplots(figsize=(7, 5))\ncolores = ['r', 'g', 'b']\n\nfor i in range(3):\n    p = grupos[pos_correcta == i]\n    ax.scatter(p[:,0], p[:,1], c=colores[i],\n               label=\"Grupo {}\".format(i))\n\nax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n           s=100, color='black', label='Centros')\n\nax.set_title(\"Agrupamiento s/modelo\")\nax.legend()\n\nplt.show()",
            "title": "Machine Learning"
        },
        {
            "location": "/ML/#introduccion-al-machine-learning",
            "text": "Una de las ramas de estudio que cada vez esta ganando m\u00e1s popularidad dentro de las  ciencias de la computaci\u00f3n  es el  aprendizaje autom\u00e1tico  o  Machine Learning . Muchos de los servicios que utilizamos en nuestro d\u00eda a d\u00eda como google, gmail, netflix, spotify o amazon se valen de las herramientas que les brinda el  Machine Learning  para alcanzar un servicio cada vez m\u00e1s personalizado y lograr as\u00ed ventajas competitivas sobre sus rivales.",
            "title": "Introducci\u00f3n al Machine Learning"
        },
        {
            "location": "/ML/#que-es-machine-learning",
            "text": "Pero, \u00bfqu\u00e9 es exactamente  Machine Learning ?. El  Machine Learning  es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del  Machine Learning  es  generalizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada . El ejemplo m\u00e1s t\u00edpico donde podemos ver el uso del  Machine Learning  es en el filtrado de los correo basura o spam. Mediante la observaci\u00f3n de miles de correos electr\u00f3nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos. El  Machine Learning  combina conceptos y t\u00e9cnicas de diferentes \u00e1reas del conocimiento, como las  matem\u00e1ticas ,  estad\u00edsticas  y las  ciencias de la computaci\u00f3n ; por tal motivo, hay muchas maneras de aprender la disciplina.",
            "title": "\u00bfQu\u00e9 es Machine Learning?"
        },
        {
            "location": "/ML/#tipos-de-machine-learning",
            "text": "El  Machine Learning  tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito, an\u00e1lisis del mercado de valores, clasificaci\u00f3n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob\u00f3tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distingir los distintos tipos de problemas de  Machine Learning  con los que nos podemos encontrar.",
            "title": "Tipos de Machine Learning"
        },
        {
            "location": "/ML/#aprendizaje-supervisado",
            "text": "En los problemas de  aprendizaje supervisado  se ense\u00f1a o entrena al  algoritmo  a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos m\u00e1s el  algoritmo  puede aprender sobre el tema. Una vez conclu\u00eddo el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el  algoritmo  de aprendizaje utiliza la experiencia pasada que adquiri\u00f3 durante la etapa de entrenamiento para predecir un resultado. Esto es similar al m\u00e9todo de aprendizaje que se utiliza en las escuelas, donde se nos ense\u00f1an problemas y las formas de resolverlos, para que luego podamos aplicar los mismos m\u00e9todos en situaciones similares.",
            "title": "Aprendizaje supervisado"
        },
        {
            "location": "/ML/#aprendizaje-no-supervisado",
            "text": "En los problemas de  aprendizaje no supervisado  el  algoritmo  es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al  algoritmo  lo que representan los datos. La idea es que el  algoritmo  pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos. El  aprendizaje no supervisado  es similar al m\u00e9todo que utilizamos para aprender a hablar cuando somos bebes, en un principio escuchamos hablar a nuestros padres y no entendemos nada; pero a medida que vamos escuchando miles de conversaciones, nuestro cerebro comenzar\u00e1 a formar un modelo sobre c\u00f3mo funciona el lenguaje y comenzaremos a reconocer patrones y a esperar ciertos sonidos.",
            "title": "Aprendizaje no supervisado"
        },
        {
            "location": "/ML/#aprendizaje-por-refuerzo",
            "text": "En los problemas de aprendizaje por refuerzo, el  algoritmo  aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m\u00e1s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar\u00e1 creando una estrategia ganadora.",
            "title": "Aprendizaje por refuerzo"
        },
        {
            "location": "/ML/#sobreajuste",
            "text": "Como mencionamos cuando definimos al  Machine Learning , la idea fundamental es encontrar patrones que podamos generalizar para luego poder aplicar esta generalizaci\u00f3n sobre los casos que todav\u00eda no hemos observado y realizar predicciones. Pero tambi\u00e9n puede ocurrir que durante el entrenamiento solo descubramos casualidades en los datos que se parecen a patrones interesantes, pero que no generalicen. Esto es lo que se conoce con el nombre de  sobreajuste  o sobreentrenamiento .  El  sobreajuste  es la tendencia que tienen la mayor\u00eda de los  algoritmos  de  Machine Learning  a ajustarse a unas caracter\u00edsticas muy espec\u00edficas de los datos de entrenamiento que no tienen relaci\u00f3n causal con la  funci\u00f3n objetivo  que estamos buscando para generalizar. El ejemplo m\u00e1s extremo de un modelo  sobreajustado  es un modelo que solo memoriza las respuestas correctas; este modelo al ser utilizado con datos que nunca antes ha visto va a tener un rendimiento azaroso, ya que nunca logr\u00f3 generalizar un patr\u00f3n para predecir.",
            "title": "Sobreajuste"
        },
        {
            "location": "/ML/#como-evitar-el-sobreajuste",
            "text": "Como mencionamos anteriormente, todos los modelos de  Machine Learning  tienen tendencia al  sobreajuste ; es por esto que debemos aprender a convivir con el mismo y tratar de tomar medidas preventivas para reducirlo lo m\u00e1s posible. Las dos principales estrategias para lidiar son el  sobreajuste  son: la  retenci\u00f3n de datos  y la  validaci\u00f3n cruzada .  En el primer caso, la idea es dividir nuestro  conjunto de datos , en uno o varios conjuntos de entrenamiento y otro/s conjuntos de evaluaci\u00f3n. Es decir, que no le vamos a pasar todos nuestros datos al  algoritmo  durante el entrenamiento, sino que vamos a  retener  una parte de los datos de entrenamiento para realizar una evaluaci\u00f3n de la efectividad del modelo. Con esto lo que buscamos es evitar que los mismos datos que usamos para entrenar sean los mismos que utilizamos para evaluar. De esta forma vamos a poder analizar con m\u00e1s precisi\u00f3n como el modelo se va comportando a medida que m\u00e1s lo vamos entrenando y poder detectar el punto cr\u00edtico en el que el modelo deja de generalizar y comienza a  sobreajustarse  a los datos de entrenamiento.  La  validaci\u00f3n cruzada  es un procedimiento m\u00e1s sofisticado que el anterior. En lugar de solo obtener una simple estimaci\u00f3n de la efectividad de la  generalizaci\u00f3n ; la idea es realizar un an\u00e1lisis estad\u00edstico para obtener otras medidas del rendimiento estimado, como la media y la varianza, y as\u00ed poder entender c\u00f3mo se espera que el rendimiento var\u00ede a trav\u00e9s de los distintos conjuntos de datos. Esta variaci\u00f3n es fundamental para la evaluaci\u00f3n de la confianza en la estimaci\u00f3n del rendimiento.\nLa  validaci\u00f3n cruzada  tambi\u00e9n hace un mejor uso de un conjunto de datos limitado; ya que a diferencia de la simple divisi\u00f3n de los datos en uno el entrenamiento y otro de evaluaci\u00f3n; la  validaci\u00f3n cruzada  calcula sus estimaciones sobre todo el  conjunto de datos  mediante la realizaci\u00f3n de m\u00faltiples divisiones e intercambios sistem\u00e1ticos entre datos de entrenamiento y datos de evaluaci\u00f3n.",
            "title": "Como evitar el sobreajuste"
        },
        {
            "location": "/ML/#pasos-para-construir-un-modelo-de-machine-learning",
            "text": "Construir un modelo de  Machine Learning , no se reduce solo a utilizar un  algoritmo  de aprendizaje o utilizar una librer\u00eda de  Machine Learning ; sino que es todo un proceso que suele involucrar los siguientes pasos:    Recolectar los datos . Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una  API  o desde una base de datos. Podemos tambi\u00e9n utilizar otros dispositivos que recolectan los datos por nosotros; o utilizar datos que son de dominio p\u00fablico. El n\u00famero de opciones que tenemos para recolectar datos no tiene fin!. Este paso parece obvio, pero es uno de los que m\u00e1s complicaciones trae y m\u00e1s tiempo consume.     Preprocesar los datos . Una vez que tenemos los datos, tenemos que asegurarnos que tiene el formato correcto para nutrir nuestro  algoritmo  de aprendizaje. Es pr\u00e1cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m\u00e1s sencillo que el paso anterior.    Explorar los datos . Una vez que ya tenemos los datos y est\u00e1n con el formato correcto, podemos realizar un pre an\u00e1lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg\u00fan patr\u00f3n en los mismos que nos facilite la construcci\u00f3n del modelo. En esta etapa suelen ser de mucha utilidad las medidas estad\u00edsticas y los gr\u00e1ficos en 2 y 3 dimensiones para tener una idea visual de como se comportan nuestros datos. En este punto podemos detectar  valores at\u00edpicos  que debamos descartar; o encontrar las caracter\u00edsticas que m\u00e1s influencia tienen para realizar una predicci\u00f3n.    Entrenar el  algoritmo . Aqu\u00ed es donde comenzamos a utilizar las t\u00e9cnicas de  Machine Learning  realmente. En esta etapa nutrimos al o los  algoritmos  de aprendizaje con los datos que venimos procesando en las etapas anteriores. La idea es que los  algoritmos  puedan extraer informaci\u00f3n \u00fatil de los datos que le pasamos para luego poder hacer predicciones.     Evaluar el  algoritmo . En esta etapa ponemos a prueba la informaci\u00f3n o conocimiento que el  algoritmo  obtuvo del entrenamiento del paso anterior. Evaluamos que tan preciso es el algoritmo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el  algoritmo  cambiando algunos par\u00e1metros hasta lograr un rendimiento aceptable.      Utilizar el modelo . En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu\u00ed tambi\u00e9n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores.",
            "title": "Pasos para construir un modelo de machine learning"
        },
        {
            "location": "/ML/#librerias-de-python-para-machine-learning",
            "text": "Como siempre me gusta comentar, una de las grandes ventajas que ofrece  Python  sobre otros lenguajes de programaci\u00f3n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer\u00edas de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de  Machine Learning , las principales librer\u00edas que podemos utilizar son:",
            "title": "Librer\u00edas de Python para machine learning"
        },
        {
            "location": "/ML/#scikit-learn",
            "text": "Scikit-learn  es la principal librer\u00eda que existe para trabajar con  Machine Learning , incluye la implementaci\u00f3n de un gran n\u00famero de  algoritmos  de aprendizaje. La podemos utilizar para  clasificaciones ,  extraccion de caracter\u00edsticas ,  regresiones ,  agrupaciones ,  reducci\u00f3n de dimensiones ,  selecci\u00f3n de modelos , o  preprocesamiento . Posee una  API  que es consistente en todos los modelos y se integra muy bien con el resto de los paquetes cient\u00edficos que ofrece  Python . Esta librer\u00eda tambi\u00e9n nos facilita las tareas de evaluaci\u00f3n, diagnostico y  validaciones cruzadas  ya que nos proporciona varios m\u00e9todos de f\u00e1brica para poder realizar estas tareas en forma muy simple.",
            "title": "Scikit-Learn"
        },
        {
            "location": "/ML/#statsmodels",
            "text": "Statsmodels  es otra gran librer\u00eda que hace foco en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios. Al igual que  Scikit-learn , tambi\u00e9n se integra muy bien con el resto de los paquetes cientificos de  Python . Si deseamos ajustar modelos lineales, hacer una an\u00e1lisis estad\u00edstico, o tal vez un poco de modelado predictivo, entonces  Statsmodels  es la librer\u00eda ideal. Las pruebas estad\u00edsticas que ofrece son bastante amplias y abarcan tareas de validaci\u00f3n para la mayor\u00eda de los casos.",
            "title": "Statsmodels"
        },
        {
            "location": "/ML/#pymc",
            "text": "pyMC  es un m\u00f3dulo de  Python  que implementa modelos estad\u00edsticos bayesianos, incluyendo la  cadena de Markov Monte Carlo(MCMC) .  pyMC   ofrece funcionalidades para hacer el an\u00e1lisis bayesiano lo mas simple posible. Incluye los modelos  bayesianos ,  distribuciones estad\u00edsticas y herramientas de diagnostico  para la covarianza de los modelos. Si queremos realizar un an\u00e1lisis  bayesiano  esta es sin duda la librer\u00eda a utilizar.",
            "title": "PyMC"
        },
        {
            "location": "/ML/#ntlk",
            "text": "NLTK  es la librer\u00eda l\u00edder para el procesamiento del lenguaje natural o  NLP  por sus siglas en ingl\u00e9s. Proporciona interfaces f\u00e1ciles de usar a m\u00e1s de 50 cuerpos y recursos l\u00e9xicos, como  WordNet , junto con un conjunto de bibliotecas de procesamiento de texto para la clasificaci\u00f3n, tokenizaci\u00f3n, el etiquetado, el an\u00e1lisis y el razonamiento sem\u00e1ntico.   Obviamente, aqu\u00ed solo estoy listando unas pocas de las muchas librer\u00edas que existen en  Python  para trabajar con problemas de  Machine Learning , los invito a realizar su propia investigaci\u00f3n sobre el tema.",
            "title": "NTLK"
        },
        {
            "location": "/ML/#algoritmos-mas-utilizados",
            "text": "Los  algoritmos   que m\u00e1s se suelen utilizar en los problemas de  Machine Learning  son los siguientes:   Regresi\u00f3n Lineal  Regresi\u00f3n Log\u00edstica  Arboles de Decision  Random Forest  SVM  o M\u00e1quinas de vectores de soporte.  KNN  o K vecinos m\u00e1s cercanos.  K-means   Todos ellos se pueden aplicar a casi cualquier problema de datos y obviamente estan todos implementados por la excelente librer\u00eda de  Python ,  Scikit-learn . Veamos algunos ejemplos de ellos.",
            "title": "Algoritmos m\u00e1s utilizados"
        },
        {
            "location": "/ML/#regresion-lineal",
            "text": "Se utiliza para estimar los valores reales (costo de las viviendas, el n\u00famero de llamadas, ventas totales, etc.) basados en variables continuas. La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar una mejor l\u00ednea recta con respecto a los puntos. Esta l\u00ednea de mejor ajuste se conoce como l\u00ednea de regresi\u00f3n y esta representada por la siguiente ecuaci\u00f3n lineal:  $$Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{n}X_{n}$$  Veamos un peque\u00f1o ejemplo de como se implementa en  Python . En este ejemplo voy a utilizar el dataset Boston que ya viene junto con  Scikit-learn  y es ideal para practicar con  Regresiones Lineales ; el mismo contiene precios de casas de varias \u00e1reas de la ciudad de Boston.   # importando pandas, numpy y matplotlib\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# importando los datasets de sklearn\nfrom sklearn import datasets\n\nboston = datasets.load_boston()\nboston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\nboston_df['TARGET'] = boston.target\nboston_df.head() # estructura de nuestro dataset.  \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       TARGET \n     \n   \n   \n     \n       0 \n       0.00632 \n       18.0 \n       2.31 \n       0.0 \n       0.538 \n       6.575 \n       65.2 \n       4.0900 \n       1.0 \n       296.0 \n       15.3 \n       396.90 \n       4.98 \n       24.0 \n     \n     \n       1 \n       0.02731 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       6.421 \n       78.9 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       396.90 \n       9.14 \n       21.6 \n     \n     \n       2 \n       0.02729 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       7.185 \n       61.1 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       392.83 \n       4.03 \n       34.7 \n     \n     \n       3 \n       0.03237 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       6.998 \n       45.8 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       394.63 \n       2.94 \n       33.4 \n     \n     \n       4 \n       0.06905 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       7.147 \n       54.2 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       396.90 \n       5.33 \n       36.2 \n     \n     # importando el modelo de regresi\u00f3n lineal\nfrom sklearn.linear_model import LinearRegression\n\nrl = LinearRegression() # Creando el modelo.\nrl.fit(boston.data, boston.target) # ajustando el modelo\n\n# haciendo las predicciones\npredicciones = rl.predict(boston.data)\npredicciones_df = pd.DataFrame(predicciones, columns=['Pred'])\npredicciones_df.head() # predicciones de las primeras 5 lineas  \n   \n     \n       \n       Pred \n     \n   \n   \n     \n       0 \n       30.008213 \n     \n     \n       1 \n       25.029861 \n     \n     \n       2 \n       30.570232 \n     \n     \n       3 \n       28.608141 \n     \n     \n       4 \n       27.942882 \n     \n     # Calculando el desvio\nnp.mean(boston.target - predicciones)  Como podemos ver, el desv\u00edo del modelo es peque\u00f1o, por lo que sus resultados para este ejemplo son bastante confiables.",
            "title": "Regresi\u00f3n Lineal"
        },
        {
            "location": "/ML/#regresion-logistica",
            "text": "Los modelos lineales, tambi\u00e9n pueden ser utilizados para clasificaciones; es decir, que primero ajustamos el modelo lineal a la probabilidad de que una cierta clase o categor\u00eda ocurra y, a luego, utilizamos una funci\u00f3n para crear un umbral en el cual especificamos el resultado de una de estas clases o categor\u00edas. La funci\u00f3n que utiliza este modelo, no es ni m\u00e1s ni menos que la funci\u00f3n log\u00edstica.  $$f(x) = \\frac{1}{1 + e^{-1}}$$  Veamos, aqu\u00ed tambi\u00e9n un peque\u00f1o ejemplo en  Python .  # Creando un dataset de ejemplo \nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1000, n_features=4)\n\n# Importando el modelo\nfrom sklearn.linear_model import LogisticRegression\n\nrlog = LogisticRegression() # Creando el modelo\n\n# Dividiendo el dataset en entrenamiento y evaluacion\nX_entrenamiento = X[:-200]\nX_evaluacion = X[-200:]\ny_entrenamiento = y[:-200]\ny_evaluacion = y[-200:]\n\nrlog.fit(X_entrenamiento, y_entrenamiento) #ajustando el modelo\n\n# Realizando las predicciones\ny_predic_entrenamiento = rlog.predict(X_entrenamiento) \ny_predic_evaluacion = rlog.predict(X_evaluacion)\n\n# Verificando la exactitud del modelo\nentrenamiento = (y_predic_entrenamiento == y_entrenamiento).sum().astype(float) / y_entrenamiento.shape[0]\nprint(\"sobre datos de entrenamiento: {0:.2f}\".format(entrenamiento))\nevaluacion = (y_predic_evaluacion == y_evaluacion).sum().astype(float) / y_evaluacion.shape[0]\nprint(\"sobre datos de evaluaci\u00f3n: {0:.2f}\".format(evaluacion))\n\nsobre datos de entrenamiento: 0.95\nsobre datos de evaluaci\u00f3n: 0.94  Como podemos ver en este ejemplo tambi\u00e9n nuestro modelo tiene bastante precisi\u00f3n clasificando las categor\u00edas de nuestro dataset.",
            "title": "Regresi\u00f3n Log\u00edstica"
        },
        {
            "location": "/ML/#arboles-de-decision",
            "text": "Los  Arboles de Decision  son diagramas con construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema.\nLos  Arboles de Decision  est\u00e1n compuestos por nodos interiores, nodos terminales y ramas que emanan de los nodos interiores. Cada nodo interior en el \u00e1rbol contiene una prueba de un atributo, y cada rama representa un valor distinto del atributo. Siguiendo las ramas desde el nodo ra\u00edz hacia abajo, cada ruta finalmente termina en un nodo terminal creando una segmentaci\u00f3n de los datos. Veamos aqu\u00ed tambi\u00e9n un peque\u00f1o ejemplo en  Python .  # Creando un dataset de ejemplo\nX, y = datasets.make_classification(1000, 20, n_informative=3)\n\n# Importando el arbol de decisi\u00f3n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nad = DecisionTreeClassifier(criterion='entropy', max_depth=5) # Creando el modelo\nad.fit(X, y) # Ajustando el modelo\n\n#generando archivo para graficar el arbol\nwith open(\"mi_arbol.dot\", 'w') as archivo_dot:\n    tree.export_graphviz(ad, out_file = archivo_dot)\n\n# utilizando el lenguaje dot para graficar el arbol.\n!dot -Tjpeg mi_arbol.dot -o arbol_decision.jpeg  Luego de usar el lenguaje  dot  para convertir nuestro arbol a formato jpeg, ya podemos ver la imagen del mismo.   # verificando la precisi\u00f3n\nprint(\"precisi\u00f3n del modelo: {0: .2f}\".format((y == ad.predict(X)).mean()))\n\nprecisi\u00f3n del modelo:  0.94  En este ejemplo, nuestro \u00e1rbol tiene una precisi\u00f3n del 89%. Tener en cuenta que los  Arboles de Decision  tienen tendencia al  sobreajuste .",
            "title": "Arboles de decisi\u00f3n"
        },
        {
            "location": "/ML/#random-forest",
            "text": "En lugar de utilizar solo un arbol para decidir, \u00bfpor qu\u00e9 no utilizar todo un bosque?!!. Esta es la idea central detr\u00e1s del  algoritmo  de  Random Forest . Tarbaja construyendo una gran cantidad de  arboles de decision  muy poco profundos, y luego toma la clase que\ncada \u00e1rbol eligi\u00f3. Esta idea es muy poderosa en  Machine Learning . Si tenemos en cuenta que un sencillo clasificador entrenado podr\u00eda tener s\u00f3lo el 60 por ciento de precisi\u00f3n, podemos entrenar un mont\u00f3n de clasificadores que sean por lo general acertados y luego podemos utilizar la sabidur\u00eda de todos los aprendices juntos.\nCon  Python  los podemos utilizar de la siguiente manera:  # Creando un dataset de ejemplo\nX, y = datasets.make_classification(1000)\n\n# Importando el random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier() # Creando el modelo\nrf.fit(X, y) # Ajustando el modelo\n\n# verificando la precisi\u00f3n\nprint(\"precisi\u00f3n del modelo: {0: .2f}\".format((y == rf.predict(X)).mean()))\n\nprecisi\u00f3n del modelo:  1.00",
            "title": "Random Forest"
        },
        {
            "location": "/ML/#svm-o-maquinas-de-vectores-de-soporte",
            "text": "La idea detr\u00e1s de  SVM  es encontrar un plano que separe los grupos dentro de los datos de la mejor forma posible. Aqu\u00ed, la separaci\u00f3n significa que la elecci\u00f3n\ndel plano maximiza el margen entre los puntos m\u00e1s cercanos en el plano; \u00e9stos puntos se denominan vectores de soporte. Pasemos al ejemplo.  # importanto SVM\nfrom sklearn import svm\n\n# importando el dataset iris\niris = datasets.load_iris()\nX = iris.data[:, :2]  # solo tomamos las primeras 2 caracter\u00edsticas\ny = iris.target\n\nh = .02  # tama\u00f1o de la malla del grafico\n\n# Creando el SVM con sus diferentes m\u00e9todos\nC = 1.0  # parametro de regulacion SVM \nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\nlin_svc = svm.LinearSVC(C=C).fit(X, y)\n\n# crear el area para graficar\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# titulos de los graficos\ntitles = ['SVC con el motor lineal',\n          'LinearSVC',\n          'SVC con el motor RBF',\n          'SVC con el motor polinomial']\n\n\nfor i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n    # Realizando el gr\u00e1fico, se le asigna un color a cada punto\n    plt.subplot(2, 2, i + 1)\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n    # Graficando tambien los puntos de datos\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('largo del petalo')\n    plt.ylabel('ancho del petalo')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(titles[i])\n\nplt.show()",
            "title": "SVM o M\u00e1quinas de vectores de soporte"
        },
        {
            "location": "/ML/#knn-o-k-vecinos-mas-cercanos",
            "text": "Este es un m\u00e9todo de clasificaci\u00f3n no param\u00e9trico, que estima el valor de la probabilidad a posteriori de que un elemento $x$ pertenezca a una clase en particular a partir de la informaci\u00f3n proporcionada por el conjunto de prototipos.\nLa regresi\u00f3n  KNN  se calcula simplemente tomando el promedio del punto k m\u00e1s cercano al punto que se est\u00e1 probando.   \n# Creando el dataset iris\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# importando KNN \nfrom sklearn.neighbors import KNeighborsRegressor\n\nknnr = KNeighborsRegressor(n_neighbors=10) # Creando el modelo con 10 vecinos\nknnr.fit(X, y) # Ajustando el modelo\n\n# Verificando el error medio del modelo\nprint(\"El error medio del modelo es: {:.2f}\".format(np.power(y - knnr.predict(X),\n2).mean()))\n\nEl error medio del modelo es: 0.02",
            "title": "KNN o k vecinos m\u00e1s cercanos"
        },
        {
            "location": "/ML/#k-means",
            "text": "K-means  es probablemente uno de los algoritmos de agrupamiento m\u00e1s conocidos y, en un sentido m\u00e1s amplio, una de las t\u00e9cnicas de aprendizaje no supervisado m\u00e1s conocidas. K-means  es en realidad un  algoritmo  muy simple que funciona para reducir al m\u00ednimo la suma de las distancias cuadradas desde la media dentro del agrupamiento. Para hacer esto establece primero un n\u00famero previamente especificado de conglomerados, K, y luego va asignando cada observaci\u00f3n a la agrupaci\u00f3n m\u00e1s cercana de acuerdo a su media. Veamos el ejemplo  # Creando el dataset\ngrupos, pos_correcta = datasets.make_blobs(1000, centers=3,\ncluster_std=1.75)\n\n# Graficando los grupos de datos\nf, ax = plt.subplots(figsize=(7, 5))\ncolores = ['r', 'g', 'b']\n\nfor i in range(3):\n    p = grupos[pos_correcta == i]\n    ax.scatter(p[:,0], p[:,1], c=colores[i],\n               label=\"Grupo {}\".format(i))\n\nax.set_title(\"Agrupamiento perfecto\")\nax.legend()\n\nplt.show()   # importando KMeans\nfrom sklearn.cluster import KMeans\n\n# Creando el modelo\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(grupos) # Ajustando el modelo\n\n# verificando los centros de los grupos\nkmeans.cluster_centers_\n\n# Graficando segun modelo\nf, ax = plt.subplots(figsize=(7, 5))\ncolores = ['r', 'g', 'b']\n\nfor i in range(3):\n    p = grupos[pos_correcta == i]\n    ax.scatter(p[:,0], p[:,1], c=colores[i],\n               label=\"Grupo {}\".format(i))\n\nax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n           s=100, color='black', label='Centros')\n\nax.set_title(\"Agrupamiento s/modelo\")\nax.legend()\n\nplt.show()",
            "title": "K-means"
        },
        {
            "location": "/datascience/",
            "text": "Ciencia de datos\n\n\n\n\nEn el mundo actual de la tecnolog\u00eda que nos rodea, donde la \ncomputaci\u00f3n en la nube\n se va haciendo parte de nuestro d\u00eda a d\u00eda (quien no usa los servicios de \nGoogle\n, \nFacebook\n, \nTwitter\n, \nDropbox\n, o \nEvernote\n); d\u00f3nde hay cada vez una mayor cantidad de dispositivos que estan las 24 horas del d\u00eda conectadas a internet (desde tel\u00e9fonos, tabletas y TVs hasta automoviles), acercandonos a\u00fan m\u00e1s al concepto de la \nInternet de las cosas\n. En este mundo d\u00f3nde estamos generando datos constantemente, en el mundo de la \nBig Data\n; se esta haciendo cada vez m\u00e1s necesario un nuevo perfil de profesionales de la informaci\u00f3n que puedan aplicar las t\u00e9cnicas de la \nCiencia de Datos\n.\n\n\n\u00bfQu\u00e9 es la Ciencia de Datos?\n\n\nLa \nCiencia de Datos\n es un campo interdisciplinario que involucra m\u00e9todos cient\u00edficos, procesos y sistemas para extraer conocimiento o un mejor entendimiento de datos en sus diferentes formas, ya sea estructurados o no estructurados. Es una continuaci\u00f3n de algunos campos de \nan\u00e1lisis de datos\n como la \nestad\u00edstica\n, la \nminer\u00eda de datos\n, el \naprendizaje autom\u00e1tico\n y el \nan\u00e1lisis predictivo\n. Comprende tres \u00e1reas distintas y superpuestas: las habilidades de un estad\u00edstico que sabe c\u00f3mo modelar y resumir conjuntos de datos (los cuales cada vez tienen mayor tama\u00f1o); las habilidades de un inform\u00e1tico que pueda dise\u00f1ar y utilizar algoritmos para almacenar, procesar y visualizar eficientemente estos datos; Y la experiencia sobre el \ncampo\n o \ndominio\n, lo que podr\u00edamos pensar como una formaci\u00f3n \ncl\u00e1sica\n en un tema; la cual es necesaria tanto para formular las preguntas correctas como para poner sus respuestas en contexto. Actualmente, a los profesionales que se dedican a esta disciplina, se los conoce como \n*Cient\u00edficos de datos\n*\n\n\nCient\u00edfico de datos\n\n\nLos \nData Scientists\n o \nCient\u00edficos de datos\n  son profesionales, generalmente con conocimientos multidisiplinarios, que poseen el entrenamiento y la curiosidad necesarias para realizar descubrimientos en el intrincado mundo de la \nBig Data\n. Ellos son capaces de darle forma a la enorme cantidad de datos desestructurados que generamos d\u00eda a d\u00eda y hacer su analisis posible. Se encargan de identificar potenciales fuentes de informaci\u00f3n, unirlas y depurar el conjunto de resultados; los \nCient\u00edficos de datos\n ayudan a los encargados de tomar las decisiones a moverse de un analisis \nad hoc\n de los datos hacia una constante conversaci\u00f3n con ellos.\n\n\nLos \nCient\u00edficos de datos\n se encargan de encontrar patrones en los datos, hacer descubrimientos en base a ellos, y comunicar las implicaciones de lo que han aprendido a trav\u00e9s de su analisis, para indicar nuevas oportunidades de negocios. Ellos aconsejan a los ejecutivos y gerentes de productos sobre las implicaciones de los datos para los productos, procesos y decisiones.\n\n\nSi bien, una primera impresi\u00f3n, se imaginar\u00eda a los \nCient\u00edficos de datos\n como personas con un fuerte perfil analitico y mucho conocimiento estad\u00edstico y matem\u00e1tico, esta impresi\u00f3n estar\u00eda por dem\u00e1s errada. Ellos se caracterizan m\u00e1s por su parte cient\u00edfica; una de las facetas dominantes de su personalidad es su intensa curiosidad, el deseo por ir m\u00e1s all\u00e1 de la superficie de los problemas, encontrar las preguntas en lo m\u00e1s profundo de ellos, e ir depurandolas hasta crear un claro conjunto de hipotesis que puedan ser probadas con datos concretos. Es por esto, que algunos de los m\u00e1s renombrados \nCient\u00edficos de datos\n en las principales empresas de tecnolog\u00eda del mundo, vienen de campos poco convencionales como la \nF\u00edsica\n y las \nCiencias Sociales\n.\n\n\nLo que motiva a los \nCient\u00edficos de datos\n no es armar hermosos reportes con informaci\u00f3n estructurada, para eso ya existen los analistas financieros; lo que realmente motiva a los \nCient\u00edficos de datos\n es crear nuevas cosas, no solo dar consejo; ellos quieren crear soluciones que funcionen y generen un impacto innovador para el negocio y los consumidores.\n\n\nUna podr\u00eda pensar a los \nCient\u00edficos de datos\n como un h\u00edbrido entre hacker, analista, comunicador y consejero; personas que tengan el conocimiento t\u00e9cnico necesario para manejar y analizar grandes cantides de datos, pero que a su vez tengan la suficiente noci\u00f3n y entendimiento de los negocios y la habilidad para comunicar los datos de una forma efectiva. Una combinaci\u00f3n realmente rara de darse, pero sumamente efectiva!.\n\n\nEn lo que hace al apartado t\u00e9cnico, una de las habilidades b\u00e1sicas que todo buen \nCient\u00edficos de datos\n deber\u00eda tener, es sin duda la habilidad de escribir c\u00f3digo, programar. Un buen \nCient\u00edficos de datos\n deber\u00eda ser eficiente con al menos un lenguaje de programaci\u00f3n de alto rendimiento (como \nC\n, \nC++\n o \nJava\n) y tener nociones sobre los principales lenguajes que se manejan en internet (\nHTML\n, \nCSS3\n, \nJavascript\n, \nPHP\n).\n\n\nTambi\u00e9n deber\u00eda poseer buenos conocimientos sobre \nprobabilidad y estad\u00edstica\n, aqu\u00ed lenguajes de programaci\u00f3n con \nR\n y \nPython\n, pueden resultar realmente \u00fatiles.\n\n\nY finalmente, deber\u00eda poseer conocimientos sobre los principales  frameworks para el manejo de la \nBig Data\n, como por ejemplo \nHadoop\n; conocimientos sobre la infraestructura de la  \ncomputaci\u00f3n en la nube\n; y sobre las principales \nbases de datos\n, tanto \nSQL\n como \nNoSQL\n.\n\n\nLos siguientes son ejemplos del trabajo realizado por los \nCient\u00edficos de datos\n:\n\n\n\n\nEvaluaci\u00f3n de modelos estad\u00edsticos para determinar la validez de los an\u00e1lisis.\n\n\nUtilizar el \naprendizaje autom\u00e1tico\n para construir mejores algoritmos predictivos.\n\n\nPruebas y mejora continua de la precisi\u00f3n de los modelos de \naprendizaje autom\u00e1tico\n.\n\n\nConstruir visualizaciones de datos para resumir la conclusi\u00f3n de un an\u00e1lisis avanzado.\n\n\n\n\nLos \nCient\u00edficos de datos\n aportan un enfoque y una perspectiva totalmente nuevos a la comprensi\u00f3n de los datos.\n\n\nOtros roles relacionados con datos\n\n\nAdem\u00e1s del rol de cient\u00edfico de datos existen otros roles relacionados con el manejo de datos, los cuales muchas veces se confunden pero no son ex\u00e1ctamente lo mismo. Estos roles son:\n\n\nAnalista de datos\n\n\nLos \nAnalistas de datos\n aportan valor a sus empresas mediante la obtenci\u00f3n de datos, su utilizaci\u00f3n para responder preguntas y la comunicaci\u00f3n de los resultados para ayudar a tomar decisiones. Las tareas m\u00e1s comunes realizadas por los analistas de datos incluyen la limpieza de datos, la realizaci\u00f3n de an\u00e1lisis y la creaci\u00f3n de visualizaciones. Dependiendo de la industria, el \nAnalista de datos\n puede terner varios t\u00edtulos diferentes (por ejemplo, analista de negocios, analista de inteligencia de negocios, analista de operaciones, analista de bases de datos). Independientemente del t\u00edtulo, el \nAnalista de datos\n es un generalista que puede encajar en muchos roles y equipos para ayudar a otros a tomar mejores decisiones basadas en datos.\n\n\nLa naturaleza de las habilidades requeridas depender\u00e1 de las necesidades espec\u00edficas de la empresa, pero estas son algunas de ellas:\n\n\n\n\nLimpieza y organizaci\u00f3n de datos en bruto.\n\n\nUso de estad\u00edsticas descriptivas para obtener una vista panor\u00e1mica de sus datos.\n\n\nAn\u00e1lisis de tendencias interesantes encontradas en los datos.\n\n\nCreaci\u00f3n de visualizaciones y cuadros de mando para ayudar a la empresa a interpretar y tomar decisiones con los datos.\n\n\nPresentasi\u00f3n de los resultados de un an\u00e1lisis t\u00e9cnico a clientes empresariales o equipos internos.\n\n\n\n\nEl \nAnalista de datos\n aporta un valor significativo tanto a los aspectos t\u00e9cnicos como no t\u00e9cnicos de una organizaci\u00f3n.\n\n\nIngeniero de datos\n\n\nLos \nIngenieros de datos\n construyen y optimizan los sistemas que permiten a los cient\u00edficos y analistas de datos realizar su trabajo. Cada empresa depende de los datos sean exactos y accesibles, para que las personas puedan trabajar con ellos.El \nIngeniero de datos\n se asegura de que cualquier dato sea recibido, transformado, almacenado y hecho accesible para otros usuarios.\n\n\nLos \nIngenieros de datos\n son responsables de construir las herramientas para trabajar con datos y, a menudo, tienen que usar t\u00e9cnicas complejas para manejar los datos a escala. A diferencia de los cient\u00edficos y analistas de datos, la ingenier\u00eda de datos se inclina mucho m\u00e1s hacia un conjunto de habilidades de desarrollo de software.\n\n\nUn buen \nIngeniero de datos\n debe permitir que los cient\u00edficos o analistas de datos puedan concentrarse en resolver problemas, en lugar de tener que preocuparse por aspectos m\u00e1s t\u00e9cnicos de la disciplina, como por ejemplo mover los datos de una fuente a otra.\n\n\nLa mentalidad del \nIngeniero de datos\n suele estar m\u00e1s centrada en la construcci\u00f3n y la optimizaci\u00f3n. Los siguientes son ejemplos de tareas en las que un ingeniero de datos podr\u00eda estar trabajando:\n\n\n\n\nCreaci\u00f3n de APIs para el consumo de datos.\n\n\nIntegraci\u00f3n de conjuntos de datos externos o nuevos en los procesos de datos existentes.\n\n\nAplicaci\u00f3n de transformaciones de atributos para los modelos de \naprendizaje autom\u00e1tico\n.\n\n\nSupervisar y probar continuamente los sistemas para asegurar un rendimiento optimizado.",
            "title": "Ciencia de datos"
        },
        {
            "location": "/datascience/#ciencia-de-datos",
            "text": "En el mundo actual de la tecnolog\u00eda que nos rodea, donde la  computaci\u00f3n en la nube  se va haciendo parte de nuestro d\u00eda a d\u00eda (quien no usa los servicios de  Google ,  Facebook ,  Twitter ,  Dropbox , o  Evernote ); d\u00f3nde hay cada vez una mayor cantidad de dispositivos que estan las 24 horas del d\u00eda conectadas a internet (desde tel\u00e9fonos, tabletas y TVs hasta automoviles), acercandonos a\u00fan m\u00e1s al concepto de la  Internet de las cosas . En este mundo d\u00f3nde estamos generando datos constantemente, en el mundo de la  Big Data ; se esta haciendo cada vez m\u00e1s necesario un nuevo perfil de profesionales de la informaci\u00f3n que puedan aplicar las t\u00e9cnicas de la  Ciencia de Datos .",
            "title": "Ciencia de datos"
        },
        {
            "location": "/datascience/#que-es-la-ciencia-de-datos",
            "text": "La  Ciencia de Datos  es un campo interdisciplinario que involucra m\u00e9todos cient\u00edficos, procesos y sistemas para extraer conocimiento o un mejor entendimiento de datos en sus diferentes formas, ya sea estructurados o no estructurados. Es una continuaci\u00f3n de algunos campos de  an\u00e1lisis de datos  como la  estad\u00edstica , la  miner\u00eda de datos , el  aprendizaje autom\u00e1tico  y el  an\u00e1lisis predictivo . Comprende tres \u00e1reas distintas y superpuestas: las habilidades de un estad\u00edstico que sabe c\u00f3mo modelar y resumir conjuntos de datos (los cuales cada vez tienen mayor tama\u00f1o); las habilidades de un inform\u00e1tico que pueda dise\u00f1ar y utilizar algoritmos para almacenar, procesar y visualizar eficientemente estos datos; Y la experiencia sobre el  campo  o  dominio , lo que podr\u00edamos pensar como una formaci\u00f3n  cl\u00e1sica  en un tema; la cual es necesaria tanto para formular las preguntas correctas como para poner sus respuestas en contexto. Actualmente, a los profesionales que se dedican a esta disciplina, se los conoce como  *Cient\u00edficos de datos *",
            "title": "\u00bfQu\u00e9 es la Ciencia de Datos?"
        },
        {
            "location": "/datascience/#cientifico-de-datos",
            "text": "Los  Data Scientists  o  Cient\u00edficos de datos   son profesionales, generalmente con conocimientos multidisiplinarios, que poseen el entrenamiento y la curiosidad necesarias para realizar descubrimientos en el intrincado mundo de la  Big Data . Ellos son capaces de darle forma a la enorme cantidad de datos desestructurados que generamos d\u00eda a d\u00eda y hacer su analisis posible. Se encargan de identificar potenciales fuentes de informaci\u00f3n, unirlas y depurar el conjunto de resultados; los  Cient\u00edficos de datos  ayudan a los encargados de tomar las decisiones a moverse de un analisis  ad hoc  de los datos hacia una constante conversaci\u00f3n con ellos.  Los  Cient\u00edficos de datos  se encargan de encontrar patrones en los datos, hacer descubrimientos en base a ellos, y comunicar las implicaciones de lo que han aprendido a trav\u00e9s de su analisis, para indicar nuevas oportunidades de negocios. Ellos aconsejan a los ejecutivos y gerentes de productos sobre las implicaciones de los datos para los productos, procesos y decisiones.  Si bien, una primera impresi\u00f3n, se imaginar\u00eda a los  Cient\u00edficos de datos  como personas con un fuerte perfil analitico y mucho conocimiento estad\u00edstico y matem\u00e1tico, esta impresi\u00f3n estar\u00eda por dem\u00e1s errada. Ellos se caracterizan m\u00e1s por su parte cient\u00edfica; una de las facetas dominantes de su personalidad es su intensa curiosidad, el deseo por ir m\u00e1s all\u00e1 de la superficie de los problemas, encontrar las preguntas en lo m\u00e1s profundo de ellos, e ir depurandolas hasta crear un claro conjunto de hipotesis que puedan ser probadas con datos concretos. Es por esto, que algunos de los m\u00e1s renombrados  Cient\u00edficos de datos  en las principales empresas de tecnolog\u00eda del mundo, vienen de campos poco convencionales como la  F\u00edsica  y las  Ciencias Sociales .  Lo que motiva a los  Cient\u00edficos de datos  no es armar hermosos reportes con informaci\u00f3n estructurada, para eso ya existen los analistas financieros; lo que realmente motiva a los  Cient\u00edficos de datos  es crear nuevas cosas, no solo dar consejo; ellos quieren crear soluciones que funcionen y generen un impacto innovador para el negocio y los consumidores.  Una podr\u00eda pensar a los  Cient\u00edficos de datos  como un h\u00edbrido entre hacker, analista, comunicador y consejero; personas que tengan el conocimiento t\u00e9cnico necesario para manejar y analizar grandes cantides de datos, pero que a su vez tengan la suficiente noci\u00f3n y entendimiento de los negocios y la habilidad para comunicar los datos de una forma efectiva. Una combinaci\u00f3n realmente rara de darse, pero sumamente efectiva!.  En lo que hace al apartado t\u00e9cnico, una de las habilidades b\u00e1sicas que todo buen  Cient\u00edficos de datos  deber\u00eda tener, es sin duda la habilidad de escribir c\u00f3digo, programar. Un buen  Cient\u00edficos de datos  deber\u00eda ser eficiente con al menos un lenguaje de programaci\u00f3n de alto rendimiento (como  C ,  C++  o  Java ) y tener nociones sobre los principales lenguajes que se manejan en internet ( HTML ,  CSS3 ,  Javascript ,  PHP ).  Tambi\u00e9n deber\u00eda poseer buenos conocimientos sobre  probabilidad y estad\u00edstica , aqu\u00ed lenguajes de programaci\u00f3n con  R  y  Python , pueden resultar realmente \u00fatiles.  Y finalmente, deber\u00eda poseer conocimientos sobre los principales  frameworks para el manejo de la  Big Data , como por ejemplo  Hadoop ; conocimientos sobre la infraestructura de la   computaci\u00f3n en la nube ; y sobre las principales  bases de datos , tanto  SQL  como  NoSQL .  Los siguientes son ejemplos del trabajo realizado por los  Cient\u00edficos de datos :   Evaluaci\u00f3n de modelos estad\u00edsticos para determinar la validez de los an\u00e1lisis.  Utilizar el  aprendizaje autom\u00e1tico  para construir mejores algoritmos predictivos.  Pruebas y mejora continua de la precisi\u00f3n de los modelos de  aprendizaje autom\u00e1tico .  Construir visualizaciones de datos para resumir la conclusi\u00f3n de un an\u00e1lisis avanzado.   Los  Cient\u00edficos de datos  aportan un enfoque y una perspectiva totalmente nuevos a la comprensi\u00f3n de los datos.",
            "title": "Cient\u00edfico de datos"
        },
        {
            "location": "/datascience/#otros-roles-relacionados-con-datos",
            "text": "Adem\u00e1s del rol de cient\u00edfico de datos existen otros roles relacionados con el manejo de datos, los cuales muchas veces se confunden pero no son ex\u00e1ctamente lo mismo. Estos roles son:",
            "title": "Otros roles relacionados con datos"
        },
        {
            "location": "/datascience/#analista-de-datos",
            "text": "Los  Analistas de datos  aportan valor a sus empresas mediante la obtenci\u00f3n de datos, su utilizaci\u00f3n para responder preguntas y la comunicaci\u00f3n de los resultados para ayudar a tomar decisiones. Las tareas m\u00e1s comunes realizadas por los analistas de datos incluyen la limpieza de datos, la realizaci\u00f3n de an\u00e1lisis y la creaci\u00f3n de visualizaciones. Dependiendo de la industria, el  Analista de datos  puede terner varios t\u00edtulos diferentes (por ejemplo, analista de negocios, analista de inteligencia de negocios, analista de operaciones, analista de bases de datos). Independientemente del t\u00edtulo, el  Analista de datos  es un generalista que puede encajar en muchos roles y equipos para ayudar a otros a tomar mejores decisiones basadas en datos.  La naturaleza de las habilidades requeridas depender\u00e1 de las necesidades espec\u00edficas de la empresa, pero estas son algunas de ellas:   Limpieza y organizaci\u00f3n de datos en bruto.  Uso de estad\u00edsticas descriptivas para obtener una vista panor\u00e1mica de sus datos.  An\u00e1lisis de tendencias interesantes encontradas en los datos.  Creaci\u00f3n de visualizaciones y cuadros de mando para ayudar a la empresa a interpretar y tomar decisiones con los datos.  Presentasi\u00f3n de los resultados de un an\u00e1lisis t\u00e9cnico a clientes empresariales o equipos internos.   El  Analista de datos  aporta un valor significativo tanto a los aspectos t\u00e9cnicos como no t\u00e9cnicos de una organizaci\u00f3n.",
            "title": "Analista de datos"
        },
        {
            "location": "/datascience/#ingeniero-de-datos",
            "text": "Los  Ingenieros de datos  construyen y optimizan los sistemas que permiten a los cient\u00edficos y analistas de datos realizar su trabajo. Cada empresa depende de los datos sean exactos y accesibles, para que las personas puedan trabajar con ellos.El  Ingeniero de datos  se asegura de que cualquier dato sea recibido, transformado, almacenado y hecho accesible para otros usuarios.  Los  Ingenieros de datos  son responsables de construir las herramientas para trabajar con datos y, a menudo, tienen que usar t\u00e9cnicas complejas para manejar los datos a escala. A diferencia de los cient\u00edficos y analistas de datos, la ingenier\u00eda de datos se inclina mucho m\u00e1s hacia un conjunto de habilidades de desarrollo de software.  Un buen  Ingeniero de datos  debe permitir que los cient\u00edficos o analistas de datos puedan concentrarse en resolver problemas, en lugar de tener que preocuparse por aspectos m\u00e1s t\u00e9cnicos de la disciplina, como por ejemplo mover los datos de una fuente a otra.  La mentalidad del  Ingeniero de datos  suele estar m\u00e1s centrada en la construcci\u00f3n y la optimizaci\u00f3n. Los siguientes son ejemplos de tareas en las que un ingeniero de datos podr\u00eda estar trabajando:   Creaci\u00f3n de APIs para el consumo de datos.  Integraci\u00f3n de conjuntos de datos externos o nuevos en los procesos de datos existentes.  Aplicaci\u00f3n de transformaciones de atributos para los modelos de  aprendizaje autom\u00e1tico .  Supervisar y probar continuamente los sistemas para asegurar un rendimiento optimizado.",
            "title": "Ingeniero de datos"
        },
        {
            "location": "/bigdata/",
            "text": "Introducci\u00f3n a la Big Data\n\n\n\n\nActualmente estamos viviendo la \nEra de los datos\n, con el enorme crecimiento en el uso de herramientas digitales como internet y los dispositivos m\u00f3biles, estamos generando datos constantemente, con cada movimiento que damos navegando en internet o interactuando en las redes sociales como Facebook o Twitter, estamos dejando una huella permante que queda almacenada en algun \nData Center\n. El volumen de informaci\u00f3n que estamos generando a cada segundo es enorme.\n\n\nEste gran crecimiento en el volumen de datos, ha obligado a rever la forma en la que dise\u00f1amos nuestros sistemas, y es as\u00ed, como el concepto de \nBig Data\n y la nueva dsiciplina de los \ncientificos de datos\n ha surgido.\n\n\n\u00bfQu\u00e9 es la Big Data?\n\n\nLa \nBig Data\n es la rama de las \nTeconlog\u00edas de la informaci\u00f3n\n que estudia las dificultades inherentes a la manipulaci\u00f3n de grandes \nconjuntos de datos\n. El verdadero poder de la \nBig Data\n reside en que se trata sobre el comportamiento de la gente, y no sobre sus datos, consiste en encontrar los patrones de relaciones y comportamientos en el caos de los grandes volumenes de datos que producimos. Los datos son tantos, que todos se vuelven estadistacamente significativos, lo que significa, que los m\u00e9todos tradicionales que utilizamos para analizarlos se han vuelto obsoletos.\n\n\nPara hacer frente a los nuevos desaf\u00edos que provoca la \nBig Data\n, nuevas herramientas informaticas han aperecido, sobre todo en el mundo del software \nopen source\n; es as\u00ed, como el movimiento de \nNoSQL\n y proyectos como el de \nApache Hadoop\n, han ganando popularidad.\n\n\nBases de datos NoSQL\n\n\nEl movimiento \nNoSQL\n hace referencia a una serie de nuevos sistemas de gestion de datos, que se alejan del tradicional modelo relacional que tienen las populares bases de datos actuales (\nOracle\n, \nDB2\n, \nMsSQL\n, \nMySQL\n, \nPostgreSQL\n, etc); la principal diferencia que caracteriza a estas nuevas bases de datos es que no poseen un esquema estructurado (tablas y sus relaciones) y no utilizan el lenguaje \nSQL\n para realizar sus consultas. Las bases de datos \nNoSQL\n surgieron principalmente para hacer frente al tratamiento de datos que los sistemas tradicionales de \nRDBMS\n no pod\u00edan manejar del todo bien (como ser toda la nueva informaci\u00f3n que surge del uso de las redes sociales). Se suelen agrupar en tres grandes grupos:\n\n\n\n\nBases de Datos Documentales:\n Donde la informaci\u00f3n se va almacenando en diferentes documentos, estos documentos son un conjunto ordenado de claves con valores asociados. El principal exponente de este tipo de base de datos \nNoSQL\n es \nMongoDB\n.\n\n\nBases de Datos clave-valor:\n  Donde los datos se van almenzanenda en pares (clave-valor). Un ejemplo de este tipo de bases es \nRedis\n.\n\n\nBig Tables:\n Grandes tablas de estructura tabular que se caracterizan por ser \ndistribuidas\n y de alta eficiencia. Las principales exponentes de este grupo son \nCassandra\n y \nHBase\n.\n\n\n\n\nHadoop\n\n\nEl proyecto \nApache Hadoop\n es un \nframework de software\n, desarrollado en el lenguaje de programci\u00f3n \nJava\n, que permite el procesamiento distribu\u00eddo de grandes \nconjuntos de datos\n a trav\u00e9s de \nclusters\n de computadoras utilizando simples modelos de programaci\u00f3n. La verdadera utilidad de \nHadoop\n radica en su capacidad para escalar facilmente de uno a miles de \nservidores distribuidos\n, cada uno aportando su poder de computaci\u00f3n y su capacidad de almecenamiento.\n\n\nLos modulos que forman parte del framework \nHadoop\n son:\n\n\n\n\nHadoop common:\n que incluye un conjunto de componentes e interfaces para el \nsistema de archivos\n distribu\u00eddo que implementa \nHadoop\n. Son las herramientas comunes que se utilizan en todos los m\u00f3dulos.\n\n\nHadoop Distributed File System (HDFS):\n Una de las herramientas principales del framework, el \nsistema de archivos\n distribu\u00eddo que corre entre los \nclusters\n de computadoras y que brinda el acceso a los datos para las aplicaciones.\n\n\nHadoop YARN(Yet Another Resource Negotiator):\n Es un sistema general de gestion de trabajos para correr aplicaciones \ndistribuidas\n.\n\n\nHadoop MapReduce:\n La herramienta principal del framework, y a la que \nHadoop\n debe su gran popularidad. Consiste en un modelo de procesamiento de datos \ndistribuidos\n que corre en forma paralela entre los \nclusters\n de computadoras que constituyen la arquitectura del framework \nHadoop\n. \nMapReduce\n es todo un modelo de programaci\u00f3n para el procesamientos de datos distribuidos. \n\n\n\n\nHadoop\n ha cambiado la econom\u00eda y la din\u00e1mica de la computaci\u00f3n a gran escala. Su impacto puede sintetizarse en cuatro caracter\u00edsticas principales.\n\n\nHadoop\n posibilita una soluci\u00f3n inform\u00e1tica que es:\n\n\n\n\nRedimensionable:\n Pueden agregarse tantos nuevos nodos como sea necesario, y agregarse sin tener que cambiar el formato de los datos, la forma en\n que se cargan los datos, la forma en que se escriben los procesos o las aplicaciones que est\u00e1n encima.\n\n\n\n\nRentable:\n \nHadoop\n incorpora masivamente la computaci\u00f3n paralela a los servidores b\u00e1sicos. El resultado de esto es una marcada reducci\u00f3n del costo por terabyte de almacenamiento, que a su vez abarata el modelado de sus datos.\n\n\n\n\n\n\nFlexible:\n \nHadoop\n funciona sin esquema y puede absorber cualquier tipo de datos, estructurados o no, provenientes de un n\u00famero cualquiera de fuentes. Los datos de diversas fuentes pueden agruparse de manera arbitraria y as\u00ed permitir an\u00e1lisis m\u00e1s profundos que los proporcionados por cualquier otro sistema.\n\n\n\n\n\n\nTolerante a fallas:\n Si usted pierde un nodo, el sistema redirige el trabajo a otra localizaci\u00f3n de los datos y contin\u00faa procesando sin perder el ritmo.\n\n\n\n\n\n\nHadoop\n es la herramienta que grandes empresas como Facebook, Yahoo, Linkedin y Twitter han elegido para almacenar todos los datos de sus usuarios y saberlo todo sobre ellos. \n\n\nSpark\n\n\nApache Spark\n es una de las nuevas estrellas en el \nan\u00e1lisis de datos\n masivos. Desarrollado en \nScala\n, \nApache Spark\n es una plataforma de computaci\u00f3n de c\u00f3digo abierto para el an\u00e1lisis y procesamiento de grandes vol\u00famenes de datos. \n\n\nAlgunas de las ventajas que nos ofrece \nApache Spark\n sobre otros \nframeworks\n, son:\n\n\n\n\n\n\nVelocidad:\n Sin dudas la velocidad es una de las principales fortalezas de \nApache Spark\n, como esta dise\u00f1ado para soportar el \nprocesameinto en memoria\n, puede alcanzar una performance sorprendente en an\u00e1lisis avanzados de datos. Algunos programas escritos utilizando \nApache Spark\n, pueden correr hasta 100x m\u00e1s r\u00e1pido que utilizando \nHadoop\n.\n\n\n\n\n\n\nF\u00e1cil de usar:\n Podemos escribir programas en \nPython\n, \nScala\n o \nJava\n que hagan uso de las herramientas que ofrece \nApache Spark\n; asimismo nos permite trabajar en forma interactiva (con \nPython\n o con \nScala\n) y su \nAPI\n es muy f\u00e1cil de aprender. \n\n\n\n\n\n\nGeneralismo:\n El mundo del an\u00e1lisis de datos incluye muchos subgrupos de distinta \u00edndole, est\u00e1n los que hacen un an\u00e1lisis investigativo, los que que realizan an\u00e1lisis exploratorios, los que construyen sistemas de procesamientos de datos, etc. Los usuarios de cada uno de esos subgrupos, al tener objetivos distintos, suelen utilizar una gran variedad de herramientas totalmente diferentes. \nApache Spark\n nos proporciona un gran n\u00famero de herramientas de alto nivel como \nSpark SQL\n, \nMLlib\n para \nmachine learning\n, \nGraphX\n, y \nSpark Streaming\n; las cuales pueden ser combinadas para crear aplicaciones multiprop\u00f3sito que ataquen los diferentes dominios del an\u00e1lisis de datos.\n\n\n\n\n\n\nOtras herramientas de Big data\n\n\nKafka\n\n\nKafka\n Es una plataforma distribuida de \nstreaming\n. Es decir, que tiene tres capacidades clave:\n\n\n\n\nPermite publicar y suscribirse a flujos de registros. A este respecto, es similar a una cola de mensajes o un sistema de mensajer\u00eda empresarial.\n\n\nPermite almacenar flujos de registros de forma tolerante a fallos.\n\n\nPermite procesar flujos de registros a medida que ocurren.\n\n\n\n\n\u00bfPara qu\u00e9 sirve \nKafka\n?\n\n\nSe utiliza para dos clases de aplicaci\u00f3n:\n\n\n\n\nCreaci\u00f3n de flujos de datos que fluyen en tiempo real entre distintos sistemas o aplicaciones.\n\n\nCreaci\u00f3n de aplicaciones de streaming en tiempo real que transforman o reaccionan a los flujos de datos.\n\n\n\n\nStorm\n\n\nStorm\n es una sistema de computaci\u00f3n distribuida en tiempo real y de c\u00f3digo abierto. Permite el procesamiento sencillo y fiable de grandes vol\u00famenes de datos en anal\u00edtica (por ejemplo para el estudio de informaci\u00f3n de modalidad continua procedente de redes sociales), \nRPC distribuida\n, \nprocesos de ETL\n. \n\n\nMientras que \nHadoop\n se encarga del procesamiento de datos por lotes, \nStorm\n se encarga de hacerlo en tiempo real. \n\n\nSi bien la \nBig Data\n trae muchas oportunidades, tambi\u00e9n abre muchas interrogantes, como ser: \u00bfCu\u00e1n \u00e9tica es esta recoleccion de datos silenciosa que realizan las grandes empresas sobre nuestros datos?; \u00bfQuienes son los verdaderos due\u00f1os de esos datos?\u00bfQu\u00e9 pueden hacer las empresas con ellos?, el debate esta abierto\u2026",
            "title": "Big Data"
        },
        {
            "location": "/bigdata/#introduccion-a-la-big-data",
            "text": "Actualmente estamos viviendo la  Era de los datos , con el enorme crecimiento en el uso de herramientas digitales como internet y los dispositivos m\u00f3biles, estamos generando datos constantemente, con cada movimiento que damos navegando en internet o interactuando en las redes sociales como Facebook o Twitter, estamos dejando una huella permante que queda almacenada en algun  Data Center . El volumen de informaci\u00f3n que estamos generando a cada segundo es enorme.  Este gran crecimiento en el volumen de datos, ha obligado a rever la forma en la que dise\u00f1amos nuestros sistemas, y es as\u00ed, como el concepto de  Big Data  y la nueva dsiciplina de los  cientificos de datos  ha surgido.",
            "title": "Introducci\u00f3n a la Big Data"
        },
        {
            "location": "/bigdata/#que-es-la-big-data",
            "text": "La  Big Data  es la rama de las  Teconlog\u00edas de la informaci\u00f3n  que estudia las dificultades inherentes a la manipulaci\u00f3n de grandes  conjuntos de datos . El verdadero poder de la  Big Data  reside en que se trata sobre el comportamiento de la gente, y no sobre sus datos, consiste en encontrar los patrones de relaciones y comportamientos en el caos de los grandes volumenes de datos que producimos. Los datos son tantos, que todos se vuelven estadistacamente significativos, lo que significa, que los m\u00e9todos tradicionales que utilizamos para analizarlos se han vuelto obsoletos.  Para hacer frente a los nuevos desaf\u00edos que provoca la  Big Data , nuevas herramientas informaticas han aperecido, sobre todo en el mundo del software  open source ; es as\u00ed, como el movimiento de  NoSQL  y proyectos como el de  Apache Hadoop , han ganando popularidad.",
            "title": "\u00bfQu\u00e9 es la Big Data?"
        },
        {
            "location": "/bigdata/#bases-de-datos-nosql",
            "text": "El movimiento  NoSQL  hace referencia a una serie de nuevos sistemas de gestion de datos, que se alejan del tradicional modelo relacional que tienen las populares bases de datos actuales ( Oracle ,  DB2 ,  MsSQL ,  MySQL ,  PostgreSQL , etc); la principal diferencia que caracteriza a estas nuevas bases de datos es que no poseen un esquema estructurado (tablas y sus relaciones) y no utilizan el lenguaje  SQL  para realizar sus consultas. Las bases de datos  NoSQL  surgieron principalmente para hacer frente al tratamiento de datos que los sistemas tradicionales de  RDBMS  no pod\u00edan manejar del todo bien (como ser toda la nueva informaci\u00f3n que surge del uso de las redes sociales). Se suelen agrupar en tres grandes grupos:   Bases de Datos Documentales:  Donde la informaci\u00f3n se va almacenando en diferentes documentos, estos documentos son un conjunto ordenado de claves con valores asociados. El principal exponente de este tipo de base de datos  NoSQL  es  MongoDB .  Bases de Datos clave-valor:   Donde los datos se van almenzanenda en pares (clave-valor). Un ejemplo de este tipo de bases es  Redis .  Big Tables:  Grandes tablas de estructura tabular que se caracterizan por ser  distribuidas  y de alta eficiencia. Las principales exponentes de este grupo son  Cassandra  y  HBase .",
            "title": "Bases de datos NoSQL"
        },
        {
            "location": "/bigdata/#hadoop",
            "text": "El proyecto  Apache Hadoop  es un  framework de software , desarrollado en el lenguaje de programci\u00f3n  Java , que permite el procesamiento distribu\u00eddo de grandes  conjuntos de datos  a trav\u00e9s de  clusters  de computadoras utilizando simples modelos de programaci\u00f3n. La verdadera utilidad de  Hadoop  radica en su capacidad para escalar facilmente de uno a miles de  servidores distribuidos , cada uno aportando su poder de computaci\u00f3n y su capacidad de almecenamiento.  Los modulos que forman parte del framework  Hadoop  son:   Hadoop common:  que incluye un conjunto de componentes e interfaces para el  sistema de archivos  distribu\u00eddo que implementa  Hadoop . Son las herramientas comunes que se utilizan en todos los m\u00f3dulos.  Hadoop Distributed File System (HDFS):  Una de las herramientas principales del framework, el  sistema de archivos  distribu\u00eddo que corre entre los  clusters  de computadoras y que brinda el acceso a los datos para las aplicaciones.  Hadoop YARN(Yet Another Resource Negotiator):  Es un sistema general de gestion de trabajos para correr aplicaciones  distribuidas .  Hadoop MapReduce:  La herramienta principal del framework, y a la que  Hadoop  debe su gran popularidad. Consiste en un modelo de procesamiento de datos  distribuidos  que corre en forma paralela entre los  clusters  de computadoras que constituyen la arquitectura del framework  Hadoop .  MapReduce  es todo un modelo de programaci\u00f3n para el procesamientos de datos distribuidos.    Hadoop  ha cambiado la econom\u00eda y la din\u00e1mica de la computaci\u00f3n a gran escala. Su impacto puede sintetizarse en cuatro caracter\u00edsticas principales.  Hadoop  posibilita una soluci\u00f3n inform\u00e1tica que es:   Redimensionable:  Pueden agregarse tantos nuevos nodos como sea necesario, y agregarse sin tener que cambiar el formato de los datos, la forma en\n que se cargan los datos, la forma en que se escriben los procesos o las aplicaciones que est\u00e1n encima.   Rentable:   Hadoop  incorpora masivamente la computaci\u00f3n paralela a los servidores b\u00e1sicos. El resultado de esto es una marcada reducci\u00f3n del costo por terabyte de almacenamiento, que a su vez abarata el modelado de sus datos.    Flexible:   Hadoop  funciona sin esquema y puede absorber cualquier tipo de datos, estructurados o no, provenientes de un n\u00famero cualquiera de fuentes. Los datos de diversas fuentes pueden agruparse de manera arbitraria y as\u00ed permitir an\u00e1lisis m\u00e1s profundos que los proporcionados por cualquier otro sistema.    Tolerante a fallas:  Si usted pierde un nodo, el sistema redirige el trabajo a otra localizaci\u00f3n de los datos y contin\u00faa procesando sin perder el ritmo.    Hadoop  es la herramienta que grandes empresas como Facebook, Yahoo, Linkedin y Twitter han elegido para almacenar todos los datos de sus usuarios y saberlo todo sobre ellos.",
            "title": "Hadoop"
        },
        {
            "location": "/bigdata/#spark",
            "text": "Apache Spark  es una de las nuevas estrellas en el  an\u00e1lisis de datos  masivos. Desarrollado en  Scala ,  Apache Spark  es una plataforma de computaci\u00f3n de c\u00f3digo abierto para el an\u00e1lisis y procesamiento de grandes vol\u00famenes de datos.   Algunas de las ventajas que nos ofrece  Apache Spark  sobre otros  frameworks , son:    Velocidad:  Sin dudas la velocidad es una de las principales fortalezas de  Apache Spark , como esta dise\u00f1ado para soportar el  procesameinto en memoria , puede alcanzar una performance sorprendente en an\u00e1lisis avanzados de datos. Algunos programas escritos utilizando  Apache Spark , pueden correr hasta 100x m\u00e1s r\u00e1pido que utilizando  Hadoop .    F\u00e1cil de usar:  Podemos escribir programas en  Python ,  Scala  o  Java  que hagan uso de las herramientas que ofrece  Apache Spark ; asimismo nos permite trabajar en forma interactiva (con  Python  o con  Scala ) y su  API  es muy f\u00e1cil de aprender.     Generalismo:  El mundo del an\u00e1lisis de datos incluye muchos subgrupos de distinta \u00edndole, est\u00e1n los que hacen un an\u00e1lisis investigativo, los que que realizan an\u00e1lisis exploratorios, los que construyen sistemas de procesamientos de datos, etc. Los usuarios de cada uno de esos subgrupos, al tener objetivos distintos, suelen utilizar una gran variedad de herramientas totalmente diferentes.  Apache Spark  nos proporciona un gran n\u00famero de herramientas de alto nivel como  Spark SQL ,  MLlib  para  machine learning ,  GraphX , y  Spark Streaming ; las cuales pueden ser combinadas para crear aplicaciones multiprop\u00f3sito que ataquen los diferentes dominios del an\u00e1lisis de datos.",
            "title": "Spark"
        },
        {
            "location": "/bigdata/#otras-herramientas-de-big-data",
            "text": "",
            "title": "Otras herramientas de Big data"
        },
        {
            "location": "/bigdata/#kafka",
            "text": "Kafka  Es una plataforma distribuida de  streaming . Es decir, que tiene tres capacidades clave:   Permite publicar y suscribirse a flujos de registros. A este respecto, es similar a una cola de mensajes o un sistema de mensajer\u00eda empresarial.  Permite almacenar flujos de registros de forma tolerante a fallos.  Permite procesar flujos de registros a medida que ocurren.   \u00bfPara qu\u00e9 sirve  Kafka ?  Se utiliza para dos clases de aplicaci\u00f3n:   Creaci\u00f3n de flujos de datos que fluyen en tiempo real entre distintos sistemas o aplicaciones.  Creaci\u00f3n de aplicaciones de streaming en tiempo real que transforman o reaccionan a los flujos de datos.",
            "title": "Kafka"
        },
        {
            "location": "/bigdata/#storm",
            "text": "Storm  es una sistema de computaci\u00f3n distribuida en tiempo real y de c\u00f3digo abierto. Permite el procesamiento sencillo y fiable de grandes vol\u00famenes de datos en anal\u00edtica (por ejemplo para el estudio de informaci\u00f3n de modalidad continua procedente de redes sociales),  RPC distribuida ,  procesos de ETL .   Mientras que  Hadoop  se encarga del procesamiento de datos por lotes,  Storm  se encarga de hacerlo en tiempo real.   Si bien la  Big Data  trae muchas oportunidades, tambi\u00e9n abre muchas interrogantes, como ser: \u00bfCu\u00e1n \u00e9tica es esta recoleccion de datos silenciosa que realizan las grandes empresas sobre nuestros datos?; \u00bfQuienes son los verdaderos due\u00f1os de esos datos?\u00bfQu\u00e9 pueden hacer las empresas con ellos?, el debate esta abierto\u2026",
            "title": "Storm"
        },
        {
            "location": "/NLP/",
            "text": "Introducci\u00f3n al Procesamiento del Lenguaje Natural\n\n\n\n\nEl \nlenguaje\n es una de las herramientas centrales en nuestra vida social y profesional. Entre otras cosas, act\u00faa como un medio para transmitir ideas, informaci\u00f3n, opiniones y sentimientos; as\u00ed como para persuadir, pedir informaci\u00f3n, o dar ordenes. Asimismo, el \nlenguaje\n humano es algo que esta en constante cambio y evoluci\u00f3n; y que puede llegar a ser muy \nambiguo\n y \nvariable\n. Tomemos por ejemplo la frase \n\"com\u00ed una pizza con amigos\"\n comparada con \n\"com\u00ed una pizza con aceitunas\"\n; su estructura es la misma, pero su significado es totalmente distinto. De la misma manera, un mismo mensaje puede ser expresado de formas diferentes; \n\"com\u00ed una pizza con amigos\"\n puede tambi\u00e9n ser expresado como \n\"compart\u00ed una pizza con amigos\"\n. \n\n\nLos seres humanos somos muy buenos a la hora de producir e interpretar el \nlenguaje\n, podemos expresar, percibir e interpretar significados muy elaborados en fracci\u00f3n de segundos casi sin dificultades; pero al mismo tiempo, somos tambi\u00e9n muy malos a la hora de entender y describir formalmente las reglas que lo gobiernan. Por este motivo, entender y producir el \nlenguaje\n por medio de una \ncomputadora\n es un problema muy dif\u00edcil de resolver. \u00c9ste problema, es el campo de estudio de lo que en \ninteligencia artificial\n se conoce como \nProcesamiento del Lenguaje Natural\n o \nNLP\n por sus siglas en ingl\u00e9s. \n\n\n\u00bfQu\u00e9 es el Procesamiento del Lenguaje Natural?\n\n\nEl \nProcesamiento del Lenguaje Natural\n o \nNLP\n  es una disciplina que se encuentra en la intersecci\u00f3n de varias ciencias, tales como las \nCiencias de la Computaci\u00f3n\n, la \nInteligencia Artificial\n y \nPsicolog\u00eda Cognitiva\n. Su idea central es la de darle a las m\u00e1quinas la capacidad de leer y comprender los idiomas que hablamos los humanos. La investigaci\u00f3n del \nProcesamiento del Lenguaje Natural\n tiene como objetivo responder a la pregunta de c\u00f3mo las personas son capaces de comprender el significado de una oraci\u00f3n oral / escrita y c\u00f3mo las personas entienden lo que sucedi\u00f3, cu\u00e1ndo y d\u00f3nde sucedi\u00f3; y las diferencias entre una suposici\u00f3n, una creencia o un hecho.\n\n\nLos elementos comunes de cualquier arquitectura est\u00e1ndar de un sistema para el \nProcesamiento del Lenguaje Natural\n son:\n\n\n\n\n\n\nReconocimiento de voz:\n Convertir una palabra hablada en un conjunto de palabras. Las palabras habladas se componen de una serie de par\u00e1metros relacionados con el sentido de la audici\u00f3n.\n\n\n\n\n\n\nComprensi\u00f3n del lenguaje:\n El objetivo de este elemento es generar un significado para las palabras habladas, y ese significado ser\u00e1 utilizado por el siguiente elemento (gesti\u00f3n del di\u00e1logo).\n\n\n\n\n\n\nGesti\u00f3n del di\u00e1logo:\n La tarea principal de este elemento es coordinar y mantener unidas todas las partes del sistema y los usuarios, y conectarse con otros sistemas.\n\n\n\n\n\n\nComunicaci\u00f3n con sistemas externos:\n como sistemas expertos, sistemas de bases de datos u otras aplicaciones inform\u00e1ticas.\n\n\n\n\n\n\nGeneraci\u00f3n de respuesta:\n Establecer el mensaje que el sistema debe entregar.\n\n\n\n\n\n\nSalida de voz:\n Uso de diferentes t\u00e9cnicas para producir el mensaje desde el sistema.\n\n\n\n\n\n\nEn general, en \nProcesamiento del Lenguaje Natural\n se utilizan seis niveles de comprensi\u00f3n con el objetivo de descubrir el significado del discurso. Estos niveles son:\n\n\n\n\n\n\nNivel fon\u00e9tico:\n Aqu\u00ed se presta atenci\u00f3n a la \nfon\u00e9tica\n, la forma en que las palabras son pronunciadas. Este nivel es importante cuando procesamos la palabra hablada, no as\u00ed cuando trabajamos con texto escrito. \n\n\n\n\n\n\nNivel morfol\u00f3gico:\n Aqu\u00ed nos interesa realizar un an\u00e1lisis \nmorfol\u00f3gico\n del discurso; estudiar la estructura de las palabras para delimitarlas y clasificarlas.\n\n\n\n\n\n\nNivel sint\u00e1ctico:\n Aqu\u00ed se realiza un an\u00e1lisis de \nsintaxis\n, el cual  incluye la acci\u00f3n de dividir una oraci\u00f3n en cada uno de sus componentes. \n\n\n\n\n\n\nNivel sem\u00e1ntico:\n Este nivel es un complemente del anterior, en el an\u00e1lisis \nsem\u00e1ntico\n se busca entender el significado de la oraci\u00f3n. Las palabras pueden tener m\u00faltiples significados, la idea es identificar el significado apropiado por medio del contexto de la oraci\u00f3n.\n\n\n\n\n\n\nNivel discursivo:\n El nivel discursivo examina el significado de la oraci\u00f3n en relaci\u00f3n a otra oraci\u00f3n en el texto o p\u00e1rrafo del mismo documento.\n\n\n\n\n\n\nNivel pragm\u00e1tico:\n Este nivel se ocupa del an\u00e1lisis de oraciones y c\u00f3mo se usan en diferentes situaciones. Adem\u00e1s, tambi\u00e9n c\u00f3mo su significado cambia dependiendo de la situaci\u00f3n.\n\n\n\n\n\n\nTodos los niveles descritos aqu\u00ed son inseparables y se complementan entre s\u00ed. El objetivo de los sistemas de \nNLP\n es incluir estas definiciones en una \ncomputadora\n y luego usarlas para crear una oraci\u00f3n estructurada y sin ambig\u00fcedades con un significado bien definido.\n\n\nAplicaciones del Procesamiento del Lenguaje Natural\n\n\nLos algoritmos de \nProcesamiento del Lenguaje Natural\n suelen basarse en algoritmos de \naprendizaje autom\u00e1tico\n. En lugar de codificar manualmente grandes conjuntos de reglas, el \nNLP\n puede confiar en el \naprendizaje autom\u00e1tico\n para aprender estas reglas autom\u00e1ticamente analizando un conjunto de ejemplos y haciendo una \ninferencia estad\u00edstica\n. En general, cuanto m\u00e1s datos analizados, m\u00e1s preciso ser\u00e1 el modelo. Estos algoritmos pueden ser utilizados en algunas de las siguientes aplicaciones:\n\n\n\n\n\n\nResumir texto:\n Podemos utilizar los modelos de \nNLP\n para extraer las ideas m\u00e1s importantes y centrales mientras ignoramos la informaci\u00f3n irrelevante.\n\n\n\n\n\n\nCrear chatbots:\n Podemos utilizar las t\u00e9cnicas de \nNLP\n para crear \nchatbots\n que puedan interactuar con las personas.\n\n\n\n\n\n\nGenerar autom\u00e1ticamente \netiquetas de palabras clave\n: Con \nNLP\n tambi\u00e9n podemos realizar un an\u00e1lisis de contenido aprovechando el algoritmo de \nLDA\n para asignar palabras claves a p\u00e1rrafos del texto.\n\n\n\n\n\n\nReconocer entidades\n: Con \nNLP\n podemos identificar a las distintas entidades del texto como ser una persona, lugar u organizaci\u00f3n.\n\n\n\n\n\n\nAn\u00e1lisis de sentimiento\n: Tambi\u00e9n podemos utilizar \nNLP\n para identificar el \nsentimiento\n de una cadena de texto, desde muy negativo a neutral y a muy positivo.\n\n\n\n\n\n\nCorpus ling\u00fc\u00edstico\n\n\nHoy en d\u00eda, es indispensable el uso de buenos recursos ling\u00fc\u00edsticos para el desarrollo de los sistemas de \nNLP\n. Estos recursos son esenciales para la creaci\u00f3n de gram\u00e1ticas, en el marco de aproximaciones simb\u00f3licas; o para llevar a cabo la formaci\u00f3n de m\u00f3dulos basados en el \naprendizaje autom\u00e1tico\n.\n\n\nUn \ncorpus ling\u00fc\u00edstico\n es un conjunto amplio y estructurado de ejemplos reales de uso de la lengua. Estos ejemplos pueden ser textos (los m\u00e1s comunes), o muestras orales (generalmente transcritas). Un \ncorpus ling\u00fc\u00edstico\n es un conjunto de textos relativamente grande, creado independientemente de sus posibles formas o usos. Es decir, en cuanto a su estructura, variedad y complejidad, un corpus debe reflejar una lengua, o su modalidad, de la forma m\u00e1s exacta posible; en cuanto a su uso, preocuparse de que su representaci\u00f3n sea real. La idea es que representen al lenguaje de la mejor forma posible para que los modelos de \nNLP\n puedan aprender los patrones necesarios para entender el \nlenguaje\n. Encontrar un buen \ncorpus\n sobre el cual trabajar no suele ser una tarea sencilla; uno que se suele utilizar para entrenar modelos es la informaci\u00f3n de \nwikipedia\n.\n\n\nLibrer\u00edas de Python para Procesamiento del Lenguaje Natural\n\n\nActualmente, \nPython\n es uno de los lenguajes m\u00e1s populares para trabajar en el campo la \nInteligencia Artificial\n. Para abordar los problemas relacionados con el \nProcesamiento del Lenguaje Natural\n \nPython\n nos proporciona las siguientes librer\u00edas:\n\n\n\n\n\n\nNLTK\n:\n Es la librer\u00eda l\u00edder para el \nProcesamiento del Lenguaje Natural\n. Proporciona interfaces f\u00e1ciles de usar a m\u00e1s de \n50 corpus\n y recursos l\u00e9xicos, junto con un conjunto de bibliotecas de procesamiento de texto para la clasificaci\u00f3n, tokenizaci\u00f3n, el etiquetado, el an\u00e1lisis y el razonamiento sem\u00e1ntico.\n\n\n\n\n\n\nSpacy\n:\n Es una librer\u00eda relativamente nueva que sobresale por su facilidad de uso y su velocidad a la hora de realizar el procesamiento de texto.\n\n\n\n\n\n\nGensim\n:\n Es una librer\u00eda dise\u00f1ada para extraer autom\u00e1ticamente los temas sem\u00e1nticos de los documentos de la forma m\u00e1s eficiente y con menos complicaciones posible.\n\n\n\n\n\n\npyLDAvis\n:\n Esta librer\u00eda est\u00e1 dise\u00f1ado para ayudar a los usuarios a interpretar los temas que surgen de un an\u00e1lisis de t\u00f3picos. Nos permite visualizar en forma muy sencilla cada uno de los temas incluidos en el texto.\n\n\n\n\n\n\nActualmente el \nNLP\n se esta sumando a la popularidad del \nDeep Learning\n, por lo que muchos de los \nframeworks\n que se utilizan en \nDeep Learning\n pueden ser aplicados para realizar modelos de \nNLP\n.\n\n\nDeep Learning y Procesamiento del Lenguaje Natural\n\n\nDurante mucho tiempo, las t\u00e9cnicas principales de \nProcesamiento del Lenguaje Natural\n fueron dominadas por m\u00e9todos de \naprendizaje autom\u00e1tico\n que utilizaron \nmodelos lineales\n como las \nm\u00e1quinas de vectores de soporte\n o la \nregresi\u00f3n log\u00edstica\n, entrenados sobre \nvectores\n de caracter\u00edsticas de muy alta dimensional pero muy escasos.\nRecientemente, el campo ha tenido cierto \u00e9xito en el cambio hacia modelos de \ndeep learning\n sobre entradas densas.\n\n\nLas \nredes neuronales\n proporcionan una poderosa maquina de aprendizaje que es muy atractiva para su uso en problemas de \nlenguaje natural\n. Un componente importante en las \nredes neuronales\n para el \nlenguaje\n es el uso de una capa de \nword embedding\n, una asignaci\u00f3n de s\u00edmbolos discretos a vectores continuos en un espacio dimensional relativamente bajo. Cuando se utiliza \nword embedding\n, se transforman los distintos s\u00edmbolos en objetos matem\u00e1ticos sobre los que se pueden realizar operaciones. En particular, la distancia entre vectores puede equipararse a la distancia entre palabras, facilitando la generalizaci\u00f3n del comportamiento de una palabra sobre otra.\nEsta representaci\u00f3n de palabras como vectores es aprendida por la red como parte del proceso de entrenamiento.\nSubiendo en la jerarqu\u00eda, la red tambi\u00e9n aprende a combinar los vectores de palabras de una manera que es \u00fatil para la predicci\u00f3n. Esta capacidad alivia en cierta medida los problemas de dispersi\u00f3n de los datos.\nHay dos tipos principales de \narquitecturas de redes neuronales\n que resultan muy \u00fatiles en los problemas de \nProcesamiento del Lenguaje Natural\n: las \nRedes neuronales prealimentadas\n y las \nRedes neuronales recurrentes\n.",
            "title": "Procesamiento de lenguaje"
        },
        {
            "location": "/NLP/#introduccion-al-procesamiento-del-lenguaje-natural",
            "text": "El  lenguaje  es una de las herramientas centrales en nuestra vida social y profesional. Entre otras cosas, act\u00faa como un medio para transmitir ideas, informaci\u00f3n, opiniones y sentimientos; as\u00ed como para persuadir, pedir informaci\u00f3n, o dar ordenes. Asimismo, el  lenguaje  humano es algo que esta en constante cambio y evoluci\u00f3n; y que puede llegar a ser muy  ambiguo  y  variable . Tomemos por ejemplo la frase  \"com\u00ed una pizza con amigos\"  comparada con  \"com\u00ed una pizza con aceitunas\" ; su estructura es la misma, pero su significado es totalmente distinto. De la misma manera, un mismo mensaje puede ser expresado de formas diferentes;  \"com\u00ed una pizza con amigos\"  puede tambi\u00e9n ser expresado como  \"compart\u00ed una pizza con amigos\" .   Los seres humanos somos muy buenos a la hora de producir e interpretar el  lenguaje , podemos expresar, percibir e interpretar significados muy elaborados en fracci\u00f3n de segundos casi sin dificultades; pero al mismo tiempo, somos tambi\u00e9n muy malos a la hora de entender y describir formalmente las reglas que lo gobiernan. Por este motivo, entender y producir el  lenguaje  por medio de una  computadora  es un problema muy dif\u00edcil de resolver. \u00c9ste problema, es el campo de estudio de lo que en  inteligencia artificial  se conoce como  Procesamiento del Lenguaje Natural  o  NLP  por sus siglas en ingl\u00e9s.",
            "title": "Introducci\u00f3n al Procesamiento del Lenguaje Natural"
        },
        {
            "location": "/NLP/#que-es-el-procesamiento-del-lenguaje-natural",
            "text": "El  Procesamiento del Lenguaje Natural  o  NLP   es una disciplina que se encuentra en la intersecci\u00f3n de varias ciencias, tales como las  Ciencias de la Computaci\u00f3n , la  Inteligencia Artificial  y  Psicolog\u00eda Cognitiva . Su idea central es la de darle a las m\u00e1quinas la capacidad de leer y comprender los idiomas que hablamos los humanos. La investigaci\u00f3n del  Procesamiento del Lenguaje Natural  tiene como objetivo responder a la pregunta de c\u00f3mo las personas son capaces de comprender el significado de una oraci\u00f3n oral / escrita y c\u00f3mo las personas entienden lo que sucedi\u00f3, cu\u00e1ndo y d\u00f3nde sucedi\u00f3; y las diferencias entre una suposici\u00f3n, una creencia o un hecho.  Los elementos comunes de cualquier arquitectura est\u00e1ndar de un sistema para el  Procesamiento del Lenguaje Natural  son:    Reconocimiento de voz:  Convertir una palabra hablada en un conjunto de palabras. Las palabras habladas se componen de una serie de par\u00e1metros relacionados con el sentido de la audici\u00f3n.    Comprensi\u00f3n del lenguaje:  El objetivo de este elemento es generar un significado para las palabras habladas, y ese significado ser\u00e1 utilizado por el siguiente elemento (gesti\u00f3n del di\u00e1logo).    Gesti\u00f3n del di\u00e1logo:  La tarea principal de este elemento es coordinar y mantener unidas todas las partes del sistema y los usuarios, y conectarse con otros sistemas.    Comunicaci\u00f3n con sistemas externos:  como sistemas expertos, sistemas de bases de datos u otras aplicaciones inform\u00e1ticas.    Generaci\u00f3n de respuesta:  Establecer el mensaje que el sistema debe entregar.    Salida de voz:  Uso de diferentes t\u00e9cnicas para producir el mensaje desde el sistema.    En general, en  Procesamiento del Lenguaje Natural  se utilizan seis niveles de comprensi\u00f3n con el objetivo de descubrir el significado del discurso. Estos niveles son:    Nivel fon\u00e9tico:  Aqu\u00ed se presta atenci\u00f3n a la  fon\u00e9tica , la forma en que las palabras son pronunciadas. Este nivel es importante cuando procesamos la palabra hablada, no as\u00ed cuando trabajamos con texto escrito.     Nivel morfol\u00f3gico:  Aqu\u00ed nos interesa realizar un an\u00e1lisis  morfol\u00f3gico  del discurso; estudiar la estructura de las palabras para delimitarlas y clasificarlas.    Nivel sint\u00e1ctico:  Aqu\u00ed se realiza un an\u00e1lisis de  sintaxis , el cual  incluye la acci\u00f3n de dividir una oraci\u00f3n en cada uno de sus componentes.     Nivel sem\u00e1ntico:  Este nivel es un complemente del anterior, en el an\u00e1lisis  sem\u00e1ntico  se busca entender el significado de la oraci\u00f3n. Las palabras pueden tener m\u00faltiples significados, la idea es identificar el significado apropiado por medio del contexto de la oraci\u00f3n.    Nivel discursivo:  El nivel discursivo examina el significado de la oraci\u00f3n en relaci\u00f3n a otra oraci\u00f3n en el texto o p\u00e1rrafo del mismo documento.    Nivel pragm\u00e1tico:  Este nivel se ocupa del an\u00e1lisis de oraciones y c\u00f3mo se usan en diferentes situaciones. Adem\u00e1s, tambi\u00e9n c\u00f3mo su significado cambia dependiendo de la situaci\u00f3n.    Todos los niveles descritos aqu\u00ed son inseparables y se complementan entre s\u00ed. El objetivo de los sistemas de  NLP  es incluir estas definiciones en una  computadora  y luego usarlas para crear una oraci\u00f3n estructurada y sin ambig\u00fcedades con un significado bien definido.",
            "title": "\u00bfQu\u00e9 es el Procesamiento del Lenguaje Natural?"
        },
        {
            "location": "/NLP/#aplicaciones-del-procesamiento-del-lenguaje-natural",
            "text": "Los algoritmos de  Procesamiento del Lenguaje Natural  suelen basarse en algoritmos de  aprendizaje autom\u00e1tico . En lugar de codificar manualmente grandes conjuntos de reglas, el  NLP  puede confiar en el  aprendizaje autom\u00e1tico  para aprender estas reglas autom\u00e1ticamente analizando un conjunto de ejemplos y haciendo una  inferencia estad\u00edstica . En general, cuanto m\u00e1s datos analizados, m\u00e1s preciso ser\u00e1 el modelo. Estos algoritmos pueden ser utilizados en algunas de las siguientes aplicaciones:    Resumir texto:  Podemos utilizar los modelos de  NLP  para extraer las ideas m\u00e1s importantes y centrales mientras ignoramos la informaci\u00f3n irrelevante.    Crear chatbots:  Podemos utilizar las t\u00e9cnicas de  NLP  para crear  chatbots  que puedan interactuar con las personas.    Generar autom\u00e1ticamente  etiquetas de palabras clave : Con  NLP  tambi\u00e9n podemos realizar un an\u00e1lisis de contenido aprovechando el algoritmo de  LDA  para asignar palabras claves a p\u00e1rrafos del texto.    Reconocer entidades : Con  NLP  podemos identificar a las distintas entidades del texto como ser una persona, lugar u organizaci\u00f3n.    An\u00e1lisis de sentimiento : Tambi\u00e9n podemos utilizar  NLP  para identificar el  sentimiento  de una cadena de texto, desde muy negativo a neutral y a muy positivo.",
            "title": "Aplicaciones del Procesamiento del Lenguaje Natural"
        },
        {
            "location": "/NLP/#corpus-linguistico",
            "text": "Hoy en d\u00eda, es indispensable el uso de buenos recursos ling\u00fc\u00edsticos para el desarrollo de los sistemas de  NLP . Estos recursos son esenciales para la creaci\u00f3n de gram\u00e1ticas, en el marco de aproximaciones simb\u00f3licas; o para llevar a cabo la formaci\u00f3n de m\u00f3dulos basados en el  aprendizaje autom\u00e1tico .  Un  corpus ling\u00fc\u00edstico  es un conjunto amplio y estructurado de ejemplos reales de uso de la lengua. Estos ejemplos pueden ser textos (los m\u00e1s comunes), o muestras orales (generalmente transcritas). Un  corpus ling\u00fc\u00edstico  es un conjunto de textos relativamente grande, creado independientemente de sus posibles formas o usos. Es decir, en cuanto a su estructura, variedad y complejidad, un corpus debe reflejar una lengua, o su modalidad, de la forma m\u00e1s exacta posible; en cuanto a su uso, preocuparse de que su representaci\u00f3n sea real. La idea es que representen al lenguaje de la mejor forma posible para que los modelos de  NLP  puedan aprender los patrones necesarios para entender el  lenguaje . Encontrar un buen  corpus  sobre el cual trabajar no suele ser una tarea sencilla; uno que se suele utilizar para entrenar modelos es la informaci\u00f3n de  wikipedia .",
            "title": "Corpus ling\u00fc\u00edstico"
        },
        {
            "location": "/NLP/#librerias-de-python-para-procesamiento-del-lenguaje-natural",
            "text": "Actualmente,  Python  es uno de los lenguajes m\u00e1s populares para trabajar en el campo la  Inteligencia Artificial . Para abordar los problemas relacionados con el  Procesamiento del Lenguaje Natural   Python  nos proporciona las siguientes librer\u00edas:    NLTK :  Es la librer\u00eda l\u00edder para el  Procesamiento del Lenguaje Natural . Proporciona interfaces f\u00e1ciles de usar a m\u00e1s de  50 corpus  y recursos l\u00e9xicos, junto con un conjunto de bibliotecas de procesamiento de texto para la clasificaci\u00f3n, tokenizaci\u00f3n, el etiquetado, el an\u00e1lisis y el razonamiento sem\u00e1ntico.    Spacy :  Es una librer\u00eda relativamente nueva que sobresale por su facilidad de uso y su velocidad a la hora de realizar el procesamiento de texto.    Gensim :  Es una librer\u00eda dise\u00f1ada para extraer autom\u00e1ticamente los temas sem\u00e1nticos de los documentos de la forma m\u00e1s eficiente y con menos complicaciones posible.    pyLDAvis :  Esta librer\u00eda est\u00e1 dise\u00f1ado para ayudar a los usuarios a interpretar los temas que surgen de un an\u00e1lisis de t\u00f3picos. Nos permite visualizar en forma muy sencilla cada uno de los temas incluidos en el texto.    Actualmente el  NLP  se esta sumando a la popularidad del  Deep Learning , por lo que muchos de los  frameworks  que se utilizan en  Deep Learning  pueden ser aplicados para realizar modelos de  NLP .",
            "title": "Librer\u00edas de Python para Procesamiento del Lenguaje Natural"
        },
        {
            "location": "/NLP/#deep-learning-y-procesamiento-del-lenguaje-natural",
            "text": "Durante mucho tiempo, las t\u00e9cnicas principales de  Procesamiento del Lenguaje Natural  fueron dominadas por m\u00e9todos de  aprendizaje autom\u00e1tico  que utilizaron  modelos lineales  como las  m\u00e1quinas de vectores de soporte  o la  regresi\u00f3n log\u00edstica , entrenados sobre  vectores  de caracter\u00edsticas de muy alta dimensional pero muy escasos.\nRecientemente, el campo ha tenido cierto \u00e9xito en el cambio hacia modelos de  deep learning  sobre entradas densas.  Las  redes neuronales  proporcionan una poderosa maquina de aprendizaje que es muy atractiva para su uso en problemas de  lenguaje natural . Un componente importante en las  redes neuronales  para el  lenguaje  es el uso de una capa de  word embedding , una asignaci\u00f3n de s\u00edmbolos discretos a vectores continuos en un espacio dimensional relativamente bajo. Cuando se utiliza  word embedding , se transforman los distintos s\u00edmbolos en objetos matem\u00e1ticos sobre los que se pueden realizar operaciones. En particular, la distancia entre vectores puede equipararse a la distancia entre palabras, facilitando la generalizaci\u00f3n del comportamiento de una palabra sobre otra.\nEsta representaci\u00f3n de palabras como vectores es aprendida por la red como parte del proceso de entrenamiento.\nSubiendo en la jerarqu\u00eda, la red tambi\u00e9n aprende a combinar los vectores de palabras de una manera que es \u00fatil para la predicci\u00f3n. Esta capacidad alivia en cierta medida los problemas de dispersi\u00f3n de los datos.\nHay dos tipos principales de  arquitecturas de redes neuronales  que resultan muy \u00fatiles en los problemas de  Procesamiento del Lenguaje Natural : las  Redes neuronales prealimentadas  y las  Redes neuronales recurrentes .",
            "title": "Deep Learning y Procesamiento del Lenguaje Natural"
        },
        {
            "location": "/python/",
            "text": "Introducci\u00f3n a Python\n\n\nPython\n es actualmente uno de los lenguajes m\u00e1s utilizados en \ninteligencia artificial\n y \nCiencia de datos\n; es un lenguaje de programaci\u00f3n de alto nivel que se caracteriza por hacer hincapi\u00e9 en una sintaxis limpia, que favorece un c\u00f3digo legible y f\u00e1cilmente administrable. \nPython\n funciona en las plataformas Windows, Linux/Unix, Mac OS X e incluso ha sido portado a las m\u00e1quinas virtuales de \nJava\n (a trav\u00e9s de \nJython\n) y \n.Net\n (a trav\u00e9s de \nIronPython\n). \nPython\n es un lenguaje libre y f\u00e1cil de aprender que te permite trabajar m\u00e1s r\u00e1pido e integrar tus sistemas de manera m\u00e1s eficaz; con \nPython\n se puede ganar r\u00e1pidamente en productividad.\n\n\nPython\n, a diferencia de otros lenguajes de programaci\u00f3n como \nC\n, \nC++\n o \nJava\n es \ninterpretado y dinamicamente tipado\n; lo que quiere decir que no es necesario compilar el fuente para poder ejecutarlo (\ninterpretado\n) y que sus variables pueden tomar distintos tipos de objetos (\ndinamicamente tipado\n); esto hace que el lenguaje sea sumamente flexible y de r\u00e1pida implementaci\u00f3n; aunque pierde en rendimiento y es m\u00e1s propenso a errores de programaci\u00f3n que los lenguajes antes mencionados.\n\n\nPrincipales fortalezas de Python\n\n\nLas principales fortalezas que hacen que uno ame a \nPython\n son:\n\n\n\n\n\n\nEs Orientado a Objetos.\n \nPython\n es un lenguaje de programaci\u00f3n \nOrientado a Objetos\n desde casi su concepci\u00f3n, su modelo de clases soporta las notaciones avanzadas de polimorfismo, sobrecarga de operadores y herencia m\u00faltiple. La programaci\u00f3n \nOrientado a Objetos\n es sumamente f\u00e1cil de aplicar con la sintaxis simple que nos proporciona \nPython\n. Asimismo, tambi\u00e9n es importante destacar que en \nPython\n, la programaci\u00f3n \nOrientado a Objetos\n es una opci\u00f3n y no algo obligatorio como es en \nJava\n; ya que \nPython\n es \nmultiparadigma\n y nos permite programar siguiendo un modelo \nOrientado a Objetos\n o un modelo \nimperativo\n.\n\n\n\n\n\n\nEs software libre\n. \nPython\n es completamente libre para ser utilizado y redistribuido; no posee restricciones para copiarlo, embeberlo en nuestros sistemas o ser vendido junto con otros productos. \nPython\n es un proyecto \nopen source\n que es administrado por \nPython Software Foundation\n, instituci\u00f3n que se encarga de su soporte y desarrollo.\n\n\n\n\n\n\nEs portable\n. La implementaci\u00f3n estandar de \nPython\n esta escrita en \nC\n, y puede ser compilada y ejecutada en pr\u00e1cticamente cualquier plataforma que se les ocurra. Podemos encontrar a \nPython\n en peque\u00f1os dispositivos, como tel\u00e9fonos celulares, hasta grandes infraestructuras de \nHardware\n, como las supercomputadoras. Al ser un lenguaje \ninterpretado\n el mismo \nc\u00f3digo fuente\n puede ser ejecutado en cualquier plataforma sin necesidad de realizar grandes cambios.\n\n\n\n\n\n\nEs poderoso\n. \nPython\n proporciona toda la sencillez y facilidad de uso de un lenguaje de programaci\u00f3n \ninterpretado\n, junto con las m\u00e1s avanzadas herramientas de ingenier\u00eda de software que se encuentran t\u00edpicamente en los lenguajes compilados. A diferencia de otros lenguajes \ninterpretados\n, esta combinaci\u00f3n hace a \nPython\n sumamente \u00fatil para proyectos de desarrollo a gran escala.\n\n\n\n\n\n\nF\u00e1cil integraci\u00f3n\n. Los programas escritos en \nPython\n pueden ser f\u00e1cilmente integrados con componentes escritos en otros lenguajes. Por ejemplo la \nC\n \nAPI\n de \nPython\n permite una f\u00e1cil integraci\u00f3n entre los dos lenguajes, permitiendo que los programas escritos en \nPython\n puedan llamar a funciones escritas en \nC\n y viceversa. \n\n\n\n\n\n\nF\u00e1cil de usar\n. Para ejecutar un programa en \nPython\n simplemente debemos escribirlo y ejecutarlo, no existen pasos intermedios de linkeo o compilaci\u00f3n como podemos tener en otros lenguajes de programaci\u00f3n. Con \nPython\n podemos programar en forma interactiva, basta tipear una sentencia para poder ver inmediatamente el resultado. Adem\u00e1s los programas en \nPython\n son m\u00e1s simples, m\u00e1s peque\u00f1os y m\u00e1s flexibles que los programas equivalentes en lenguajes como \nC\n, \nC++\n o \nJava\n.\n\n\n\n\n\n\nF\u00e1cil de aprender\n. Desde mi punto de vista, esta es sin duda la principal fortaleza del lenguaje; comparado con otros lenguajes de programaci\u00f3n, \nPython\n es sumamente f\u00e1cil de aprender, en tan s\u00f3lo un par de d\u00edas se puede estar programando eficientemente con \nPython\n.\n\n\n\n\n\n\nInstalando Python\n\n\nEn Linux\n\n\nInstalar \nPython\n en \nLinux\n no es necesario, ya que viene preinstalado en todas las distribuciones m\u00e1s populares.\n\n\nEn Windows\n\n\nLa forma m\u00e1s sencilla de poder instalar \nPython\n en Windows es instalando alguna de las distribuciones de \nPython\n que ya vienen armadas con los principales m\u00f3dulos. Yo les recomiendo la distribuci\u00f3n \nAnaconda\n, que se puede descargar en forma gratuita y viene integrada con todos los principales paquetes que vamos a necesitar para trabajar con \nPython\n. Una vez que la descargan, simplemente siguen los pasos del instalador y listo, ya tendr\u00e1n todo un ambiente \nPython\n para trabajar en Windows.\n\n\nOtra distribuci\u00f3n de \nPython\n que pueden utilizar en Windows, es \nWinPython\n, la cual puede ser utilizada incluso en forma portable.\n\n\nLibrer\u00edas esenciales para el analisis de datos\n\n\nNumpy\n\n\nNumpy\n, abreviatura de Numerical \nPython\n , es el paquete fundamental para la computaci\u00f3n cient\u00edfica en \nPython\n. Dispone, entre otras cosas de:\n\n\n\n\nUn objeto \nmatriz\n multidimensional \nndarray\n,r\u00e1pido y eficiente.\n\n\nFunciones para realizar c\u00e1lculos elemento a elemento u otras operaciones matem\u00e1ticas con \nmatrices\n. \n\n\nHerramientas para la lectura y escritura de los conjuntos de datos basados \nmatrices\n.\n\n\nOperaciones de \n\u00e1lgebra lineal\n, \ntransformaciones de Fourier\n, y generaci\u00f3n de n\u00fameros aleatorios.\n\n\nHerramientas de integraci\u00f3n para conectar \nC\n, \nC++\n y \nFortran\n con \nPython\n\n\n\n\nM\u00e1s all\u00e1 de las capacidades de procesamiento r\u00e1pido de \nmatrices\n que \nNumpy\n a\u00f1ade a \nPython\n, uno de sus\nprop\u00f3sitos principales con respecto al an\u00e1lisis de datos es la utilizaci\u00f3n de sus \nestructuras de datos\n como contenedores para transmitir los datos entre diferentes algoritmos. Para datos num\u00e9ricos , las \nmatrices\n de \nNumpy\n son una forma mucho m\u00e1s eficiente de almacenar y manipular datos que cualquier otra de las \nestructuras de datos\n est\u00e1ndar incorporadas en \nPython\n. Asimismo, librer\u00edas escritas en un lenguaje de bajo nivel, como \nC\n o \nFortran\n, pueden operar en los datos almacenados en \nmatrices\n de \nNumpy\n sin necesidad de copiar o modificar ning\u00fan dato.\n\n\nJupyter\n\n\nJupyter\n promueve un ambiente de trabajo de \nejecutar-explorar\n en contraposici\u00f3n al tradicional modelo de desarrollo de software de \neditar-compilar-ejecutar\n. Es decir, que el problema computacional a resolver es m\u00e1s visto como todo un proceso de ejecucion de tareas, en lugar del tradicional modelo de producir una respuesta(\noutput\n) a una pregunta(\ninput\n).  \nJupyter\n tambi\u00e9n provee una estrecha integraci\u00f3n con nuestro sistema operativo, permitiendo acceder f\u00e1cilmente a todos nuestros archivos desde la misma herramienta.\n\n\nAlgunas de las caracter\u00edsticas sobresalientes de \nJupyter\n son:\n\n\n\n\nSu poderoso \nshell\n interactivo con soporte para m\u00faltiples lenguajes.\n\n\nNotebook\n, su interfase web con soporte para c\u00f3digo, texto, expresiones matem\u00e1ticas, gr\u00e1ficos en l\u00ednea y multimedia.\n\n\nSu soporte para poder realizar visualizaciones de datos en forma interactiva. \nJupyter\n esta totalmente integrado con \nmatplotlib\n.\n\n\nSu simple y flexible interfase para trabajar con la \ncomputaci\u00f3n paralela\n.\n\n\n\n\nMatplotlib\n\n\nMatplotlib\n es la librer\u00eda m\u00e1s popular en \nPython\n para visualizaciones y gr\u00e1ficos. \nMatplotlib\n puede producir gr\u00e1ficos de alta calidad dignos de cualquier publicaci\u00f3n cient\u00edfica.\n\n\nAlgunas de las muchas ventajas que nos ofrece \nMatplotlib\n, incluyen:\n\n\n\n\nEs f\u00e1cil de aprender.\n\n\nSoporta texto, t\u00edtulos y etiquetas en formato $\\LaTeX$.\n\n\nProporciona un gran control sobre cada uno de los elementos de las figuras, como ser su tama\u00f1o, el trazado de sus l\u00edneas, etc.\n\n\nNos permite crear gr\u00e1ficos y figuras de gran calidad que pueden ser guardados en varios formatos, como ser: PNG, PDF, SVG, EPS, y PGF.\n\n\n\n\nMatplotlib\n se integra de maravilla con \nJupyter\n (ver m\u00e1s abajo), lo que nos proporciona un ambiente confortable para las visualizaciones y la exploraci\u00f3n de datos interactiva.\n\n\nPandas\n\n\nPandas\n es una librer\u00eda \nopen source\n que aporta a \nPython\n unas estructuras de datos f\u00e1ciles de user y de alta performance, junto con un gran n\u00famero de funciones esenciales para el an\u00e1lisis de datos. Con la ayuda de \nPandas\n podemos trabajar con \ndatos estructurados\n de una forma m\u00e1s r\u00e1pida y expresiva.\n\n\nAlgunas de las cosas sobresalientes que nos aporta \nPandas\n son:\n\n\n\n\nUn r\u00e1pido y eficiente objeto \nDataFrame\n para manipular datos con indexaci\u00f3n integrada;\n\n\nherramientas para la \nlectura y escritura de datos\n entre estructuras de datos r\u00e1pidas y eficientes manejadas en memoria, como el \nDataFrame\n, con la mayor\u00eda de los formatos conocidos para el manejo de datos, como ser: CSV y archivos de texto, archivos Microsoft Excel, bases de datos \nSQL\n, y el formato cient\u00edfico HDF5.\n\n\nProporciona una \nalineaci\u00f3n inteligente de datos\n y un manejo integrado de los datos faltantes; con estas funciones podemos obtener una ganancia de performace en los c\u00e1lculos entre \nDataFrames\n y una f\u00e1cil manipulaci\u00f3n y ordenamiento de los datos de nuestro \ndata set\n;\n\n\nFlexibilidad para \nmanipular y redimensionar\n nuestro \ndata set\n, facilidad para construir \ntablas pivote\n;\n\n\nLa posibilidad de \nfiltrar los datos, agregar o eliminar columnas\n de una forma sumamente expresiva;\n\n\nOperaciones de \nmerge\n y *join\n* altamente eficientes sobre nuestros conjuntos de datos;\n\n\nIndexaci\u00f3n jer\u00e1rquica\n que proporciona una forma intuitiva de trabajar con datos de alta dimensi\u00f3n en una estructura de datos de menor dimensi\u00f3n ;\n\n\nPosibilidad de realizar c\u00e1lculos agregados o transformaciones de datos con el poderoso motor \ngroup by\n que nos permite dividir-aplicar-combinar nuestros conjuntos de datos;\n\n\ncombina las \ncaracter\u00edsticas de las matrices de alto rendimiento de \nNumpy\n con las flexibles capacidades de manipulaci\u00f3n de datos de las hojas de c\u00e1lculo\n y bases de datos relacionales (tales como \nSQL\n);\n\n\nGran n\u00famero de funcionalidades para el manejo de \nseries de tiempo\n ideales para el an\u00e1lisis financiero;\n\n\nTodas sus funciones y estructuras de datos est\u00e1n \noptimizadas para el alto rendimiento\n, con las partes cr\u00edticas del c\u00f3digo escritas en \nCython\n o \nC\n.\n\n\n\n\nScikit-Lean\n\n\nScikit-learn\n es una librer\u00eda especializada en algoritmos para \ndata mining\n y \nmachine learning\n.  \n\n\nAlgunos de los problemas que podemos resolver utilizando las herramientas de \nScikit-learn\n, son:\n\n\n\n\nClasificaciones\n: Identificar las categor\u00edas a que cada observaci\u00f3n del conjunto de datos pertenece.\n\n\nRegresiones\n: Predecire el valor continuo para cada nuevo ejemplo.\n\n\nAgrupaciones\n: Agrupaci\u00f3n autom\u00e1tica de objetos similares en un conjunto.\n\n\nReducci\u00f3n de dimensiones\n: Reducir el n\u00famero de variables aleatorias a considerar.\n\n\nSelecci\u00f3n de Modelos\n: Comparar, validar y elegir par\u00e1metros y modelos.\n\n\nPreprocesamiento\n: Extracci\u00f3n de caracter\u00edsticas a analizar y normalizaci\u00f3n de datos.\n\n\n\n\nSciPy\n\n\nSciPy\n es un conjunto de paquetes donde cada uno ellos ataca un problema distinto dentro de la computaci\u00f3n cient\u00edfica y el an\u00e1lisis num\u00e9rico. Algunos de los paquetes que incluye, son:\n\n\n\n\nscipy.integrate\n: que proporciona diferentes funciones para resolver problemas de integraci\u00f3n num\u00e9rica.\n\n\nscipy.linalg\n: que proporciona funciones para resolver problemas de \u00e1lgebra lineal.\n\n\nscipy.optimize\n: para los problemas de optimizaci\u00f3n y minimizaci\u00f3n.\n\n\nscipy.signal\n: para el an\u00e1lisis y procesamiento de se\u00f1ales.\n\n\nscipy.sparse\n: para matrices dispersas y solucionar sistemas lineales dispersos\n\n\nscipy.stats\n: para el an\u00e1lisis de estad\u00edstica y probabilidades.\n\n\n\n\nFrameworks para Deep Learning\n\n\nEn estos momentos, si hay un campo en donde \nPython\n sobresale sobre cualquier otro lenguaje, es en su soporte para frameworks de \nDeep Learning\n. Existen una gran variedad y de muy buena calidad, entre los que se destacan:\n\n\n\n\n\n\nTensorFlow\n: \nTensorFlow\n es un frameworks desarrollado por Google. Es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos. \n\n\n\n\n\n\nPyTorch\n: \nPyTorch\n es un framework de \nDeep Learning\n que utiliza el lenguaje \nPython\n y cuenta con el apoyo de Facebook.\n\n\n\n\n\n\nTheano\n: \nTheano\n es una librer\u00eda de \nPython\n que permite definir, optimizar y evaluar expresiones matem\u00e1ticas que involucran tensores de manera eficiente. \n\n\n\n\n\n\nCNTK\n: \nCNTK\n es un conjunto de herramientas, desarrolladas por Microsoft, f\u00e1ciles de usar, de c\u00f3digo abierto que entrena algoritmos de \nDeep Learning\n para aprender como el cerebro humano.\n\n\n\n\n\n\nKeras\n: \nKeras\n es una librer\u00eda de alto nivel, muy f\u00e1cil de utilizar. Est\u00e1 escrita y mantenida por Francis Chollet, miembro del equipo de Google Brain. Permite a los usuarios elegir si los modelos que se construyen seran ejecutados en el grafo simb\u00f3lico de \nTheano\n, \nTensorFlow\n o \nCNTK\n.\n\n\n\n\n\n\nMXNet\n: \nMXNet\n es una librer\u00eda flexible y eficiente para armar modelos de \nDeep Learning\n con soporte para varios idiomas.",
            "title": "Intro Python"
        },
        {
            "location": "/python/#introduccion-a-python",
            "text": "Python  es actualmente uno de los lenguajes m\u00e1s utilizados en  inteligencia artificial  y  Ciencia de datos ; es un lenguaje de programaci\u00f3n de alto nivel que se caracteriza por hacer hincapi\u00e9 en una sintaxis limpia, que favorece un c\u00f3digo legible y f\u00e1cilmente administrable.  Python  funciona en las plataformas Windows, Linux/Unix, Mac OS X e incluso ha sido portado a las m\u00e1quinas virtuales de  Java  (a trav\u00e9s de  Jython ) y  .Net  (a trav\u00e9s de  IronPython ).  Python  es un lenguaje libre y f\u00e1cil de aprender que te permite trabajar m\u00e1s r\u00e1pido e integrar tus sistemas de manera m\u00e1s eficaz; con  Python  se puede ganar r\u00e1pidamente en productividad.  Python , a diferencia de otros lenguajes de programaci\u00f3n como  C ,  C++  o  Java  es  interpretado y dinamicamente tipado ; lo que quiere decir que no es necesario compilar el fuente para poder ejecutarlo ( interpretado ) y que sus variables pueden tomar distintos tipos de objetos ( dinamicamente tipado ); esto hace que el lenguaje sea sumamente flexible y de r\u00e1pida implementaci\u00f3n; aunque pierde en rendimiento y es m\u00e1s propenso a errores de programaci\u00f3n que los lenguajes antes mencionados.",
            "title": "Introducci\u00f3n a Python"
        },
        {
            "location": "/python/#principales-fortalezas-de-python",
            "text": "Las principales fortalezas que hacen que uno ame a  Python  son:    Es Orientado a Objetos.   Python  es un lenguaje de programaci\u00f3n  Orientado a Objetos  desde casi su concepci\u00f3n, su modelo de clases soporta las notaciones avanzadas de polimorfismo, sobrecarga de operadores y herencia m\u00faltiple. La programaci\u00f3n  Orientado a Objetos  es sumamente f\u00e1cil de aplicar con la sintaxis simple que nos proporciona  Python . Asimismo, tambi\u00e9n es importante destacar que en  Python , la programaci\u00f3n  Orientado a Objetos  es una opci\u00f3n y no algo obligatorio como es en  Java ; ya que  Python  es  multiparadigma  y nos permite programar siguiendo un modelo  Orientado a Objetos  o un modelo  imperativo .    Es software libre .  Python  es completamente libre para ser utilizado y redistribuido; no posee restricciones para copiarlo, embeberlo en nuestros sistemas o ser vendido junto con otros productos.  Python  es un proyecto  open source  que es administrado por  Python Software Foundation , instituci\u00f3n que se encarga de su soporte y desarrollo.    Es portable . La implementaci\u00f3n estandar de  Python  esta escrita en  C , y puede ser compilada y ejecutada en pr\u00e1cticamente cualquier plataforma que se les ocurra. Podemos encontrar a  Python  en peque\u00f1os dispositivos, como tel\u00e9fonos celulares, hasta grandes infraestructuras de  Hardware , como las supercomputadoras. Al ser un lenguaje  interpretado  el mismo  c\u00f3digo fuente  puede ser ejecutado en cualquier plataforma sin necesidad de realizar grandes cambios.    Es poderoso .  Python  proporciona toda la sencillez y facilidad de uso de un lenguaje de programaci\u00f3n  interpretado , junto con las m\u00e1s avanzadas herramientas de ingenier\u00eda de software que se encuentran t\u00edpicamente en los lenguajes compilados. A diferencia de otros lenguajes  interpretados , esta combinaci\u00f3n hace a  Python  sumamente \u00fatil para proyectos de desarrollo a gran escala.    F\u00e1cil integraci\u00f3n . Los programas escritos en  Python  pueden ser f\u00e1cilmente integrados con componentes escritos en otros lenguajes. Por ejemplo la  C   API  de  Python  permite una f\u00e1cil integraci\u00f3n entre los dos lenguajes, permitiendo que los programas escritos en  Python  puedan llamar a funciones escritas en  C  y viceversa.     F\u00e1cil de usar . Para ejecutar un programa en  Python  simplemente debemos escribirlo y ejecutarlo, no existen pasos intermedios de linkeo o compilaci\u00f3n como podemos tener en otros lenguajes de programaci\u00f3n. Con  Python  podemos programar en forma interactiva, basta tipear una sentencia para poder ver inmediatamente el resultado. Adem\u00e1s los programas en  Python  son m\u00e1s simples, m\u00e1s peque\u00f1os y m\u00e1s flexibles que los programas equivalentes en lenguajes como  C ,  C++  o  Java .    F\u00e1cil de aprender . Desde mi punto de vista, esta es sin duda la principal fortaleza del lenguaje; comparado con otros lenguajes de programaci\u00f3n,  Python  es sumamente f\u00e1cil de aprender, en tan s\u00f3lo un par de d\u00edas se puede estar programando eficientemente con  Python .",
            "title": "Principales fortalezas de Python"
        },
        {
            "location": "/python/#instalando-python",
            "text": "",
            "title": "Instalando Python"
        },
        {
            "location": "/python/#en-linux",
            "text": "Instalar  Python  en  Linux  no es necesario, ya que viene preinstalado en todas las distribuciones m\u00e1s populares.",
            "title": "En Linux"
        },
        {
            "location": "/python/#en-windows",
            "text": "La forma m\u00e1s sencilla de poder instalar  Python  en Windows es instalando alguna de las distribuciones de  Python  que ya vienen armadas con los principales m\u00f3dulos. Yo les recomiendo la distribuci\u00f3n  Anaconda , que se puede descargar en forma gratuita y viene integrada con todos los principales paquetes que vamos a necesitar para trabajar con  Python . Una vez que la descargan, simplemente siguen los pasos del instalador y listo, ya tendr\u00e1n todo un ambiente  Python  para trabajar en Windows.  Otra distribuci\u00f3n de  Python  que pueden utilizar en Windows, es  WinPython , la cual puede ser utilizada incluso en forma portable.",
            "title": "En Windows"
        },
        {
            "location": "/python/#librerias-esenciales-para-el-analisis-de-datos",
            "text": "",
            "title": "Librer\u00edas esenciales para el analisis de datos"
        },
        {
            "location": "/python/#numpy",
            "text": "Numpy , abreviatura de Numerical  Python  , es el paquete fundamental para la computaci\u00f3n cient\u00edfica en  Python . Dispone, entre otras cosas de:   Un objeto  matriz  multidimensional  ndarray ,r\u00e1pido y eficiente.  Funciones para realizar c\u00e1lculos elemento a elemento u otras operaciones matem\u00e1ticas con  matrices .   Herramientas para la lectura y escritura de los conjuntos de datos basados  matrices .  Operaciones de  \u00e1lgebra lineal ,  transformaciones de Fourier , y generaci\u00f3n de n\u00fameros aleatorios.  Herramientas de integraci\u00f3n para conectar  C ,  C++  y  Fortran  con  Python   M\u00e1s all\u00e1 de las capacidades de procesamiento r\u00e1pido de  matrices  que  Numpy  a\u00f1ade a  Python , uno de sus\nprop\u00f3sitos principales con respecto al an\u00e1lisis de datos es la utilizaci\u00f3n de sus  estructuras de datos  como contenedores para transmitir los datos entre diferentes algoritmos. Para datos num\u00e9ricos , las  matrices  de  Numpy  son una forma mucho m\u00e1s eficiente de almacenar y manipular datos que cualquier otra de las  estructuras de datos  est\u00e1ndar incorporadas en  Python . Asimismo, librer\u00edas escritas en un lenguaje de bajo nivel, como  C  o  Fortran , pueden operar en los datos almacenados en  matrices  de  Numpy  sin necesidad de copiar o modificar ning\u00fan dato.",
            "title": "Numpy"
        },
        {
            "location": "/python/#jupyter",
            "text": "Jupyter  promueve un ambiente de trabajo de  ejecutar-explorar  en contraposici\u00f3n al tradicional modelo de desarrollo de software de  editar-compilar-ejecutar . Es decir, que el problema computacional a resolver es m\u00e1s visto como todo un proceso de ejecucion de tareas, en lugar del tradicional modelo de producir una respuesta( output ) a una pregunta( input ).   Jupyter  tambi\u00e9n provee una estrecha integraci\u00f3n con nuestro sistema operativo, permitiendo acceder f\u00e1cilmente a todos nuestros archivos desde la misma herramienta.  Algunas de las caracter\u00edsticas sobresalientes de  Jupyter  son:   Su poderoso  shell  interactivo con soporte para m\u00faltiples lenguajes.  Notebook , su interfase web con soporte para c\u00f3digo, texto, expresiones matem\u00e1ticas, gr\u00e1ficos en l\u00ednea y multimedia.  Su soporte para poder realizar visualizaciones de datos en forma interactiva.  Jupyter  esta totalmente integrado con  matplotlib .  Su simple y flexible interfase para trabajar con la  computaci\u00f3n paralela .",
            "title": "Jupyter"
        },
        {
            "location": "/python/#matplotlib",
            "text": "Matplotlib  es la librer\u00eda m\u00e1s popular en  Python  para visualizaciones y gr\u00e1ficos.  Matplotlib  puede producir gr\u00e1ficos de alta calidad dignos de cualquier publicaci\u00f3n cient\u00edfica.  Algunas de las muchas ventajas que nos ofrece  Matplotlib , incluyen:   Es f\u00e1cil de aprender.  Soporta texto, t\u00edtulos y etiquetas en formato $\\LaTeX$.  Proporciona un gran control sobre cada uno de los elementos de las figuras, como ser su tama\u00f1o, el trazado de sus l\u00edneas, etc.  Nos permite crear gr\u00e1ficos y figuras de gran calidad que pueden ser guardados en varios formatos, como ser: PNG, PDF, SVG, EPS, y PGF.   Matplotlib  se integra de maravilla con  Jupyter  (ver m\u00e1s abajo), lo que nos proporciona un ambiente confortable para las visualizaciones y la exploraci\u00f3n de datos interactiva.",
            "title": "Matplotlib"
        },
        {
            "location": "/python/#pandas",
            "text": "Pandas  es una librer\u00eda  open source  que aporta a  Python  unas estructuras de datos f\u00e1ciles de user y de alta performance, junto con un gran n\u00famero de funciones esenciales para el an\u00e1lisis de datos. Con la ayuda de  Pandas  podemos trabajar con  datos estructurados  de una forma m\u00e1s r\u00e1pida y expresiva.  Algunas de las cosas sobresalientes que nos aporta  Pandas  son:   Un r\u00e1pido y eficiente objeto  DataFrame  para manipular datos con indexaci\u00f3n integrada;  herramientas para la  lectura y escritura de datos  entre estructuras de datos r\u00e1pidas y eficientes manejadas en memoria, como el  DataFrame , con la mayor\u00eda de los formatos conocidos para el manejo de datos, como ser: CSV y archivos de texto, archivos Microsoft Excel, bases de datos  SQL , y el formato cient\u00edfico HDF5.  Proporciona una  alineaci\u00f3n inteligente de datos  y un manejo integrado de los datos faltantes; con estas funciones podemos obtener una ganancia de performace en los c\u00e1lculos entre  DataFrames  y una f\u00e1cil manipulaci\u00f3n y ordenamiento de los datos de nuestro  data set ;  Flexibilidad para  manipular y redimensionar  nuestro  data set , facilidad para construir  tablas pivote ;  La posibilidad de  filtrar los datos, agregar o eliminar columnas  de una forma sumamente expresiva;  Operaciones de  merge  y *join * altamente eficientes sobre nuestros conjuntos de datos;  Indexaci\u00f3n jer\u00e1rquica  que proporciona una forma intuitiva de trabajar con datos de alta dimensi\u00f3n en una estructura de datos de menor dimensi\u00f3n ;  Posibilidad de realizar c\u00e1lculos agregados o transformaciones de datos con el poderoso motor  group by  que nos permite dividir-aplicar-combinar nuestros conjuntos de datos;  combina las  caracter\u00edsticas de las matrices de alto rendimiento de  Numpy  con las flexibles capacidades de manipulaci\u00f3n de datos de las hojas de c\u00e1lculo  y bases de datos relacionales (tales como  SQL );  Gran n\u00famero de funcionalidades para el manejo de  series de tiempo  ideales para el an\u00e1lisis financiero;  Todas sus funciones y estructuras de datos est\u00e1n  optimizadas para el alto rendimiento , con las partes cr\u00edticas del c\u00f3digo escritas en  Cython  o  C .",
            "title": "Pandas"
        },
        {
            "location": "/python/#scikit-lean",
            "text": "Scikit-learn  es una librer\u00eda especializada en algoritmos para  data mining  y  machine learning .    Algunos de los problemas que podemos resolver utilizando las herramientas de  Scikit-learn , son:   Clasificaciones : Identificar las categor\u00edas a que cada observaci\u00f3n del conjunto de datos pertenece.  Regresiones : Predecire el valor continuo para cada nuevo ejemplo.  Agrupaciones : Agrupaci\u00f3n autom\u00e1tica de objetos similares en un conjunto.  Reducci\u00f3n de dimensiones : Reducir el n\u00famero de variables aleatorias a considerar.  Selecci\u00f3n de Modelos : Comparar, validar y elegir par\u00e1metros y modelos.  Preprocesamiento : Extracci\u00f3n de caracter\u00edsticas a analizar y normalizaci\u00f3n de datos.",
            "title": "Scikit-Lean"
        },
        {
            "location": "/python/#scipy",
            "text": "SciPy  es un conjunto de paquetes donde cada uno ellos ataca un problema distinto dentro de la computaci\u00f3n cient\u00edfica y el an\u00e1lisis num\u00e9rico. Algunos de los paquetes que incluye, son:   scipy.integrate : que proporciona diferentes funciones para resolver problemas de integraci\u00f3n num\u00e9rica.  scipy.linalg : que proporciona funciones para resolver problemas de \u00e1lgebra lineal.  scipy.optimize : para los problemas de optimizaci\u00f3n y minimizaci\u00f3n.  scipy.signal : para el an\u00e1lisis y procesamiento de se\u00f1ales.  scipy.sparse : para matrices dispersas y solucionar sistemas lineales dispersos  scipy.stats : para el an\u00e1lisis de estad\u00edstica y probabilidades.",
            "title": "SciPy"
        },
        {
            "location": "/python/#frameworks-para-deep-learning",
            "text": "En estos momentos, si hay un campo en donde  Python  sobresale sobre cualquier otro lenguaje, es en su soporte para frameworks de  Deep Learning . Existen una gran variedad y de muy buena calidad, entre los que se destacan:    TensorFlow :  TensorFlow  es un frameworks desarrollado por Google. Es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos.     PyTorch :  PyTorch  es un framework de  Deep Learning  que utiliza el lenguaje  Python  y cuenta con el apoyo de Facebook.    Theano :  Theano  es una librer\u00eda de  Python  que permite definir, optimizar y evaluar expresiones matem\u00e1ticas que involucran tensores de manera eficiente.     CNTK :  CNTK  es un conjunto de herramientas, desarrolladas por Microsoft, f\u00e1ciles de usar, de c\u00f3digo abierto que entrena algoritmos de  Deep Learning  para aprender como el cerebro humano.    Keras :  Keras  es una librer\u00eda de alto nivel, muy f\u00e1cil de utilizar. Est\u00e1 escrita y mantenida por Francis Chollet, miembro del equipo de Google Brain. Permite a los usuarios elegir si los modelos que se construyen seran ejecutados en el grafo simb\u00f3lico de  Theano ,  TensorFlow  o  CNTK .    MXNet :  MXNet  es una librer\u00eda flexible y eficiente para armar modelos de  Deep Learning  con soporte para varios idiomas.",
            "title": "Frameworks para Deep Learning"
        },
        {
            "location": "/rlang/",
            "text": "Introducci\u00f3n a R\n\n\n\u00bfQu\u00e9 es R?\n\n\nR\n es un lenguaje de programaci\u00f3n \ninterpretado\n dise\u00f1ado espec\u00edficamente para el \nan\u00e1lisis estad\u00edstico\n y la manipulaci\u00f3n de datos. Esta inspirado, y es en su mayor medida compatible, por el lenguaje de programaci\u00f3n \nS\n desarrollado por AT&T. Es ampliamente utilizado en todos los campos donde se deben manipular datos, como ser: los negocios, la industria, el gobierno, la medicina, el \u00e1mbito acad\u00e9mico, y dem\u00e1s.\n\n\n\u00bfPor qu\u00e9 utilizar R?\n\n\nR\n cuenta con varias virtudes, como ser:\n\n\n\n\n\n\nEs una implementaci\u00f3n de dominio p\u00fablico del lenguaje estad\u00edstico \nS\n; y la plataforma R/S se ha convertido en el defecto dentro del c\u00edrculo de los profesionales de la \nestad\u00edstica\n.\n\n\n\n\n\n\nEs comparable, y a menudo superior en funcionalidad a productos comerciales; ya sea en gr\u00e1ficas, variedad de operaciones, y algoritmos implementados.\n\n\n\n\n\n\nEs multiplataforma, se encuentra disponible para los sistemas operativos Windows, Mac y Linux.\n\n\n\n\n\n\nAdem\u00e1s de proporcionar operaciones estad\u00edsticas, \nR\n es un lenguaje de programaci\u00f3n de prop\u00f3sito general; es decir, que puede ser utilizado para automatizar an\u00e1lisis y crear nuevas funciones que ampl\u00eden las funcionalidades existentes.\n\n\n\n\n\n\nIncorpora caracter\u00edsticas encontradas en la programaci\u00f3n \norientados a objetos\n y \nfuncional\n.\n\n\n\n\n\n\nEl sistema guarda los conjuntos de datos entre sesiones, por lo que no es necesario volver a cargar los datos cada vez que ingresamos. Tambi\u00e9n guarda nuestro historial de comandos, lo que nos ahorra bastante tiempo y mejora la productividad.\n\n\n\n\n\n\nDebido a que \nR\n es un software de \nc\u00f3digo abierto\n, es f\u00e1cil obtener ayuda de la comunidad de usuarios. Adem\u00e1s, muchas nuevas funciones son aportadas por los usuarios, los cuales son prominentes estad\u00edsticos.\n\n\n\n\n\n\nSi bien existe una peque\u00f1a curva de aprendizaje, \u00e9sta es bastante m\u00ednima en comparaci\u00f3n con otros lenguajes y programas. Asimismo existe una enorme red de colaboradores que constantemente est\u00e1n creando nuevos paquetes que hacen que sea mucho m\u00e1s f\u00e1cil aplicar todo tipo de t\u00e9cnicas y funciones para manipular y analizar nuestros datos con la ayuda de \nR\n.\n\n\n\u00bfC\u00f3mo obtengo R?\n\n\nPara descargar \nR\n, deben dirigirse a \nCRAN\n, la red de archivos de \nR\n. \nCRAN\n se compone de un conjunto de servidores distribuidos en todo el mundo y se utiliza para distribuir \nR\n junto con sus paquetes.\n\n\nRStudio\n\n\nRStudio\n es un entorno de desarrollo integrado, o \nIDE\n, dise\u00f1ado espec\u00edficamente para la programaci\u00f3n con \nR\n. \nRStudio\n hace que \nR\n sea m\u00e1s f\u00e1cil de usar. Incluye un editor de c\u00f3digo, herramientas de depuraci\u00f3n y visualizaci\u00f3n. Si estas dando tus primeros pasos con \nR\n, \nRStudio\n hace la experiencia mucho m\u00e1s amigable.\n\n\nLibrer\u00edas para Ciencia de datos\n\n\nBien, luego de este paseo por las principales estructuras de datos que podemos encontrar en \nR\n, lleg\u00f3 el momento de adentrarnos en el fascinante mundo de la ciencia de datos. Algunas de las librer\u00edas que se han vuelto sumamente \u00fatiles para analizar y manipular datos con \nR\n, son las siguientes:\n\n\nTidyverse\n\n\nUna de las tareas m\u00e1s importantes en cualquier proceso de an\u00e1lisis de datos consiste en ordenarlos y darles una estructura. En general recibimos datos en crudo y debemos procesarlos para poder luego utilizarlos en nuestros modelos. Si de explorar, ordenar y analizar datos se trata el paquete \ntidiverse\n es fundamental. Este paquete incluye las librer\u00edas ggplot2, tibble, tidyr, readr, purrr, y dplyr; las cuales comparten una filosof\u00eda propia y est\u00e1n dise\u00f1ados para trabajar naturalmente entre ellos.\n\n\nCaret\n\n\nA la hora de simplificar el proceso de \nMachine Learning\n, el paquete \nCaret\n puede sernos de gran ayuda. Este paquete nos ofrece una serie de herramientas para la construcci\u00f3n de modelos de \nMachine Learning\n en \nR\n. \nCaret\n nos proporciona herramientas esenciales para: la etapa de preparaci\u00f3n de los datos, para dividir el conjunto de datos, seleccionar los principales atributos, y para evaluar los modelos.\n\n\nData.table\n\n\nSi de organizar grandes vol\u00famenes de datos de una manera intuitiva se trata, \ndata.table\n es el paquete indicado. Esta librer\u00eda nos extiende la estructura de datos del dataFrame para poder trabajar con archivos realmente extensos, y poder realizar operaciones de agregado, agrupado y uniones de una forma m\u00e1s sencilla.\n\n\nOtras librer\u00edas que deber\u00edamos conocer\n\n\nE1071\n\n\nSi lo que buscamos es trabajar con \nM\u00e1quinas de vectores de soporte, SVM\n; o con cualquiera de las principales funciones que podemos encontrar en las clases de probabilidad y estad\u00edstica; entonces el paquete \nE1071\n es exactamente lo que necesitamos. \n\n\nrandomForest\n\n\nPara trabajar espec\u00edficamente con modelos de \nRandom Forest\n el paquete \nrandomForest\n puede ser una buena opci\u00f3n; este paquete nos permite crear este tipo de modelos en forma muy sencilla.\n\n\nrpart\n\n\nEl paquete \nrpart\n es una buena alternativa para trabajar con \u00e1rboles de clasificaciones. Implementa los principales algoritmos para trabajar con este tipo de modelos.\n\n\nnnet\n\n\nLas \nredes neuronales\n han recibido mucha atenci\u00f3n \u00faltimamente por sus habilidades para \naprender\n las relaciones entre las variables. Representan una t\u00e9cnica innovadora para la adaptaci\u00f3n de los modelo que no se basa en los supuestos convencionales necesarios por el modelado est\u00e1ndar; y que adem\u00e1s pueden manejar muy eficazmente los datos multivariantes. Un gran paquete para trabajar con \nredes neuronales\n en forma muy sencilla es \nnnet\n.\n\n\nigraph\n\n\nSi lo que necesitamos es analizar y visualizar redes y grafos, el paquete \nigraph\n es la mejor opci\u00f3n. Este paquete nos proporciona una serie de rutinas altamente eficientes para visualizar y analizar las conexiones de las redes. \n\n\nOutliers\n\n\nSi lo que necesitamos es encontrar valores at\u00edpicos, entonces \noutliers\n es el paquete que debemos utilizar. Esta librer\u00eda nos ofrece varias funciones y tests para poder identificar los valores at\u00edpicos.   \n\n\nSurvival\n\n\nSurvival\n es un paquete que nos facilita la tarea de realizar \nan\u00e1lisis de supervivencia\n. \n\n\nForecast\n\n\nForecast\n proporciona m\u00e9todos y herramientas para mostrar y analizar predicciones univariadas de \nseries de tiempo\n, incluyendo el suavizado exponencial a trav\u00e9s de modelos de espacios de estados y el modelado \nARIMA\n autom\u00e1tico.",
            "title": "Intro R"
        },
        {
            "location": "/rlang/#introduccion-a-r",
            "text": "",
            "title": "Introducci\u00f3n a R"
        },
        {
            "location": "/rlang/#que-es-r",
            "text": "R  es un lenguaje de programaci\u00f3n  interpretado  dise\u00f1ado espec\u00edficamente para el  an\u00e1lisis estad\u00edstico  y la manipulaci\u00f3n de datos. Esta inspirado, y es en su mayor medida compatible, por el lenguaje de programaci\u00f3n  S  desarrollado por AT&T. Es ampliamente utilizado en todos los campos donde se deben manipular datos, como ser: los negocios, la industria, el gobierno, la medicina, el \u00e1mbito acad\u00e9mico, y dem\u00e1s.",
            "title": "\u00bfQu\u00e9 es R?"
        },
        {
            "location": "/rlang/#por-que-utilizar-r",
            "text": "R  cuenta con varias virtudes, como ser:    Es una implementaci\u00f3n de dominio p\u00fablico del lenguaje estad\u00edstico  S ; y la plataforma R/S se ha convertido en el defecto dentro del c\u00edrculo de los profesionales de la  estad\u00edstica .    Es comparable, y a menudo superior en funcionalidad a productos comerciales; ya sea en gr\u00e1ficas, variedad de operaciones, y algoritmos implementados.    Es multiplataforma, se encuentra disponible para los sistemas operativos Windows, Mac y Linux.    Adem\u00e1s de proporcionar operaciones estad\u00edsticas,  R  es un lenguaje de programaci\u00f3n de prop\u00f3sito general; es decir, que puede ser utilizado para automatizar an\u00e1lisis y crear nuevas funciones que ampl\u00eden las funcionalidades existentes.    Incorpora caracter\u00edsticas encontradas en la programaci\u00f3n  orientados a objetos  y  funcional .    El sistema guarda los conjuntos de datos entre sesiones, por lo que no es necesario volver a cargar los datos cada vez que ingresamos. Tambi\u00e9n guarda nuestro historial de comandos, lo que nos ahorra bastante tiempo y mejora la productividad.    Debido a que  R  es un software de  c\u00f3digo abierto , es f\u00e1cil obtener ayuda de la comunidad de usuarios. Adem\u00e1s, muchas nuevas funciones son aportadas por los usuarios, los cuales son prominentes estad\u00edsticos.    Si bien existe una peque\u00f1a curva de aprendizaje, \u00e9sta es bastante m\u00ednima en comparaci\u00f3n con otros lenguajes y programas. Asimismo existe una enorme red de colaboradores que constantemente est\u00e1n creando nuevos paquetes que hacen que sea mucho m\u00e1s f\u00e1cil aplicar todo tipo de t\u00e9cnicas y funciones para manipular y analizar nuestros datos con la ayuda de  R .",
            "title": "\u00bfPor qu\u00e9 utilizar R?"
        },
        {
            "location": "/rlang/#como-obtengo-r",
            "text": "Para descargar  R , deben dirigirse a  CRAN , la red de archivos de  R .  CRAN  se compone de un conjunto de servidores distribuidos en todo el mundo y se utiliza para distribuir  R  junto con sus paquetes.",
            "title": "\u00bfC\u00f3mo obtengo R?"
        },
        {
            "location": "/rlang/#rstudio",
            "text": "RStudio  es un entorno de desarrollo integrado, o  IDE , dise\u00f1ado espec\u00edficamente para la programaci\u00f3n con  R .  RStudio  hace que  R  sea m\u00e1s f\u00e1cil de usar. Incluye un editor de c\u00f3digo, herramientas de depuraci\u00f3n y visualizaci\u00f3n. Si estas dando tus primeros pasos con  R ,  RStudio  hace la experiencia mucho m\u00e1s amigable.",
            "title": "RStudio"
        },
        {
            "location": "/rlang/#librerias-para-ciencia-de-datos",
            "text": "Bien, luego de este paseo por las principales estructuras de datos que podemos encontrar en  R , lleg\u00f3 el momento de adentrarnos en el fascinante mundo de la ciencia de datos. Algunas de las librer\u00edas que se han vuelto sumamente \u00fatiles para analizar y manipular datos con  R , son las siguientes:",
            "title": "Librer\u00edas para Ciencia de datos"
        },
        {
            "location": "/rlang/#tidyverse",
            "text": "Una de las tareas m\u00e1s importantes en cualquier proceso de an\u00e1lisis de datos consiste en ordenarlos y darles una estructura. En general recibimos datos en crudo y debemos procesarlos para poder luego utilizarlos en nuestros modelos. Si de explorar, ordenar y analizar datos se trata el paquete  tidiverse  es fundamental. Este paquete incluye las librer\u00edas ggplot2, tibble, tidyr, readr, purrr, y dplyr; las cuales comparten una filosof\u00eda propia y est\u00e1n dise\u00f1ados para trabajar naturalmente entre ellos.",
            "title": "Tidyverse"
        },
        {
            "location": "/rlang/#caret",
            "text": "A la hora de simplificar el proceso de  Machine Learning , el paquete  Caret  puede sernos de gran ayuda. Este paquete nos ofrece una serie de herramientas para la construcci\u00f3n de modelos de  Machine Learning  en  R .  Caret  nos proporciona herramientas esenciales para: la etapa de preparaci\u00f3n de los datos, para dividir el conjunto de datos, seleccionar los principales atributos, y para evaluar los modelos.",
            "title": "Caret"
        },
        {
            "location": "/rlang/#datatable",
            "text": "Si de organizar grandes vol\u00famenes de datos de una manera intuitiva se trata,  data.table  es el paquete indicado. Esta librer\u00eda nos extiende la estructura de datos del dataFrame para poder trabajar con archivos realmente extensos, y poder realizar operaciones de agregado, agrupado y uniones de una forma m\u00e1s sencilla.",
            "title": "Data.table"
        },
        {
            "location": "/rlang/#otras-librerias-que-deberiamos-conocer",
            "text": "",
            "title": "Otras librer\u00edas que deber\u00edamos conocer"
        },
        {
            "location": "/rlang/#e1071",
            "text": "Si lo que buscamos es trabajar con  M\u00e1quinas de vectores de soporte, SVM ; o con cualquiera de las principales funciones que podemos encontrar en las clases de probabilidad y estad\u00edstica; entonces el paquete  E1071  es exactamente lo que necesitamos.",
            "title": "E1071"
        },
        {
            "location": "/rlang/#randomforest",
            "text": "Para trabajar espec\u00edficamente con modelos de  Random Forest  el paquete  randomForest  puede ser una buena opci\u00f3n; este paquete nos permite crear este tipo de modelos en forma muy sencilla.",
            "title": "randomForest"
        },
        {
            "location": "/rlang/#rpart",
            "text": "El paquete  rpart  es una buena alternativa para trabajar con \u00e1rboles de clasificaciones. Implementa los principales algoritmos para trabajar con este tipo de modelos.",
            "title": "rpart"
        },
        {
            "location": "/rlang/#nnet",
            "text": "Las  redes neuronales  han recibido mucha atenci\u00f3n \u00faltimamente por sus habilidades para  aprender  las relaciones entre las variables. Representan una t\u00e9cnica innovadora para la adaptaci\u00f3n de los modelo que no se basa en los supuestos convencionales necesarios por el modelado est\u00e1ndar; y que adem\u00e1s pueden manejar muy eficazmente los datos multivariantes. Un gran paquete para trabajar con  redes neuronales  en forma muy sencilla es  nnet .",
            "title": "nnet"
        },
        {
            "location": "/rlang/#igraph",
            "text": "Si lo que necesitamos es analizar y visualizar redes y grafos, el paquete  igraph  es la mejor opci\u00f3n. Este paquete nos proporciona una serie de rutinas altamente eficientes para visualizar y analizar las conexiones de las redes.",
            "title": "igraph"
        },
        {
            "location": "/rlang/#outliers",
            "text": "Si lo que necesitamos es encontrar valores at\u00edpicos, entonces  outliers  es el paquete que debemos utilizar. Esta librer\u00eda nos ofrece varias funciones y tests para poder identificar los valores at\u00edpicos.",
            "title": "Outliers"
        },
        {
            "location": "/rlang/#survival",
            "text": "Survival  es un paquete que nos facilita la tarea de realizar  an\u00e1lisis de supervivencia .",
            "title": "Survival"
        },
        {
            "location": "/rlang/#forecast",
            "text": "Forecast  proporciona m\u00e9todos y herramientas para mostrar y analizar predicciones univariadas de  series de tiempo , incluyendo el suavizado exponencial a trav\u00e9s de modelos de espacios de estados y el modelado  ARIMA  autom\u00e1tico.",
            "title": "Forecast"
        },
        {
            "location": "/about/",
            "text": "IAAR\n es la comunidad argentina de inteligencia artificial\n\n\nAgrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo \u00e9tico y humanitario de las tecnolog\u00edas cognitivas.\n\n\nObjetivos\n\n\n\n\nImpulsar el desarrollo y apoyar emprendimientos escalables, contribuyendo a la industria argentina en general, y a la Industria del Conocimiento argentina en particular.\n\n\nEntrenar y formar profesionales en Inteligencia Artificial y Ciencia de Datos que el pa\u00eds necesita y necesitar\u00e1 en un mundo cada vez m\u00e1s competitivo.\n\n\nCrear e incubar nuevos grupos interdisciplinarios de investigaci\u00f3n que exporten producci\u00f3n cient\u00edfica hacia el mundo.\n\n\nDivulgar el concepto, la importancia y los impactos tecnol\u00f3gicos, sociales y econ\u00f3micos de la Inteligencia Artificial al p\u00fablico argentino en general, ayudando a periodistas y difusores a lograr \u00e9ste cometido sin sensacionalismos.\n\n\n\n\nValores\n\n\n\n\nInclusi\u00f3n:\n Tecnolog\u00eda al servicio de la comunidad en su conjunto.\n\n\nDesarrollo:\n Industria argentina para el mundo.\n\n\nEmpat\u00eda:\n La vision de todos los afectados es tenida en cuenta.\n\n\nComunicaci\u00f3n:\n Abierta, honesta, directa y efectiva.\n\n\nTransparencia:\n Todas las operaciones estan abiertas a la comunidad.\n\n\n\n\nSumarse a la comunidad\n\n\nLos invitamos a sumarse a comunidad, nos pueden encontrar en los grupos de facebook \nIAAR\n, \nIAAR capacitaciones\n, \nIAAR debates\n y \nIAAR proyectos\n; o visitando el \nblog de IAAR\n. Tambi\u00e9n les recomendamos estar atentos a nuestros \nmeetups\n.",
            "title": "Sobre IAAR"
        },
        {
            "location": "/about/#iaar-es-la-comunidad-argentina-de-inteligencia-artificial",
            "text": "Agrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo \u00e9tico y humanitario de las tecnolog\u00edas cognitivas.",
            "title": "IAAR es la comunidad argentina de inteligencia artificial"
        },
        {
            "location": "/about/#objetivos",
            "text": "Impulsar el desarrollo y apoyar emprendimientos escalables, contribuyendo a la industria argentina en general, y a la Industria del Conocimiento argentina en particular.  Entrenar y formar profesionales en Inteligencia Artificial y Ciencia de Datos que el pa\u00eds necesita y necesitar\u00e1 en un mundo cada vez m\u00e1s competitivo.  Crear e incubar nuevos grupos interdisciplinarios de investigaci\u00f3n que exporten producci\u00f3n cient\u00edfica hacia el mundo.  Divulgar el concepto, la importancia y los impactos tecnol\u00f3gicos, sociales y econ\u00f3micos de la Inteligencia Artificial al p\u00fablico argentino en general, ayudando a periodistas y difusores a lograr \u00e9ste cometido sin sensacionalismos.",
            "title": "Objetivos"
        },
        {
            "location": "/about/#valores",
            "text": "Inclusi\u00f3n:  Tecnolog\u00eda al servicio de la comunidad en su conjunto.  Desarrollo:  Industria argentina para el mundo.  Empat\u00eda:  La vision de todos los afectados es tenida en cuenta.  Comunicaci\u00f3n:  Abierta, honesta, directa y efectiva.  Transparencia:  Todas las operaciones estan abiertas a la comunidad.",
            "title": "Valores"
        },
        {
            "location": "/about/#sumarse-a-la-comunidad",
            "text": "Los invitamos a sumarse a comunidad, nos pueden encontrar en los grupos de facebook  IAAR ,  IAAR capacitaciones ,  IAAR debates  y  IAAR proyectos ; o visitando el  blog de IAAR . Tambi\u00e9n les recomendamos estar atentos a nuestros  meetups .",
            "title": "Sumarse a la comunidad"
        },
        {
            "location": "/autor/",
            "text": "Ra\u00fal E. L\u00f3pez Briega\n\n\n\n\nYo vivo\u2026\n\n\nEn la ciudad de Buenos Aires, \nArgentina\n. Mi pais cuenta con una gran variedad de bellezas naturales, que invito a todos a conocer; yo mismo aun no conozco ni la mitad de mi hermoso pa\u00eds, espero alg\u00fan d\u00eda tener la posibilidad de recorrerlo de punta a punta.\n\n\nYo soy\u2026\n\n\nContador Publico y Licenciado en Administraci\u00f3n. He estudiado mis dos carreras en la \nUniversidad Nacional de La Matanza\n. Muchos se preguntaran \u00bfqu\u00e9 hace un contador escribiendo sobre programaci\u00f3n?; la verdad que yo tampoco s\u00e9 la respuesta a esa pregunta!\n\n\nYo trabajo\u2026\n\n\nComo consultor, para \nTGV\n. Mi carrera profesional comenz\u00f3 en \nMolas y Asociados\n, un estudio contable; all\u00ed adquir\u00ed un conocimiento generalista de la contabilidad y los sistemas administrativos, pasando por la liquidaci\u00f3n de impuestos, la tenedur\u00eda de libros y la auditor\u00eda. Luego mi carrera continuo en \nIBM\n, all\u00ed me desempe\u00f1\u00e9 como analista de Revenue recognition; IBM me mostro lo que es trabajar para una gran multinacional y despert\u00f3 mi pasi\u00f3n por la tecnolog\u00eda. Posteriormente ingrese a \nGrupo ASSA\n, all\u00ed pude materializar la union entre mis conocimientos administrativos-contables y mi pasion por la teconolog\u00eda, trabajando en la consultoria del  ERP \nJD Edwards\n. Finalmente, despues de 5 a\u00f1os en Grupo Assa, ingres\u00e9 a \nTGV\n en busca de nuevos desaf\u00edos profesionales.\n\n\nPara los que quieran conocer mi perfil profesional en m\u00e1s profundidad pueden visitar mi perfil en \nLinkedIn\n.\n\n\nMis pasiones\u2026\n\n\nMis pasiones son:\n\n\n\n\nLa \nTecnolog\u00eda\n, me encanta estar al tanto de las \u00faltimas corrientes tecnologicas.\n\n\nLa \nLiteratura\n, me gusta mucho leer, leo de todo y variado, con una leve predileccion por las novelas de ciencia ficcion; mis novelas favoritas son: \n1984, de George Orwell\n y \nLa rebelion de Atlas, de Ayn Rand\n.\n\n\nEl \nAjedrez\n, el juego que me ense\u00f1o mi padre a los 6 a\u00f1os, lo he jugado desde entonces y se convirti\u00f3 en el juego m\u00e1s apasionante que jam\u00e1s haya jugado.\n\n\nRiver Plate\n, el club de futbol de mis amores, el m\u00e1s grande de la Argentina!!\n\n\n\n\nMi Mision\u2026\n\n\nContribuir al desarrollo de un mundo m\u00e1s inteligente y productivo a trav\u00e9s del uso de las tecnolog\u00edas de la informaci\u00f3n.\n\n\nMis Blogs\n\n\nPara aquellos que est\u00e9n interesados en las matem\u00e1ticas, el an\u00e1lisis de datos y las \u00faltimas novedades tecnol\u00f3gicas, los invito a que ingresen en mis blogs \nrelopezbriega.github.io\n y \nrelopezbriega.com.ar\n.\n\n\nIAAR\n\n\nTambi\u00e9n colaboro con la comunidad argentina de Inteligencia Artificial \nIAAR\n, con la cual desarrollamos este \nlibro online\n. Los invito a sumarse a los grupos de facebook \nIAAR\n, \nIAAR capacitaciones\n, \nIAAR debates\n y \nIAAR proyectos\n; o visitar el \nblog de IAAR\n.",
            "title": "Sobre el autor"
        },
        {
            "location": "/autor/#raul-e-lopez-briega",
            "text": "Yo vivo\u2026  En la ciudad de Buenos Aires,  Argentina . Mi pais cuenta con una gran variedad de bellezas naturales, que invito a todos a conocer; yo mismo aun no conozco ni la mitad de mi hermoso pa\u00eds, espero alg\u00fan d\u00eda tener la posibilidad de recorrerlo de punta a punta.  Yo soy\u2026  Contador Publico y Licenciado en Administraci\u00f3n. He estudiado mis dos carreras en la  Universidad Nacional de La Matanza . Muchos se preguntaran \u00bfqu\u00e9 hace un contador escribiendo sobre programaci\u00f3n?; la verdad que yo tampoco s\u00e9 la respuesta a esa pregunta!  Yo trabajo\u2026  Como consultor, para  TGV . Mi carrera profesional comenz\u00f3 en  Molas y Asociados , un estudio contable; all\u00ed adquir\u00ed un conocimiento generalista de la contabilidad y los sistemas administrativos, pasando por la liquidaci\u00f3n de impuestos, la tenedur\u00eda de libros y la auditor\u00eda. Luego mi carrera continuo en  IBM , all\u00ed me desempe\u00f1\u00e9 como analista de Revenue recognition; IBM me mostro lo que es trabajar para una gran multinacional y despert\u00f3 mi pasi\u00f3n por la tecnolog\u00eda. Posteriormente ingrese a  Grupo ASSA , all\u00ed pude materializar la union entre mis conocimientos administrativos-contables y mi pasion por la teconolog\u00eda, trabajando en la consultoria del  ERP  JD Edwards . Finalmente, despues de 5 a\u00f1os en Grupo Assa, ingres\u00e9 a  TGV  en busca de nuevos desaf\u00edos profesionales.  Para los que quieran conocer mi perfil profesional en m\u00e1s profundidad pueden visitar mi perfil en  LinkedIn .  Mis pasiones\u2026  Mis pasiones son:   La  Tecnolog\u00eda , me encanta estar al tanto de las \u00faltimas corrientes tecnologicas.  La  Literatura , me gusta mucho leer, leo de todo y variado, con una leve predileccion por las novelas de ciencia ficcion; mis novelas favoritas son:  1984, de George Orwell  y  La rebelion de Atlas, de Ayn Rand .  El  Ajedrez , el juego que me ense\u00f1o mi padre a los 6 a\u00f1os, lo he jugado desde entonces y se convirti\u00f3 en el juego m\u00e1s apasionante que jam\u00e1s haya jugado.  River Plate , el club de futbol de mis amores, el m\u00e1s grande de la Argentina!!   Mi Mision\u2026  Contribuir al desarrollo de un mundo m\u00e1s inteligente y productivo a trav\u00e9s del uso de las tecnolog\u00edas de la informaci\u00f3n.  Mis Blogs  Para aquellos que est\u00e9n interesados en las matem\u00e1ticas, el an\u00e1lisis de datos y las \u00faltimas novedades tecnol\u00f3gicas, los invito a que ingresen en mis blogs  relopezbriega.github.io  y  relopezbriega.com.ar .  IAAR  Tambi\u00e9n colaboro con la comunidad argentina de Inteligencia Artificial  IAAR , con la cual desarrollamos este  libro online . Los invito a sumarse a los grupos de facebook  IAAR ,  IAAR capacitaciones ,  IAAR debates  y  IAAR proyectos ; o visitar el  blog de IAAR .",
            "title": "Ra\u00fal E. L\u00f3pez Briega"
        },
        {
            "location": "/contact/",
            "text": "Contacto\n\n\nPonerse en contacto con nosotros es muy f\u00e1cil. Completa el siguiente formulario:\n\n\nTambi\u00e9n pueden anotarse en el siguiente \nformulario\n\n\n\n    \n\n        \nSu nombre\n\n        \n\n    \n\n    \n\n        \nSu email\n\n        \n\n    \n\n    \n\n        \nMensaje",
            "title": "Contacto"
        },
        {
            "location": "/contact/#contacto",
            "text": "Ponerse en contacto con nosotros es muy f\u00e1cil. Completa el siguiente formulario:  Tambi\u00e9n pueden anotarse en el siguiente  formulario  \n     \n         Su nombre \n         \n     \n     \n         Su email \n         \n     \n     \n         Mensaje",
            "title": "Contacto"
        }
    ]
}