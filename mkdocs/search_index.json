{
    "docs": [
        {
            "location": "/",
            "text": "Introducci\u00f3n a la Inteligencia Artificial\n\n\n\n\nIntroducci\u00f3n\n\n\nEl cerebro es el \u00f3rgano m\u00e1s incre\u00edble del cuerpo humano. Establece la forma en que percibimos las im\u00e1genes, el sonido, los olores, los sabores y el tacto. Nos permite almacenar recuerdos, experimentar emociones e incluso so\u00f1ar. Sin el, ser\u00edamos organismos primitivos, incapaces de otra cosa que el m\u00e1s simple de los reflejos. El cerebro es, en definitiva, lo que nos hace inteligentes.\nDurante d\u00e9cadas hemos so\u00f1ado con construir m\u00e1quinas inteligentes con cerebros como los nuestros; asistentes robotizados para limpiar nuestras casas, coches que se conducen por s\u00ed mismos, microscopios que detecten enfermedades autom\u00e1ticamente. Pero construir estas m\u00e1quinas \nartificialmente inteligentes\n nos obliga a resolver algunos de los problemas computacionales m\u00e1s complejos que hemos tenido; problemas que nuestros cerebros ya pueden resolver en una fracci\u00f3n de segundos. La forma de atacar y resolver estos problemas, es el campo de estudio de la \nInteligencia Artificial\n.\n\n\n\u00bfQu\u00e9 es la Inteligencia Artificial?\n\n\nDefinir el concepto de \nInteligencia Artificial\n no es nada f\u00e1cil. Una definici\u00f3n sumamente general ser\u00eda que la \nIA\n es el estudio de la \ninfrom\u00e1tica\n centr\u00e1ndose en el desarrollo de software o \nm\u00e1quinas que exhiben una inteligencia humana\n. \n\n\nObjetivos de la Inteligencia Artificial\n\n\nLos objetivos principales de la \nIA\n incluyen la deducci\u00f3n y el razonamiento, la representaci\u00f3n del conocimiento, la planificaci\u00f3n, el procesamiento del lenguaje natural (\nNLP\n), el aprendizaje, la percepci\u00f3n y la capacidad de manipular y mover objetos. Los objetivos a largo plazo incluyen el logro de la Creatividad, la Inteligencia Social y la Inteligencia General (a nivel Humano).\n\n\nCuatro enfoques distintos\n\n\nPodemos distinguir cuatro enfoques distintos de abordar el problema de la \nInteligencia Artificial\n. \n\n\n\n\nSistemas que se comportan como humanos\n: Aqu\u00ed la idea es desarrollar m\u00e1quinas capaces de realizar funciones para las cuales se requerir\u00eda un humano inteligente. Dentro de este enfoque podemos encontrar la famosa \nPrueba de Turing\n. Para poder superar esta prueba, la m\u00e1quina deber\u00eda poseer las siguientes capacidades:\n\n\nProcesamiento de lenguaje natural\n, que le permita comunicarse satisfactoriamente. \n\n\nRepresentaci\u00f3n del conocimiento\n, para almacenar lo que se conoce o se siente.\n\n\nRazonamiento autom\u00e1tico\n, para utilizar la informaci\u00f3n almacenada para responder a preguntas y extraer nuevas conclusiones.\n\n\nAprendizaje autom\u00e1tico\n, para adaptarse a nuevas circunstancias y para detectar y extrapolar patrones.\n\n\nVisi\u00f3n computacional\n, para percibir objetos.\n\n\n\n\nRob\u00f3tica\n, para manipular y mover objetos.  \n\n\n\n\n\n\nSistemas que piensan como humanos\n: Aqu\u00ed la idea es hacer que las m\u00e1quinas piensen como humanos en el sentido m\u00e1s literal; es decir, que tengan capacidades cognitivas de toma de decisiones, resoluci\u00f3n de problemas, aprendizaje, etc. Dentro de este enfoque podemos encontrar al campo\ninterdisciplinario de la \nciencia cognitiva\n, en el cual convergen modelos computacionales de \nIA\n y\nt\u00e9cnicas experimentales de \npsicolog\u00eda\n intentando elaborar teor\u00edas precisas y verificables sobre el funcionamiento de la mente humana.\n\n\n\n\n\n\nSistemas que piensan racionalmente\n: Aqu\u00ed la idea es descubrir los c\u00e1lculos que hacen posible percibir, razonar y actuar; es decir, encontrar las \nleyes\n que rigen el pensamiento racional. Dentro de este enfoque podemos encontrar a la \nL\u00f3gica\n, que intenta expresar las \nleyes\n que gobiernan la manera de operar de la mente.\n\n\n\n\n\n\nSistemas que se comportan racionalmente\n: Aqu\u00ed la idea es dise\u00f1ar \nagentes\n inteligentes. Dentro de este enfoque un \nagente racional\n es aquel que act\u00faa con la intenci\u00f3n de alcanzar el mejor resultado o, cuando hay incertidumbre, el mejor resultado esperado. Un elemento importante a tener en cuenta es que tarde o temprano uno se dar\u00e1 cuenta de que obtener una racionalidad perfecta (hacer siempre lo correcto) no es del todo posible en entornos complejos. La demanda computacional que esto implica es demasiado grande, por lo que debemos conformarnos con una racionalidad limitada. Como lo que se busca en este enfoque es realizar inferencias correctas, se necesitan las mismas habilidades que para la \nPrueba de Turing\n, es decir, es necesario contar con la \ncapacidad para representar el conocimiento y razonar bas\u00e1ndonos en \u00e9l\n, porque ello permitir\u00e1 alcanzar decisiones correctas en una amplia gama de situaciones. Es necesario \nser capaz de generar sentencias comprensibles en lenguaje natural\n, ya que el enunciado de tales oraciones permite a los agentes desenvolverse en una sociedad compleja. El \naprendizaje\n no se lleva a cabo por erudici\u00f3n exclusivamente, sino que \nprofundizar en el conocimiento\n de c\u00f3mo funciona el mundo facilita la concepci\u00f3n de estrategias mejores para manejarse en \u00e9l.\n\n\n\n\n\n\nFundamentos de la Inteligencia artificial\n\n\nExisten varias disciplinas que han contribuido con ideas, puntos de vista y t\u00e9cnicas al desarrollo del campo de la \nInteligencia Artificial\n. Ellas son:\n\n\nFilosof\u00eda\n\n\nMuchas han sido las contribuciones de la \nFilosof\u00eda\n a las ciencias. En el campo de la \nInteligencia Artificial\n a contribuido con varios aportes entre los que se destacan los conceptos de \nIA d\u00e9bil\n y \nIA fuerte\n. \n\n\nLa \nIA d\u00e9bil\n se define como la inteligencia artificial racional que se centra t\u00edpicamente en una tarea estrecha. La inteligencia de la \nIA d\u00e9bil\n es limitada, no hay autoconciencia o inteligencia genuina. \nSiri\n es un buen ejemplo de una \nIA d\u00e9bil\n que combina varias t\u00e9cnicas de \nIA d\u00e9bil\n para funcionar. \nSiri\n puede hacer un mont\u00f3n de cosas por nosotros, pero a medida que intentamos tener conversaciones con el asistente virtual, nos damos cuenta de cuan limitada es.\n\n\nLa \nIA fuerte\n es aquella \ninteligencia artificial\n que iguala o excede la inteligencia humana promedio. Este tipo de \nAI\n  ser\u00e1 capaz de realizar todas las tareas que un ser humano podr\u00eda hacer. Hay mucha investigaci\u00f3n en este campo, pero todav\u00eda no han habido grandes avances.\n\n\nMuchos son los debates filos\u00f3ficos alrededor de la \ninteligencia artificial\n, para aquellos interesados en los aspectos filos\u00f3ficos les recomiendo inscribirse en nuestro \ngrupo de debate de IAAR\n\n\nMatem\u00e1ticas\n\n\nSi de ciencias aplicadas se trata, no puede faltar el aporte de las \nMatem\u00e1ticas\n. Para entender y desarrollar los principales \nalgoritmos\n que se utilizan en el campo de la \nInteligencia Artificial\n, deber\u00edamos tener nociones de: \n\n\n\u00c1lgebra lineal\n\n\nEl \n\u00e1lgebra lineal\n es una rama de las matem\u00e1ticas que estudia conceptos tales como vectores, matrices, tensores, sistemas de ecuaciones lineales y en su enfoque de manera m\u00e1s formal, espacios vectoriales y sus transformaciones lineales. Una buena comprensi\u00f3n del \n\u00e1lgebra lineal\n es esencial para entender y trabajar con muchos algoritmos de \nMachine Learning\n, y especialmente para los algoritmos de \nDeep Learning\n.\n\n\nC\u00e1lculo\n\n\nEl \nC\u00e1lculo\n es el campo de la matem\u00e1tica que incluye el estudio de los l\u00edmites, derivadas, integrales y series infinitas, y m\u00e1s concretamente se puede decir que es el estudio del \ncambio\n. Particularmente para el campo de la \nInteligencia Artificial\n algunos conceptos que se deber\u00edan conocer incluyen: \nC\u00e1lculo Diferencial e Integral\n, \nDerivadas Parciales\n, Funciones de Valores Vectoriales, y \nGradientes\n.\n\n\nOptimizaci\u00f3n matem\u00e1tica\n\n\nLa \nOptimizaci\u00f3n matem\u00e1tica\n es la herramienta matem\u00e1tica que nos permite optimizar decisiones, es decir, seleccionar la mejor alternativa de un conjunto de criterios disponibles. Su comprensi\u00f3n es fundamental para poder entender la eficiencia computacional y la escalabilidad de los principales algoritmos de \nMachine Learning\n y \nDeep Learning\n, los cuales suelen trabajar con matrices dispersas de gran tama\u00f1o.\n\n\nProbabilidad y estad\u00edstica\n\n\nLa \nProbabilidad y estad\u00edstica\n es la rama de la matem\u00e1tica que trata con la \nincertidumbre\n, la \naleatoriedad\n y la \ninferencia\n. Sus conceptos son fundamentales para cualquier algoritmo de \nMachine Learning\n o \nDeep Learning\n.\n\n\nUna buena introducci\u00f3n a cada uno de estos campos de las matem\u00e1ticas que son fundamentales para la \nInteligencia Artificial\n, la pueden encontrar en mi \nblog\n.\n\n\nLing\u00fc\u00edstica\n\n\nLa \nLing\u00fc\u00edstica\n moderna y la \nInteligencia Artificial\n nacieron al mismo tiempo y maduraron juntas, solap\u00e1ndose en un campo h\u00edbrido llamado ling\u00fc\u00edstica computacional o \nprocesamiento de lenguaje natural\n. El entendimiento del lenguaje requiere la comprensi\u00f3n de la materia bajo estudio y de su contexto, y no solamente el entendimiento de la estructura de las sentencias; lo que lo convierte en un problema bastante complejo de abordar.\n\n\nNeurociencias\n\n\nLa \nNeurociencia\n es el estudio del sistema neurol\u00f3gico, y en especial del cerebro. La forma exacta en la que en un cerebro se genera el pensamiento es uno de los grandes misterios de la ciencia. El hecho de que una colecci\u00f3n de simples c\u00e9lulas puede llegar a generar razonamiento, acci\u00f3n, y conciencia es un enigma a resolver. Cerebros y computadores realizan tareas bastante diferentes y tienen propiedades muy distintas. Seg\u00fan los c\u00e1lculos de los expertos se estima que para el 2020 las computadoras igualaran la capacidad de procesamiento de los cerebros. Muchos modelos de \nIA\n fueron inspirados en la estructura y el funcionamiento de nuestro cerebro.\n\n\nPsicolog\u00eda\n\n\nLa \nPsicolog\u00eda\n trata sobre el estudio y an\u00e1lisis de la conducta y los procesos mentales de los individuos y grupos humanos. La rama que m\u00e1s influencia ha tenido para la \nInteligencia Artificial\n es la de la \npsicolog\u00eda cognitiva\n que se encarga del estudio de la cognici\u00f3n; es decir, de los procesos mentales implicados en el conocimiento. Tiene como objeto de estudio los mecanismos b\u00e1sicos y profundos por los que se elabora el conocimiento, desde la percepci\u00f3n, la memoria y el aprendizaje, hasta la formaci\u00f3n de conceptos y razonamiento l\u00f3gico. Las teor\u00eda descritas por esta rama han sido utilizados para desarrollar varios modelos de \nInteligencia Artificial\n y \nMachine Learning\n\n\nRamas de la Inteligencia artificial\n\n\nDentro de la \nInteligencia Artificial\n podemos encontrar distintas ramas, entre las que se destacan:\n\n\nMachine Learning\n\n\nEl \nMachine Learning\n es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del \nMachine Learning\n es \ngeneralizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada\n. El ejemplo m\u00e1s t\u00edpico donde podemos ver el uso del Machine Learning es en el filtrado de los correo basura o spam. Mediante la observaci\u00f3n de miles de correos electr\u00f3nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos. \n\n\nEl \nMachine Learning\n tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito, an\u00e1lisis del mercado de valores, clasificaci\u00f3n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob\u00f3tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distingir los distintos \ntipos de problemas de \nMachine Learning\n con los que nos podemos encontrar.\n\n\nAprendizaje supervisado\n\n\nEn los problemas de \naprendizaje supervisado\n se ense\u00f1a o entrena al algoritmo a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos, el algoritmo podr\u00e1 generalizar en una forma m\u00e1s precisa. Una vez concluido el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el algoritmo de aprendizaje utiliza la experiencia pasada que adquiri\u00f3 durante la etapa de entrenamiento para predecir un resultado.\n\n\nAprendizaje no supervisado\n\n\nEn los problemas de \naprendizaje no supervisado\n, el algoritmo es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al algoritmo lo que representan los datos. La idea es que el algoritmo pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos. \n\n\nAprendizaje por refuerzo\n\n\nEn los problemas de \naprendizaje por refuerzo\n, el algoritmo aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m\u00e1s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar\u00e1 creando una estrategia ganadora.\n\n\nDeep Learning\n\n\nEl \nDeep Learning\n constituye un conjunto particular de algoritmos de \nMachine Learning\n que utilizan estructuras profundas de \nredes neuronales\n para encontrar patrones en los datos. Estos tipos de algoritmos cuentan actualmente con un gran inter\u00e9s, ya que han demostrado ser sumamente exitosos para resolver determinados tipos de problemas; como por ejemplo, el reconocimiento de im\u00e1genes. Muchos consideran que este tipo de modelos son los que en el futuro nos llevaran a resolver definitivamente el problema de la \nInteligencia Artificial\n.\n\n\nRazonamiento probabil\u00edstico\n\n\nEl \nrazonamiento probabil\u00edstico\n se encarga de lidiar con la incertidumbre inherente de todo proceso de aprendizaje. El problema para crear una \nInteligencia Artificial\n entonces se convierte en encontrar la forma de trabajar con informaci\u00f3n ruidosa, incompleta e incluso muchas veces contradictoria. Estos algoritmos est\u00e1n sumamente ligados a la \nestad\u00edstica bayesiana\n; y la principal herramienta en la que se apoyan es en el \nteorema de Bayes\n.\n\n\nAlgortimos gen\u00e9ticos\n\n\nLos \nalgoritmos gen\u00e9ticos\n se basan en la idea de que la madre de todo aprendizaje es la \nselecci\u00f3n natural\n. Si la Naturaleza pudo crearnos, puede crear cualquier cosa; por tal motivo lo \u00fanico que deber\u00edamos hacer para alcanzar una \nInteligencia Artificial\n es simular sus mecanismos en una computadora. La idea de estos algoritmos es imitar a la Evoluci\u00f3n; funcionan seleccionando individuos de una poblaci\u00f3n de soluciones candidatas, y luego intentando producir nuevas generaciones de soluciones mejores que las anteriores una y otra vez hasta aproximarse a una soluci\u00f3n perfecta.\n\n\nAplicaciones de la Inteligencia artificial\n\n\nLas t\u00e9cnicas de la \nInteligencia Artificial\n pueden ser aplicadas en una gran variedad de industrias y situaciones, como ser:\n\n\nMedicina\n\n\nApoy\u00e1ndose en las herramientas que proporciona la \nInteligencia Artificial\n, los doctores podr\u00edan realizar diagn\u00f3sticos m\u00e1s certeros y oportunos, lo que llevar\u00eda a mejores tratamientos y m\u00e1s vidas salvadas. \n\n\nAutos aut\u00f3nomos\n\n\nUtilizando \nInteligencia Artificial\n podr\u00edamos crear autos aut\u00f3nomos que aprendan de los datos y experiencias de millones de otros autos, mejorando el tr\u00e1fico y haciendo mucho m\u00e1s segura la conducci\u00f3n.\n\n\nBancos\n\n\nUtilizando t\u00e9cnicas de \nMachine Learning\n los bancos pueden detectar fraudes antes de que ocurran por medio de analizar los patrones de comportamiento de gastos e identificando r\u00e1pidamente actividades sospechosas. \n\n\nAgricultura\n\n\nEn Agricultura se podr\u00eda optimizar el rendimiento de los cultivos por medio de la utilizaci\u00f3n de las t\u00e9cnicas de \nInteligencia Artificial\n para analizar los datos del suelo y del clima en tiempo real, logrando producir m\u00e1s alimentos incluso con climas perjudiciales. \n\n\nEducaci\u00f3n\n\n\nEn la Educaci\u00f3n se podr\u00edan utilizar las t\u00e9cnicas de la \nInteligencia Artificial\n para dise\u00f1ar programas de estudios personalizados basados en datos que mejoren el rendimiento y el ritmo de aprendizaje de los alumnos.\n\n\nLa \u00e9tica y los riesgos de desarrollar una Inteligencia Artificial\n\n\nActualmente tambi\u00e9n ha surgido un debate \u00e9tico alrededor de la \nInteligencia Artificial\n. Algunos de los pensadores m\u00e1s importantes del planeta han establecido su preocupaci\u00f3n sobre el progreso de la \nIA\n. Entre los problemas que puede traer aparejado el desarrollo de la \nInteligencia Artificial\n, podemos encontrar los siguientes:\n\n\n\n\nLas personas podr\u00edan perder sus trabajos por la automatizaci\u00f3n.\n\n\nLas personas podr\u00edan tener demasiado (o muy poco) tiempo de ocio.\n\n\nLas personas podr\u00edan perder el sentido de ser \u00fanicos.\n\n\nLas personas podr\u00edan perder algunos de sus derechos privados.\n\n\nLa utilizaci\u00f3n de los sistemas de \nIA\n podr\u00eda llevar a la p\u00e9rdida de responsabilidad.\n\n\nEl \u00e9xito de la \nIA\n podr\u00eda significar el fin de la raza humana.\n\n\n\n\nEl debate sobre los beneficios y riesgos del desarrollo de la \nInteligencia Artificial\n est\u00e1 todav\u00eda abierto.\n\n\n\u00bfC\u00f3mo iniciarse en el campo de la Inteligencia artificial?\n\n\nSi luego de leer esta introducci\u00f3n, te has quedado fascinado por el campo de la \nInteligencia Artificial\n y quieres incursionar en el mismo, aqu\u00ed te dejo algunas recomendaciones para iniciarse.\n\n\nIAAR\n\n\nIAAR\n es la comunidad argentina de inteligencia artificial. Agrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo \u00e9tico y humanitario de las tecnolog\u00edas cognitivas. Para comenzar a formar parte de la comunidad pueden inscribirse en los grupos de facebook: \nIAAR\n, \nDebates\n, \nProyectos\n, \nCapacitaci\u00f3n\n; y/o en el \nmeetup\n.\n\n\nProgramaci\u00f3n\n\n\nPara poder trabajar en problemas relacionados al campo de la \nInteligencia Artificial\n es necesario saber programar. Los principales lenguajes que se utilizan son \nPython\n y \nR\n. En los repositorios de \nAcademia de IAAR\n van a poder encontrar material sobre estos lenguajes.\n\n\nFrameworks\n\n\nExisten varios frameworks open source que nos facilitan el trabajar con modelos de \nDeep Learning\n, entre los que se destacan:\n\n\n\n\nTensorFlow\n: \nTensorFlow\n es un frameworks desarrollado por Google. Es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos que utiliza el lenguaje \nPython\n. \n\n\nPyTorch\n: \nPyTorch\n es un framework de \nDeep Learning\n que utiliza el lenguaje \nPython\n y cuenta con el apoyo de Facebook.\n\n\nCaffe\n: \nCaffe\n es un framework de \nDeep Learning\n hecho con expresi\u00f3n, velocidad y modularidad en mente, el cual es desarrollado por la universidad de Berkeley.\n\n\nCNTK\n: \nCNTK\n es un conjunto de herramientas, desarrolladas por Microsoft, f\u00e1ciles de usar, de c\u00f3digo abierto que entrena algoritmos de \nDeep Learning\n para aprender como el cerebro humano.\n\n\nTheano\n: \nTheano\n es una librer\u00eda de \nPython\n que permite definir, optimizar y evaluar expresiones matem\u00e1ticas que involucran tensores de manera eficiente. \n\n\nDeepLearning4j\n: \nDeepLearning4j\n Es una librer\u00eda open source para trabajar con modelos de \nDeep Learning\n distribuidos utilizando el lenguaje \nJava\n.\n\n\n\n\nBots\n\n\nUna de las ramas con mayor crecimiento y que m\u00e1s se ha beneficiado con el boom de la \nInteligencia Artificial\n es la de los \nBots\n. Generar peque\u00f1os \nBots\n que puedan tener conversaciones b\u00e1sicas con los usuarios es bastante simple. Pueden encontrar una gu\u00eda con una gran n\u00famero de herramientas en el \nblog de IAAR\n.",
            "title": "Inteligencia Artificial"
        },
        {
            "location": "/#introduccion-a-la-inteligencia-artificial",
            "text": "",
            "title": "Introducci\u00f3n a la Inteligencia Artificial"
        },
        {
            "location": "/#introduccion",
            "text": "El cerebro es el \u00f3rgano m\u00e1s incre\u00edble del cuerpo humano. Establece la forma en que percibimos las im\u00e1genes, el sonido, los olores, los sabores y el tacto. Nos permite almacenar recuerdos, experimentar emociones e incluso so\u00f1ar. Sin el, ser\u00edamos organismos primitivos, incapaces de otra cosa que el m\u00e1s simple de los reflejos. El cerebro es, en definitiva, lo que nos hace inteligentes.\nDurante d\u00e9cadas hemos so\u00f1ado con construir m\u00e1quinas inteligentes con cerebros como los nuestros; asistentes robotizados para limpiar nuestras casas, coches que se conducen por s\u00ed mismos, microscopios que detecten enfermedades autom\u00e1ticamente. Pero construir estas m\u00e1quinas  artificialmente inteligentes  nos obliga a resolver algunos de los problemas computacionales m\u00e1s complejos que hemos tenido; problemas que nuestros cerebros ya pueden resolver en una fracci\u00f3n de segundos. La forma de atacar y resolver estos problemas, es el campo de estudio de la  Inteligencia Artificial .",
            "title": "Introducci\u00f3n"
        },
        {
            "location": "/#que-es-la-inteligencia-artificial",
            "text": "Definir el concepto de  Inteligencia Artificial  no es nada f\u00e1cil. Una definici\u00f3n sumamente general ser\u00eda que la  IA  es el estudio de la  infrom\u00e1tica  centr\u00e1ndose en el desarrollo de software o  m\u00e1quinas que exhiben una inteligencia humana .",
            "title": "\u00bfQu\u00e9 es la Inteligencia Artificial?"
        },
        {
            "location": "/#objetivos-de-la-inteligencia-artificial",
            "text": "Los objetivos principales de la  IA  incluyen la deducci\u00f3n y el razonamiento, la representaci\u00f3n del conocimiento, la planificaci\u00f3n, el procesamiento del lenguaje natural ( NLP ), el aprendizaje, la percepci\u00f3n y la capacidad de manipular y mover objetos. Los objetivos a largo plazo incluyen el logro de la Creatividad, la Inteligencia Social y la Inteligencia General (a nivel Humano).",
            "title": "Objetivos de la Inteligencia Artificial"
        },
        {
            "location": "/#cuatro-enfoques-distintos",
            "text": "Podemos distinguir cuatro enfoques distintos de abordar el problema de la  Inteligencia Artificial .    Sistemas que se comportan como humanos : Aqu\u00ed la idea es desarrollar m\u00e1quinas capaces de realizar funciones para las cuales se requerir\u00eda un humano inteligente. Dentro de este enfoque podemos encontrar la famosa  Prueba de Turing . Para poder superar esta prueba, la m\u00e1quina deber\u00eda poseer las siguientes capacidades:  Procesamiento de lenguaje natural , que le permita comunicarse satisfactoriamente.   Representaci\u00f3n del conocimiento , para almacenar lo que se conoce o se siente.  Razonamiento autom\u00e1tico , para utilizar la informaci\u00f3n almacenada para responder a preguntas y extraer nuevas conclusiones.  Aprendizaje autom\u00e1tico , para adaptarse a nuevas circunstancias y para detectar y extrapolar patrones.  Visi\u00f3n computacional , para percibir objetos.   Rob\u00f3tica , para manipular y mover objetos.      Sistemas que piensan como humanos : Aqu\u00ed la idea es hacer que las m\u00e1quinas piensen como humanos en el sentido m\u00e1s literal; es decir, que tengan capacidades cognitivas de toma de decisiones, resoluci\u00f3n de problemas, aprendizaje, etc. Dentro de este enfoque podemos encontrar al campo\ninterdisciplinario de la  ciencia cognitiva , en el cual convergen modelos computacionales de  IA  y\nt\u00e9cnicas experimentales de  psicolog\u00eda  intentando elaborar teor\u00edas precisas y verificables sobre el funcionamiento de la mente humana.    Sistemas que piensan racionalmente : Aqu\u00ed la idea es descubrir los c\u00e1lculos que hacen posible percibir, razonar y actuar; es decir, encontrar las  leyes  que rigen el pensamiento racional. Dentro de este enfoque podemos encontrar a la  L\u00f3gica , que intenta expresar las  leyes  que gobiernan la manera de operar de la mente.    Sistemas que se comportan racionalmente : Aqu\u00ed la idea es dise\u00f1ar  agentes  inteligentes. Dentro de este enfoque un  agente racional  es aquel que act\u00faa con la intenci\u00f3n de alcanzar el mejor resultado o, cuando hay incertidumbre, el mejor resultado esperado. Un elemento importante a tener en cuenta es que tarde o temprano uno se dar\u00e1 cuenta de que obtener una racionalidad perfecta (hacer siempre lo correcto) no es del todo posible en entornos complejos. La demanda computacional que esto implica es demasiado grande, por lo que debemos conformarnos con una racionalidad limitada. Como lo que se busca en este enfoque es realizar inferencias correctas, se necesitan las mismas habilidades que para la  Prueba de Turing , es decir, es necesario contar con la  capacidad para representar el conocimiento y razonar bas\u00e1ndonos en \u00e9l , porque ello permitir\u00e1 alcanzar decisiones correctas en una amplia gama de situaciones. Es necesario  ser capaz de generar sentencias comprensibles en lenguaje natural , ya que el enunciado de tales oraciones permite a los agentes desenvolverse en una sociedad compleja. El  aprendizaje  no se lleva a cabo por erudici\u00f3n exclusivamente, sino que  profundizar en el conocimiento  de c\u00f3mo funciona el mundo facilita la concepci\u00f3n de estrategias mejores para manejarse en \u00e9l.",
            "title": "Cuatro enfoques distintos"
        },
        {
            "location": "/#fundamentos-de-la-inteligencia-artificial",
            "text": "Existen varias disciplinas que han contribuido con ideas, puntos de vista y t\u00e9cnicas al desarrollo del campo de la  Inteligencia Artificial . Ellas son:",
            "title": "Fundamentos de la Inteligencia artificial"
        },
        {
            "location": "/#filosofia",
            "text": "Muchas han sido las contribuciones de la  Filosof\u00eda  a las ciencias. En el campo de la  Inteligencia Artificial  a contribuido con varios aportes entre los que se destacan los conceptos de  IA d\u00e9bil  y  IA fuerte .   La  IA d\u00e9bil  se define como la inteligencia artificial racional que se centra t\u00edpicamente en una tarea estrecha. La inteligencia de la  IA d\u00e9bil  es limitada, no hay autoconciencia o inteligencia genuina.  Siri  es un buen ejemplo de una  IA d\u00e9bil  que combina varias t\u00e9cnicas de  IA d\u00e9bil  para funcionar.  Siri  puede hacer un mont\u00f3n de cosas por nosotros, pero a medida que intentamos tener conversaciones con el asistente virtual, nos damos cuenta de cuan limitada es.  La  IA fuerte  es aquella  inteligencia artificial  que iguala o excede la inteligencia humana promedio. Este tipo de  AI   ser\u00e1 capaz de realizar todas las tareas que un ser humano podr\u00eda hacer. Hay mucha investigaci\u00f3n en este campo, pero todav\u00eda no han habido grandes avances.  Muchos son los debates filos\u00f3ficos alrededor de la  inteligencia artificial , para aquellos interesados en los aspectos filos\u00f3ficos les recomiendo inscribirse en nuestro  grupo de debate de IAAR",
            "title": "Filosof\u00eda"
        },
        {
            "location": "/#matematicas",
            "text": "Si de ciencias aplicadas se trata, no puede faltar el aporte de las  Matem\u00e1ticas . Para entender y desarrollar los principales  algoritmos  que se utilizan en el campo de la  Inteligencia Artificial , deber\u00edamos tener nociones de:",
            "title": "Matem\u00e1ticas"
        },
        {
            "location": "/#algebra-lineal",
            "text": "El  \u00e1lgebra lineal  es una rama de las matem\u00e1ticas que estudia conceptos tales como vectores, matrices, tensores, sistemas de ecuaciones lineales y en su enfoque de manera m\u00e1s formal, espacios vectoriales y sus transformaciones lineales. Una buena comprensi\u00f3n del  \u00e1lgebra lineal  es esencial para entender y trabajar con muchos algoritmos de  Machine Learning , y especialmente para los algoritmos de  Deep Learning .",
            "title": "\u00c1lgebra lineal"
        },
        {
            "location": "/#calculo",
            "text": "El  C\u00e1lculo  es el campo de la matem\u00e1tica que incluye el estudio de los l\u00edmites, derivadas, integrales y series infinitas, y m\u00e1s concretamente se puede decir que es el estudio del  cambio . Particularmente para el campo de la  Inteligencia Artificial  algunos conceptos que se deber\u00edan conocer incluyen:  C\u00e1lculo Diferencial e Integral ,  Derivadas Parciales , Funciones de Valores Vectoriales, y  Gradientes .",
            "title": "C\u00e1lculo"
        },
        {
            "location": "/#optimizacion-matematica",
            "text": "La  Optimizaci\u00f3n matem\u00e1tica  es la herramienta matem\u00e1tica que nos permite optimizar decisiones, es decir, seleccionar la mejor alternativa de un conjunto de criterios disponibles. Su comprensi\u00f3n es fundamental para poder entender la eficiencia computacional y la escalabilidad de los principales algoritmos de  Machine Learning  y  Deep Learning , los cuales suelen trabajar con matrices dispersas de gran tama\u00f1o.",
            "title": "Optimizaci\u00f3n matem\u00e1tica"
        },
        {
            "location": "/#probabilidad-y-estadistica",
            "text": "La  Probabilidad y estad\u00edstica  es la rama de la matem\u00e1tica que trata con la  incertidumbre , la  aleatoriedad  y la  inferencia . Sus conceptos son fundamentales para cualquier algoritmo de  Machine Learning  o  Deep Learning .  Una buena introducci\u00f3n a cada uno de estos campos de las matem\u00e1ticas que son fundamentales para la  Inteligencia Artificial , la pueden encontrar en mi  blog .",
            "title": "Probabilidad y estad\u00edstica"
        },
        {
            "location": "/#linguistica",
            "text": "La  Ling\u00fc\u00edstica  moderna y la  Inteligencia Artificial  nacieron al mismo tiempo y maduraron juntas, solap\u00e1ndose en un campo h\u00edbrido llamado ling\u00fc\u00edstica computacional o  procesamiento de lenguaje natural . El entendimiento del lenguaje requiere la comprensi\u00f3n de la materia bajo estudio y de su contexto, y no solamente el entendimiento de la estructura de las sentencias; lo que lo convierte en un problema bastante complejo de abordar.",
            "title": "Ling\u00fc\u00edstica"
        },
        {
            "location": "/#neurociencias",
            "text": "La  Neurociencia  es el estudio del sistema neurol\u00f3gico, y en especial del cerebro. La forma exacta en la que en un cerebro se genera el pensamiento es uno de los grandes misterios de la ciencia. El hecho de que una colecci\u00f3n de simples c\u00e9lulas puede llegar a generar razonamiento, acci\u00f3n, y conciencia es un enigma a resolver. Cerebros y computadores realizan tareas bastante diferentes y tienen propiedades muy distintas. Seg\u00fan los c\u00e1lculos de los expertos se estima que para el 2020 las computadoras igualaran la capacidad de procesamiento de los cerebros. Muchos modelos de  IA  fueron inspirados en la estructura y el funcionamiento de nuestro cerebro.",
            "title": "Neurociencias"
        },
        {
            "location": "/#psicologia",
            "text": "La  Psicolog\u00eda  trata sobre el estudio y an\u00e1lisis de la conducta y los procesos mentales de los individuos y grupos humanos. La rama que m\u00e1s influencia ha tenido para la  Inteligencia Artificial  es la de la  psicolog\u00eda cognitiva  que se encarga del estudio de la cognici\u00f3n; es decir, de los procesos mentales implicados en el conocimiento. Tiene como objeto de estudio los mecanismos b\u00e1sicos y profundos por los que se elabora el conocimiento, desde la percepci\u00f3n, la memoria y el aprendizaje, hasta la formaci\u00f3n de conceptos y razonamiento l\u00f3gico. Las teor\u00eda descritas por esta rama han sido utilizados para desarrollar varios modelos de  Inteligencia Artificial  y  Machine Learning",
            "title": "Psicolog\u00eda"
        },
        {
            "location": "/#ramas-de-la-inteligencia-artificial",
            "text": "Dentro de la  Inteligencia Artificial  podemos encontrar distintas ramas, entre las que se destacan:",
            "title": "Ramas de la Inteligencia artificial"
        },
        {
            "location": "/#machine-learning",
            "text": "El  Machine Learning  es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del  Machine Learning  es  generalizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada . El ejemplo m\u00e1s t\u00edpico donde podemos ver el uso del Machine Learning es en el filtrado de los correo basura o spam. Mediante la observaci\u00f3n de miles de correos electr\u00f3nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos.   El  Machine Learning  tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito, an\u00e1lisis del mercado de valores, clasificaci\u00f3n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob\u00f3tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distingir los distintos  tipos de problemas de  Machine Learning  con los que nos podemos encontrar.",
            "title": "Machine Learning"
        },
        {
            "location": "/#aprendizaje-supervisado",
            "text": "En los problemas de  aprendizaje supervisado  se ense\u00f1a o entrena al algoritmo a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos, el algoritmo podr\u00e1 generalizar en una forma m\u00e1s precisa. Una vez concluido el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el algoritmo de aprendizaje utiliza la experiencia pasada que adquiri\u00f3 durante la etapa de entrenamiento para predecir un resultado.",
            "title": "Aprendizaje supervisado"
        },
        {
            "location": "/#aprendizaje-no-supervisado",
            "text": "En los problemas de  aprendizaje no supervisado , el algoritmo es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al algoritmo lo que representan los datos. La idea es que el algoritmo pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos.",
            "title": "Aprendizaje no supervisado"
        },
        {
            "location": "/#aprendizaje-por-refuerzo",
            "text": "En los problemas de  aprendizaje por refuerzo , el algoritmo aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m\u00e1s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar\u00e1 creando una estrategia ganadora.",
            "title": "Aprendizaje por refuerzo"
        },
        {
            "location": "/#deep-learning",
            "text": "El  Deep Learning  constituye un conjunto particular de algoritmos de  Machine Learning  que utilizan estructuras profundas de  redes neuronales  para encontrar patrones en los datos. Estos tipos de algoritmos cuentan actualmente con un gran inter\u00e9s, ya que han demostrado ser sumamente exitosos para resolver determinados tipos de problemas; como por ejemplo, el reconocimiento de im\u00e1genes. Muchos consideran que este tipo de modelos son los que en el futuro nos llevaran a resolver definitivamente el problema de la  Inteligencia Artificial .",
            "title": "Deep Learning"
        },
        {
            "location": "/#razonamiento-probabilistico",
            "text": "El  razonamiento probabil\u00edstico  se encarga de lidiar con la incertidumbre inherente de todo proceso de aprendizaje. El problema para crear una  Inteligencia Artificial  entonces se convierte en encontrar la forma de trabajar con informaci\u00f3n ruidosa, incompleta e incluso muchas veces contradictoria. Estos algoritmos est\u00e1n sumamente ligados a la  estad\u00edstica bayesiana ; y la principal herramienta en la que se apoyan es en el  teorema de Bayes .",
            "title": "Razonamiento probabil\u00edstico"
        },
        {
            "location": "/#algortimos-geneticos",
            "text": "Los  algoritmos gen\u00e9ticos  se basan en la idea de que la madre de todo aprendizaje es la  selecci\u00f3n natural . Si la Naturaleza pudo crearnos, puede crear cualquier cosa; por tal motivo lo \u00fanico que deber\u00edamos hacer para alcanzar una  Inteligencia Artificial  es simular sus mecanismos en una computadora. La idea de estos algoritmos es imitar a la Evoluci\u00f3n; funcionan seleccionando individuos de una poblaci\u00f3n de soluciones candidatas, y luego intentando producir nuevas generaciones de soluciones mejores que las anteriores una y otra vez hasta aproximarse a una soluci\u00f3n perfecta.",
            "title": "Algortimos gen\u00e9ticos"
        },
        {
            "location": "/#aplicaciones-de-la-inteligencia-artificial",
            "text": "Las t\u00e9cnicas de la  Inteligencia Artificial  pueden ser aplicadas en una gran variedad de industrias y situaciones, como ser:",
            "title": "Aplicaciones de la Inteligencia artificial"
        },
        {
            "location": "/#medicina",
            "text": "Apoy\u00e1ndose en las herramientas que proporciona la  Inteligencia Artificial , los doctores podr\u00edan realizar diagn\u00f3sticos m\u00e1s certeros y oportunos, lo que llevar\u00eda a mejores tratamientos y m\u00e1s vidas salvadas.",
            "title": "Medicina"
        },
        {
            "location": "/#autos-autonomos",
            "text": "Utilizando  Inteligencia Artificial  podr\u00edamos crear autos aut\u00f3nomos que aprendan de los datos y experiencias de millones de otros autos, mejorando el tr\u00e1fico y haciendo mucho m\u00e1s segura la conducci\u00f3n.",
            "title": "Autos aut\u00f3nomos"
        },
        {
            "location": "/#bancos",
            "text": "Utilizando t\u00e9cnicas de  Machine Learning  los bancos pueden detectar fraudes antes de que ocurran por medio de analizar los patrones de comportamiento de gastos e identificando r\u00e1pidamente actividades sospechosas.",
            "title": "Bancos"
        },
        {
            "location": "/#agricultura",
            "text": "En Agricultura se podr\u00eda optimizar el rendimiento de los cultivos por medio de la utilizaci\u00f3n de las t\u00e9cnicas de  Inteligencia Artificial  para analizar los datos del suelo y del clima en tiempo real, logrando producir m\u00e1s alimentos incluso con climas perjudiciales.",
            "title": "Agricultura"
        },
        {
            "location": "/#educacion",
            "text": "En la Educaci\u00f3n se podr\u00edan utilizar las t\u00e9cnicas de la  Inteligencia Artificial  para dise\u00f1ar programas de estudios personalizados basados en datos que mejoren el rendimiento y el ritmo de aprendizaje de los alumnos.",
            "title": "Educaci\u00f3n"
        },
        {
            "location": "/#la-etica-y-los-riesgos-de-desarrollar-una-inteligencia-artificial",
            "text": "Actualmente tambi\u00e9n ha surgido un debate \u00e9tico alrededor de la  Inteligencia Artificial . Algunos de los pensadores m\u00e1s importantes del planeta han establecido su preocupaci\u00f3n sobre el progreso de la  IA . Entre los problemas que puede traer aparejado el desarrollo de la  Inteligencia Artificial , podemos encontrar los siguientes:   Las personas podr\u00edan perder sus trabajos por la automatizaci\u00f3n.  Las personas podr\u00edan tener demasiado (o muy poco) tiempo de ocio.  Las personas podr\u00edan perder el sentido de ser \u00fanicos.  Las personas podr\u00edan perder algunos de sus derechos privados.  La utilizaci\u00f3n de los sistemas de  IA  podr\u00eda llevar a la p\u00e9rdida de responsabilidad.  El \u00e9xito de la  IA  podr\u00eda significar el fin de la raza humana.   El debate sobre los beneficios y riesgos del desarrollo de la  Inteligencia Artificial  est\u00e1 todav\u00eda abierto.",
            "title": "La \u00e9tica y los riesgos de desarrollar una Inteligencia Artificial"
        },
        {
            "location": "/#como-iniciarse-en-el-campo-de-la-inteligencia-artificial",
            "text": "Si luego de leer esta introducci\u00f3n, te has quedado fascinado por el campo de la  Inteligencia Artificial  y quieres incursionar en el mismo, aqu\u00ed te dejo algunas recomendaciones para iniciarse.",
            "title": "\u00bfC\u00f3mo iniciarse en el campo de la Inteligencia artificial?"
        },
        {
            "location": "/#iaar",
            "text": "IAAR  es la comunidad argentina de inteligencia artificial. Agrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo \u00e9tico y humanitario de las tecnolog\u00edas cognitivas. Para comenzar a formar parte de la comunidad pueden inscribirse en los grupos de facebook:  IAAR ,  Debates ,  Proyectos ,  Capacitaci\u00f3n ; y/o en el  meetup .",
            "title": "IAAR"
        },
        {
            "location": "/#programacion",
            "text": "Para poder trabajar en problemas relacionados al campo de la  Inteligencia Artificial  es necesario saber programar. Los principales lenguajes que se utilizan son  Python  y  R . En los repositorios de  Academia de IAAR  van a poder encontrar material sobre estos lenguajes.",
            "title": "Programaci\u00f3n"
        },
        {
            "location": "/#frameworks",
            "text": "Existen varios frameworks open source que nos facilitan el trabajar con modelos de  Deep Learning , entre los que se destacan:   TensorFlow :  TensorFlow  es un frameworks desarrollado por Google. Es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos que utiliza el lenguaje  Python .   PyTorch :  PyTorch  es un framework de  Deep Learning  que utiliza el lenguaje  Python  y cuenta con el apoyo de Facebook.  Caffe :  Caffe  es un framework de  Deep Learning  hecho con expresi\u00f3n, velocidad y modularidad en mente, el cual es desarrollado por la universidad de Berkeley.  CNTK :  CNTK  es un conjunto de herramientas, desarrolladas por Microsoft, f\u00e1ciles de usar, de c\u00f3digo abierto que entrena algoritmos de  Deep Learning  para aprender como el cerebro humano.  Theano :  Theano  es una librer\u00eda de  Python  que permite definir, optimizar y evaluar expresiones matem\u00e1ticas que involucran tensores de manera eficiente.   DeepLearning4j :  DeepLearning4j  Es una librer\u00eda open source para trabajar con modelos de  Deep Learning  distribuidos utilizando el lenguaje  Java .",
            "title": "Frameworks"
        },
        {
            "location": "/#bots",
            "text": "Una de las ramas con mayor crecimiento y que m\u00e1s se ha beneficiado con el boom de la  Inteligencia Artificial  es la de los  Bots . Generar peque\u00f1os  Bots  que puedan tener conversaciones b\u00e1sicas con los usuarios es bastante simple. Pueden encontrar una gu\u00eda con una gran n\u00famero de herramientas en el  blog de IAAR .",
            "title": "Bots"
        },
        {
            "location": "/deeplearning/",
            "text": "Introducci\u00f3n al Deep Learning\n\n\n\n\nIntroducci\u00f3n \n\n\nEl \nDeep Learning\n es sin duda el \u00e1rea de investigaci\u00f3n m\u00e1s popular dentro del campo de la \ninteligencia artificial\n. La mayor\u00eda de las nuevas investigaciones que se realizan, trabajan con modelos basados en las t\u00e9cnicas de \nDeep Learning\n; ya que las mismas han logrado resultados sorprendes en campos como \nProcesamiento del lenguaje natural\n y \nVisi\u00f3n por computadora\n. Pero... \u00bfqu\u00e9 es este misterioso concepto que ha ganado tanta popularidad? y... \u00bfc\u00f3mo se relaciona con el campo de la \ninteligencia artificial\n y el \nMachine Learning\n?. \n\n\nInteligencia artificial, Machine learning y Deep learning \n\n\nEn general se suelen utilizar los t\u00e9rminos de \ninteligencia artificial\n, \nMachine Learning\n y \nDeep Learning\n en forma intercambiada. Sin embargo, \u00e9stos t\u00e9rminos no son los mismo y abarcan distintas cosas. \n\n\nInteligencia Artificial\n\n\nEl t\u00e9rmino \ninteligencia artificial\n es el m\u00e1s general y engloba a los campos de \nMachine Learning\n y \nDeep Learning\n junto con otras t\u00e9cnicas como los \nalgoritmos de b\u00fasqueda\n, el \nrazonamiento simb\u00f3lico\n, el \nrazonamiento l\u00f3gico\n y la \nestad\u00edstica\n. Naci\u00f3 en los a\u00f1os 1950s, cuando un grupo de pioneros de la computaci\u00f3n comenzaron a preguntarse si se pod\u00eda hacer que las computadoras \npensaran\n. Una definici\u00f3n concisa de la \ninteligencia artificial\n ser\u00eda: \nel esfuerzo para automatizar las tareas intelectuales que normalmente realizan los seres humanos\n. \n\n\nMachine Learning\n\n\nEl \nMachine Learning\n o \nAprendizaje autom\u00e1tico\n se refiere a un amplio conjunto de t\u00e9cnicas inform\u00e1ticas que nos permiten dar a las computadoras \nla capacidad de aprender sin ser expl\u00edcitamente programadas\n. Hay muchos tipos diferentes de algoritmos de \nAprendizaje autom\u00e1tico\n, entre los que se encuentran el \naprendizaje por refuerzo\n, los \nalgoritmos gen\u00e9ticos\n, el aprendizaje basado en \nreglas de asociaci\u00f3n\n, los \nalgoritmos de agrupamiento\n, los \n\u00e1rboles de decisi\u00f3n\n, las \nm\u00e1quinas de vectores de soporte\n y las \nredes neuronales\n. Actualmente, los algoritmos m\u00e1s populares dentro de este campo son los de \nDeep Learning\n.\n\n\nDeep Learning\n\n\nEl \nDeep Learning\n o \naprendizaje profundo\n es un subcampo dentro del \nMachine Learning\n, el cu\u00e1l utiliza distintas estructuras de \nredes neuronales\n para lograr el aprendizaje de sucesivas \ncapas de representaciones\n cada vez m\u00e1s significativas de los datos. El \nprofundo\n o \ndeep\n en \nDeep Learning\n hace referencia a la cantidad de \ncapas de representaciones\n que se utilizan en el modelo; en general se suelen utilizar decenas o incluso cientos de \ncapas de representaci\u00f3n\n. las cuales \naprenden\n automaticamente a medida que el modelo es entrenado con los datos.\n\n\n\n\n\u00bfQu\u00e9 es el Deep Learning? \n\n\nAntes de poder entender que es el \nDeep Learning\n, debemos en primer lugar conocer dos conceptos fundamentales: las \nredes neuronales artificiales\n y la \nPropagaci\u00f3n hacia atr\u00e1s\n.\n\n\nRedes Neuronales\n\n\nLas \nredes neuronales\n son un modelo computacional basado en un gran conjunto de unidades neuronales simples (\nneuronas artificiales\n), de forma aproximadamente an\u00e1loga al comportamiento observado en los axones de las neuronas en los cerebros biol\u00f3gicos. \n\n\nCada una de estas neuronas simples, va a tener una forma similar al siguiente diagrama:\n\n\n\n\nEn donde sus componentes son:\n\n\n\n\n\n\n$x_1, x_2, \\dots, x_n$: Los datos de entrada en la neurona, los cuales tambi\u00e9n puede ser que sean producto de la salida de otra neurona de la red.\n\n\n\n\n\n\n$x_0$: La unidad de sesgo; un valor constante que se le suma a la entrada de la funci\u00f3n de activaci\u00f3n de la neurona. Generalmente tiene el valor 1. Este valor va a permitir cambiar la funci\u00f3n de activaci\u00f3n hacia la derecha o izquierda, otorg\u00e1ndole m\u00e1s flexibilidad para aprender a la neurona.\n\n\n\n\n\n\n$w_0, w_1, w_2, \\dots, w_n$: Los pesos relativos de cada entrada. Tener en cuenta que incluso la unidad de sesgo tiene un peso.\n\n\n\n\n\n\na: La salida de la neurona. Que va a ser calculada de la siguiente forma:\n\n\n\n\n\n\n$$a = f\\left(\\sum_{i=0}^n w_i \\cdot x_i \\right)$$\n\n\nAqu\u00ed $f$ es la \nfunci\u00f3n de activaci\u00f3n\n de la neurona. Esta funci\u00f3n es la que le otorga tanta flexibilidad a las \nredes neuronales\n y le permite estimar complejas relaciones no lineales en los datos. Puede ser tanto una \nfunci\u00f3n lineal\n, una \nfunci\u00f3n log\u00edstica\n, \nhiperb\u00f3lica\n, etc.\n\n\nCada unidad neuronal est\u00e1 conectada con muchas otras y los enlaces entre ellas pueden incrementar o inhibir el estado de activaci\u00f3n de las neuronas adyacentes. Estos sistemas aprenden y se forman a s\u00ed mismos, en lugar de ser programados de forma expl\u00edcita, y sobresalen en \u00e1reas donde la detecci\u00f3n de soluciones o caracter\u00edsticas es dif\u00edcil de expresar con la programaci\u00f3n convencional.\n\n\n\n\nPropagaci\u00f3n hacia atr\u00e1s\n\n\nLa \npropagaci\u00f3n hacia atr\u00e1s\n o \nbackpropagation\n es un algoritmo que funciona mediante la determinaci\u00f3n de la p\u00e9rdida (o error) en la salida y luego propag\u00e1ndolo de nuevo hacia atr\u00e1s en la red. De esta forma los pesos se van actualizando para minimizar el error resultante de cada neurona. Este algoritmo es lo que les permite a las \nredes neuronales\n aprender.\n\n\n\n\n\u00bfC\u00f3mo funciona el Deep Learning?  \n\n\nEn general, cualquier t\u00e9cnica de \nMachine Learning\n trata de realizar la asignaci\u00f3n de entradas (por ejemplo, im\u00e1genes) a salidas objetivo (Por ejemplo, la etiqueta \"gato\"), mediante la observaci\u00f3n de un gran n\u00famero de ejemplos de entradas y salidas. El \nDeep Learning\n realiza este mapeo de entrada-a-objetivo por medio de una \nred neuronal artificial\n que est\u00e1 compuesta de un n\u00famero grande de \ncapas\n dispuestas en forma de jerarqu\u00eda. La \nred\n aprende algo simple en la capa inicial de la jerarqu\u00eda y luego env\u00eda esta informaci\u00f3n a la siguiente capa. La siguiente capa toma esta informaci\u00f3n simple, lo combina en algo que es un poco m\u00e1s complejo, y lo pasa a la tercer capa. Este proceso contin\u00faa de forma tal que cada capa de la jerarqu\u00eda construye algo m\u00e1s complejo de la entrada que recibi\u00f3 de la capa anterior. De esta forma, la \nred\n ir\u00e1 \naprendiendo\n por medio de la exposici\u00f3n a los datos de ejemplo.\n\n\nLa especificaci\u00f3n de lo que cada \ncapa\n hace a la entrada que recibe es almacenada en los \npesos\n de la capa, que en esencia, no son m\u00e1s que n\u00fameros. Utilizando terminolog\u00eda m\u00e1s t\u00e9cnica podemos decir que la transformaci\u00f3n de datos que se produce en la \ncapa\n es \nparametrizada\n por sus \npesos\n. Para que la \nred\n aprenda debemos encontrar los \npesos\n de todas las \ncapas\n de forma tal que la \nred\n realice un mapeo perfecto entre los ejemplos de entrada con sus respectivas salidas objetivo. Pero el problema reside en que una \nred\n de \nDeep Learning\n puede tener millones de \npar\u00e1metros\n, por lo que encontrar el valor correcto de todos ellos puede ser una tarea realmente muy dif\u00edcil, especialmente si la modificaci\u00f3n del valor de uno de ellos afecta a todos los dem\u00e1s.\n\n\n\n\nPara poder controlar algo, en primer lugar debemos poder observarlo. En este sentido, para controlar la salida de la \nred neuronal\n, deber\u00edamos poder medir cuan lejos esta la salida que obtuvimos de la que se esperaba obtener. Este es el trabajo de la \nfunci\u00f3n de p\u00e9rdida\n de la \nred\n. Esta funci\u00f3n toma las predicciones que realiza el modelo y los valores objetivos (lo que realmente esperamos que la \nred\n produzca), y calcula cu\u00e1n lejos estamos de ese valor, de esta manera, podemos capturar que tan bien esta funcionando el modelo para el ejemplo especificado. El truco fundamental del \nDeep Learning\n es utilizar el valor que nos devuelve esta  \nfunci\u00f3n de p\u00e9rdida\n para retroalimentar la  \nred\n y ajustar los \npesos\n en la direcci\u00f3n que vayan reduciendo la \np\u00e9rdida\n del modelo para cada ejemplo. Este ajuste, es el trabajo del \noptimizador\n, el cu\u00e1l implementa la \npropagaci\u00f3n hacia atr\u00e1s\n. \n\n\n\n\nResumiendo, el funcionamiento ser\u00eda el siguiente: inicialmente, los \npesos\n de cada \ncapa\n son asignados en forma aleatoria, por lo que la \nred\n simplemente implementa una serie de transformaciones aleatorias. En este primer paso, obviamente la salida del modelo dista bastante del ideal que deseamos obtener, por lo que el valor de la \nfunci\u00f3n de p\u00e9rdida\n va a ser bastante alto. Pero a medida que la \nred\n va procesando nuevos casos, los \npesos\n se van ajustando de forma tal de ir reduciendo cada vez m\u00e1s el valor de la \nfunci\u00f3n de p\u00e9rdida\n. Este proceso es el que se conoce como \nentrenamiento\n de la \nred\n, el cual repetido una suficiente cantidad de veces, generalmente 10 iteraciones de miles de ejemplos, logra que los \npesos\n se ajusten a los que minimizan la \nfunci\u00f3n de p\u00e9rdida\n. Una \nred\n que ha minimizado la \np\u00e9rdida\n es la que logra los resultados que mejor se ajustan a las salidas objetivo, es decir, que el modelo se encuentra \nentrenado\n. \n\n\nArquitecturas de Deep Learning \n\n\nLa estructura de datos fundamental de una \nred neuronal\n est\u00e1 vagamente inspirada en el cerebro humano. Cada una de nuestras c\u00e9lulas cerebrales (neuronas) est\u00e1 conectada a muchas otras neuronas por sinapsis. A medida que experimentamos e interactuamos con el mundo, nuestro cerebro crea nuevas conexiones, refuerza algunas conexiones y debilita a los dem\u00e1s. De esta forma, en nuestro cerebro se desarrollan ciertas regiones que se especializan en el procesamiento de determinadas \nentradas\n. As\u00ed vamos a tener un \u00e1rea especializada en la visi\u00f3n, otra que se especializa en la audici\u00f3n, otra para el lenguaje, etc. De forma similar, dependiendo del tipo de \nentradas\n con las que trabajemos, van a existir distintas \narquitecturas\n de \nredes neuronales\n que mejor se adaptan para procesar esa informaci\u00f3n. Algunas de las arquitecturas m\u00e1s populares son:\n\n\nRedes neuronales prealimentadas\n\n\nLas \nRedes neuronales prealimentadas\n fueron las primeras que se desarrollaron y son el modelo m\u00e1s sencillo. En estas redes la informaci\u00f3n se mueve en una sola direcci\u00f3n: hacia adelante. Los principales exponentes de este tipo de arquitectura son el \nperceptr\u00f3n\n y el \nperceptr\u00f3n multicapa\n. Se suelen utilizar en problemas de clasificaci\u00f3n simples. \n\n\n\n\nRedes neuronales convolucionales\n\n\nLas \nredes neuronales convolucionales\n son muy similares a las \nredes neuronales\n ordinarias como el \nperceptron multicapa\n; se componen de \nneuronas\n que tienen \npesos\n y \nsesgos\n que pueden aprender. Cada \nneurona\n recibe algunas entradas, realiza un \nproducto escalar\n y luego aplica una funci\u00f3n de activaci\u00f3n. Al igual que en el \nperceptron multicapa\n tambi\u00e9n vamos a tener una \nfunci\u00f3n de p\u00e9rdida o costo\n sobre la \u00faltima capa, la cual estar\u00e1 totalmente conectada. Lo que diferencia a las \nredes neuronales convolucionales\n es que suponen expl\u00edcitamente que las entradas son im\u00e1genes, lo que nos permite codificar ciertas propiedades en la arquitectura; permitiendo ganar en eficiencia y reducir la cantidad de par\u00e1metros en la red. \n\n\nEn general, las \nredes neuronales convolucionales\n van a estar construidas con una estructura que contendr\u00e1 3 tipos distintos de capas:\n\n\n\n\nUna capa \nconvolucional\n, que es la que le da le nombre a la red.\n\n\nUna capa de reducci\u00f3n o de \npooling\n, la cual va a reducir la cantidad de par\u00e1metros al quedarse con las caracter\u00edsticas m\u00e1s comunes.\n\n\nUna capa clasificadora totalmente conectada, la cual nos va dar el resultado final de la red.\n\n\n\n\nAlgunas implementaciones espec\u00edficas que podemos encontrar sobre este tipo de redes son: \ninception v3\n, \nResNet\n, \nVGG16\n y \nxception\n, entre otras. Todas ellas han logrado excelentes resultados.\n\n\n\n\nRedes neuronales recurrentes\n\n\nLos seres humanos no comenzamos nuestro pensamiento desde cero cada segundo, sino que los mismos tienen una persistencia. Las \nRedes neuronales prealimentadas\n tradicionales no cuentan con esta persistencia, y esto parece una deficiencia importante. Las \nRedes neuronales recurrentes\n abordan este problema. Son redes con bucles de retroalimentaci\u00f3n, que permiten que la informaci\u00f3n persista.\n\n\nUna \nRed neural recurrente\n puede ser pensada como una red con m\u00faltiples copias de ella misma, en las que cada una de ellas pasa un mensaje a su sucesor. Esta naturaleza en forma de cadena revela que las \nRedes neurales recurrentes\n est\u00e1n \u00edntimamente relacionadas con las secuencias y listas; por lo que son ideales para trabajar con este tipo de datos. En los \u00faltimos a\u00f1os, ha habido un \u00e9xito incre\u00edble aplicando \nRedes neurales recurrentes\n  a una variedad de problemas como: reconocimiento de voz, modelado de lenguaje, traducci\u00f3n, subt\u00edtulos de im\u00e1genes y la lista contin\u00faa.\n\n\nLas \nredes de memoria de largo plazo a corto plazo\n - generalmente llamadas \nLSTMs\n - son un tipo especial de \nRedes neurales recurrentes\n, capaces de aprender dependencias a largo plazo. Ellas tambi\u00e9n tienen una estructura como cadena, pero el m\u00f3dulo de repetici\u00f3n tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, tiene cuatro, que interact\u00faan de una manera especial permitiendo tener una memoria a m\u00e1s largo plazo.\n\n\n\n\nPara m\u00e1s informaci\u00f3n sobre diferentes arquitecturas de \nredes neuronales\n pueden visitar el siguiente art\u00edculo de \nwikipedia\n.\n\n\nLogros del Deep Learning \n\n\nEn los \u00faltimos a\u00f1os el \nDeep Learning\n ha producido toda una revoluci\u00f3n en el campo del \nMachine Learning\n, con resultados notables en todos los problemas de \npercepci\u00f3n\n, como \nver\n y \nescuchar\n, problemas que implican habilidades que parecen muy naturales e intuitivas para los seres humanos, pero que desde hace tiempo se han mostrado dif\u00edciles para las m\u00e1quinas. En particular, el \nDeep Learning\n ha logrado los siguientes avances, todos ellos en \u00e1reas hist\u00f3ricamente dif\u00edciles del \nMachine Learning\n.\n\n\n\n\nUn nivel casi humano para la clasificaci\u00f3n de im\u00e1genes.\n\n\nUn nivel casi humano para el reconocimiento del lenguaje hablado.\n\n\nUn nivel casi humano en el reconocimiento de escritura.\n\n\nGrandes mejoras en traducciones de lenguas.\n\n\nGrandes mejoras en conversaciones \ntext-to-speech\n.\n\n\nAsistentes digitales como Google Now o Siri.\n\n\nUn nivel casi humano en autos aut\u00f3nomos.\n\n\nMejores resultados de b\u00fasqueda en la web.\n\n\nGrandes mejoras para responder preguntas en lenguaje natural.\n\n\nAlcanzado Nivel maestro (superior al humano) en varios juegos.\n\n\n\n\nEn muchos sentidos, el \nDeep Learning\n todav\u00eda sigue siendo un campo misterioso para explorar, por lo que seguramente veremos nuevos avances en nuevas \u00e1reas utilizando estas t\u00e9cnicas. Tal vez alg\u00fan d\u00eda el \nDeep Learning\n ayuda a los seres humanos a hacer ciencia, desarrollar software y mucho m\u00e1s.\n\n\n\u00bfPor qu\u00e9 estos sorprendentes resultados surgen ahora?  \n\n\nMuchos de los conceptos del \nDeep Learning\n se desarrollaron en los a\u00f1os 80s y 90s, algunos incluso mucho antes. Sin embargo, los primeros resultados exitosos del \nDeep Learning\n surgieron en los \u00faltimos 5 a\u00f1os. \u00bfqu\u00e9 fue lo que cambio para lograr la popularidad y \u00e9xito de los modelos basados en \nDeep Learning\n en estos \u00faltimos a\u00f1os? \n\n\nSi bien existen m\u00faltiples factores para explicar esta \nrevoluci\u00f3n\n del \nDeep Learning\n, los dos principales componentes parecen ser la \ndisponibilidad de masivos vol\u00famenes de datos\n, lo que actualmente se conoce bajo el nombre de \nBig Data\n; y el \nprogreso en el poder de computo\n, especialmente gracias a los \nGPUs\n. Entonces, dentro de los factores que explican esta popularidad de los modelos de \nDeep Learning\n podemos encontrar:\n\n\n\n\n\n\nLa disponibilidad de conjuntos de datos enormes y de buena calidad\n. Gracias a la  revoluci\u00f3n digital en que nos encontramos, podemos generar conjuntos de datos enormes con los cuales alimentar a los algoritmos de \nDeep Learning\n, los cuales necesitan de muchos datos para poder \ngeneralizar\n.\n\n\n\n\n\n\nComputaci\u00f3n paralela masiva con \nGPUs\n. En l\u00edneas generales, los modelos de \nredes neuronales\n no son m\u00e1s que complicados c\u00e1lculos num\u00e9ricos que se realizan en paralelo. Gracias al desarrollo de los \nGPUs\n estos c\u00e1lculos ahora se pueden realizar en forma mucho m\u00e1s r\u00e1pida, permitiendo que podamos entrenar modelos m\u00e1s profundos y grandes. \n\n\n\n\n\n\nFunciones de activaci\u00f3n amigables para \nBackpropagation\n. La \nprogaci\u00f3n hacia atr\u00e1s\n o \nBackpropagation\n es el algoritmo fundamental que hace funcionar a las \nredes neuronales\n; pero la forma en que trabaja implica c\u00e1lculos realmente complicados. La transici\u00f3n desde funciones de activaci\u00f3n como \ntanh\n o \nsigmoid\n a funciones como \nReLU\n o \nSELU\n han simplificado estos problemas. \n\n\n\n\n\n\nNuevas arquitecturas\n. Arquitecturas como \nResnets\n, \ninception\n y \nGAN\n mantienen el campo actualizado y contin\u00faan aumentando las flexibilidad de los modelos. \n\n\n\n\n\n\nNuevas t\u00e9cnicas de \nregularizaci\u00f3n\n. T\u00e9cnicas como \ndropout\n, \nbatch normalization\n y \ndata-augmentation\n nos permiten entrenar redes m\u00e1s grandes con menos peligro de \nsobreajuste\n.\n\n\n\n\n\n\nOptimizadores m\u00e1s robustos\n. La \noptimizaci\u00f3n\n es fundamental para el funcionamiento de las \nredes neuronales\n. Mejoras sobre el tradicional procedimiento de \nSGD\n, como \nADAM\n han ayudado a mejorar el rendimiento de los modelos.\n\n\n\n\n\n\nPlataformas de software\n. Herramientas como \nTensorFlow\n, \nTheano\n, \nKeras\n, \nCNTK\n, \nPyTorch\n, \nChainer\n, y \nmxnet\n nos permiten crear prototipos en forma m\u00e1s r\u00e1pida y trabajar con \nGPUs\n sin tantas complicaciones. Nos permiten enfocarnos en la estructura del modelo sin tener que preocuparnos por los detalles de m\u00e1s bajo nivel.\n\n\n\n\n\n\nOtra raz\u00f3n por la que el \nDeep Learning\n ha tenido tanta repercusi\u00f3n \u00faltimamente adem\u00e1s de ofrecer un mejor rendimiento en muchos problemas; es que el \nDeep Learning\n esta haciendo la resoluci\u00f3n de problemas mucho m\u00e1s f\u00e1cil, ya que automatiza completamente lo que sol\u00eda ser uno de los pasos m\u00e1s dif\u00edciles y cruciales en el flujo de trabajo de \nMachine Learning\n: la \ningenier\u00eda de atributos\n. Antes del \nDeep Learning\n, para poder entrenar un modelo, primero deb\u00edamos refinar las \nentradas\n para adaptarlas al tipo de transformaci\u00f3n del modelo; ten\u00edamos que cuidadosamente \nseleccionar los atributos\n m\u00e1s representativos y desechar los poco informativos. El \nDeep Learning\n, en cambio, automatiza este proceso; aprendemos todos los atributos de una sola pasada y el mismo modelo se encarga de adaptarse y quedarse con lo m\u00e1s representativo.  \n\n\n\u00bfC\u00f3mo mantenerse actualizado en el campo de Deep Learning? \n\n\nEl campo del \nDeep Learning\n se mueve muy rapidamente, con varios \npapers\n que se publican por mes; por tal motivo, mantenerse actualizado con las \u00faltimas tendencias del campo puede ser bastante complicado. Algunos consejos pueden ser:\n\n\n\n\n\n\nEstarse atento a las publicaciones en \narxiv\n, especialmente a la secci\u00f3n de \nmachine learning\n. La mayor\u00eda de los \npapers\n m\u00e1s relevantes, los vamos a poder encontrar en esa plataforma.\n\n\n\n\n\n\nSeguir el blog de \nkeras\n en el cual podemos encontrar como implementar varios modelos utilizando esta genial librer\u00eda.\n\n\n\n\n\n\nSeguir el blog de \nopenai\n en d\u00f3nde detallan las investigaciones que van realizando, especialmente trabajando con \nGANs\n.\n\n\n\n\n\n\nSeguir el blog de \nGoogle research\n; en d\u00f3nde se viene haciendo bastante foco en los modelos de \nDeep Learning\n.\n\n\n\n\n\n\nUtilizar la secci\u00f3n de Machine Learning de \nreddit\n.\n\n\n\n\n\n\nSuscribirse al podcast \nTalking machines\n; en d\u00f3nde se entrevista a los principales exponentes del campo de la \ninteligencia artificial\n.\n\n\n\n\n\n\nPor \u00faltimo, obviamente estar atentos a las \npublicaciones que se realizan en \nIAAR\n.",
            "title": "Deep Learning"
        },
        {
            "location": "/deeplearning/#introduccion-al-deep-learning",
            "text": "",
            "title": "Introducci\u00f3n al Deep Learning"
        },
        {
            "location": "/deeplearning/#introduccion",
            "text": "El  Deep Learning  es sin duda el \u00e1rea de investigaci\u00f3n m\u00e1s popular dentro del campo de la  inteligencia artificial . La mayor\u00eda de las nuevas investigaciones que se realizan, trabajan con modelos basados en las t\u00e9cnicas de  Deep Learning ; ya que las mismas han logrado resultados sorprendes en campos como  Procesamiento del lenguaje natural  y  Visi\u00f3n por computadora . Pero... \u00bfqu\u00e9 es este misterioso concepto que ha ganado tanta popularidad? y... \u00bfc\u00f3mo se relaciona con el campo de la  inteligencia artificial  y el  Machine Learning ?.",
            "title": "Introducci\u00f3n "
        },
        {
            "location": "/deeplearning/#inteligencia-artificial-machine-learning-y-deep-learning",
            "text": "En general se suelen utilizar los t\u00e9rminos de  inteligencia artificial ,  Machine Learning  y  Deep Learning  en forma intercambiada. Sin embargo, \u00e9stos t\u00e9rminos no son los mismo y abarcan distintas cosas.",
            "title": "Inteligencia artificial, Machine learning y Deep learning "
        },
        {
            "location": "/deeplearning/#inteligencia-artificial",
            "text": "El t\u00e9rmino  inteligencia artificial  es el m\u00e1s general y engloba a los campos de  Machine Learning  y  Deep Learning  junto con otras t\u00e9cnicas como los  algoritmos de b\u00fasqueda , el  razonamiento simb\u00f3lico , el  razonamiento l\u00f3gico  y la  estad\u00edstica . Naci\u00f3 en los a\u00f1os 1950s, cuando un grupo de pioneros de la computaci\u00f3n comenzaron a preguntarse si se pod\u00eda hacer que las computadoras  pensaran . Una definici\u00f3n concisa de la  inteligencia artificial  ser\u00eda:  el esfuerzo para automatizar las tareas intelectuales que normalmente realizan los seres humanos .",
            "title": "Inteligencia Artificial"
        },
        {
            "location": "/deeplearning/#machine-learning",
            "text": "El  Machine Learning  o  Aprendizaje autom\u00e1tico  se refiere a un amplio conjunto de t\u00e9cnicas inform\u00e1ticas que nos permiten dar a las computadoras  la capacidad de aprender sin ser expl\u00edcitamente programadas . Hay muchos tipos diferentes de algoritmos de  Aprendizaje autom\u00e1tico , entre los que se encuentran el  aprendizaje por refuerzo , los  algoritmos gen\u00e9ticos , el aprendizaje basado en  reglas de asociaci\u00f3n , los  algoritmos de agrupamiento , los  \u00e1rboles de decisi\u00f3n , las  m\u00e1quinas de vectores de soporte  y las  redes neuronales . Actualmente, los algoritmos m\u00e1s populares dentro de este campo son los de  Deep Learning .",
            "title": "Machine Learning"
        },
        {
            "location": "/deeplearning/#deep-learning",
            "text": "El  Deep Learning  o  aprendizaje profundo  es un subcampo dentro del  Machine Learning , el cu\u00e1l utiliza distintas estructuras de  redes neuronales  para lograr el aprendizaje de sucesivas  capas de representaciones  cada vez m\u00e1s significativas de los datos. El  profundo  o  deep  en  Deep Learning  hace referencia a la cantidad de  capas de representaciones  que se utilizan en el modelo; en general se suelen utilizar decenas o incluso cientos de  capas de representaci\u00f3n . las cuales  aprenden  automaticamente a medida que el modelo es entrenado con los datos.",
            "title": "Deep Learning"
        },
        {
            "location": "/deeplearning/#que-es-el-deep-learning",
            "text": "Antes de poder entender que es el  Deep Learning , debemos en primer lugar conocer dos conceptos fundamentales: las  redes neuronales artificiales  y la  Propagaci\u00f3n hacia atr\u00e1s .",
            "title": "\u00bfQu\u00e9 es el Deep Learning? "
        },
        {
            "location": "/deeplearning/#redes-neuronales",
            "text": "Las  redes neuronales  son un modelo computacional basado en un gran conjunto de unidades neuronales simples ( neuronas artificiales ), de forma aproximadamente an\u00e1loga al comportamiento observado en los axones de las neuronas en los cerebros biol\u00f3gicos.   Cada una de estas neuronas simples, va a tener una forma similar al siguiente diagrama:   En donde sus componentes son:    $x_1, x_2, \\dots, x_n$: Los datos de entrada en la neurona, los cuales tambi\u00e9n puede ser que sean producto de la salida de otra neurona de la red.    $x_0$: La unidad de sesgo; un valor constante que se le suma a la entrada de la funci\u00f3n de activaci\u00f3n de la neurona. Generalmente tiene el valor 1. Este valor va a permitir cambiar la funci\u00f3n de activaci\u00f3n hacia la derecha o izquierda, otorg\u00e1ndole m\u00e1s flexibilidad para aprender a la neurona.    $w_0, w_1, w_2, \\dots, w_n$: Los pesos relativos de cada entrada. Tener en cuenta que incluso la unidad de sesgo tiene un peso.    a: La salida de la neurona. Que va a ser calculada de la siguiente forma:    $$a = f\\left(\\sum_{i=0}^n w_i \\cdot x_i \\right)$$  Aqu\u00ed $f$ es la  funci\u00f3n de activaci\u00f3n  de la neurona. Esta funci\u00f3n es la que le otorga tanta flexibilidad a las  redes neuronales  y le permite estimar complejas relaciones no lineales en los datos. Puede ser tanto una  funci\u00f3n lineal , una  funci\u00f3n log\u00edstica ,  hiperb\u00f3lica , etc.  Cada unidad neuronal est\u00e1 conectada con muchas otras y los enlaces entre ellas pueden incrementar o inhibir el estado de activaci\u00f3n de las neuronas adyacentes. Estos sistemas aprenden y se forman a s\u00ed mismos, en lugar de ser programados de forma expl\u00edcita, y sobresalen en \u00e1reas donde la detecci\u00f3n de soluciones o caracter\u00edsticas es dif\u00edcil de expresar con la programaci\u00f3n convencional.",
            "title": "Redes Neuronales"
        },
        {
            "location": "/deeplearning/#propagacion-hacia-atras",
            "text": "La  propagaci\u00f3n hacia atr\u00e1s  o  backpropagation  es un algoritmo que funciona mediante la determinaci\u00f3n de la p\u00e9rdida (o error) en la salida y luego propag\u00e1ndolo de nuevo hacia atr\u00e1s en la red. De esta forma los pesos se van actualizando para minimizar el error resultante de cada neurona. Este algoritmo es lo que les permite a las  redes neuronales  aprender.",
            "title": "Propagaci\u00f3n hacia atr\u00e1s"
        },
        {
            "location": "/deeplearning/#como-funciona-el-deep-learning",
            "text": "En general, cualquier t\u00e9cnica de  Machine Learning  trata de realizar la asignaci\u00f3n de entradas (por ejemplo, im\u00e1genes) a salidas objetivo (Por ejemplo, la etiqueta \"gato\"), mediante la observaci\u00f3n de un gran n\u00famero de ejemplos de entradas y salidas. El  Deep Learning  realiza este mapeo de entrada-a-objetivo por medio de una  red neuronal artificial  que est\u00e1 compuesta de un n\u00famero grande de  capas  dispuestas en forma de jerarqu\u00eda. La  red  aprende algo simple en la capa inicial de la jerarqu\u00eda y luego env\u00eda esta informaci\u00f3n a la siguiente capa. La siguiente capa toma esta informaci\u00f3n simple, lo combina en algo que es un poco m\u00e1s complejo, y lo pasa a la tercer capa. Este proceso contin\u00faa de forma tal que cada capa de la jerarqu\u00eda construye algo m\u00e1s complejo de la entrada que recibi\u00f3 de la capa anterior. De esta forma, la  red  ir\u00e1  aprendiendo  por medio de la exposici\u00f3n a los datos de ejemplo.  La especificaci\u00f3n de lo que cada  capa  hace a la entrada que recibe es almacenada en los  pesos  de la capa, que en esencia, no son m\u00e1s que n\u00fameros. Utilizando terminolog\u00eda m\u00e1s t\u00e9cnica podemos decir que la transformaci\u00f3n de datos que se produce en la  capa  es  parametrizada  por sus  pesos . Para que la  red  aprenda debemos encontrar los  pesos  de todas las  capas  de forma tal que la  red  realice un mapeo perfecto entre los ejemplos de entrada con sus respectivas salidas objetivo. Pero el problema reside en que una  red  de  Deep Learning  puede tener millones de  par\u00e1metros , por lo que encontrar el valor correcto de todos ellos puede ser una tarea realmente muy dif\u00edcil, especialmente si la modificaci\u00f3n del valor de uno de ellos afecta a todos los dem\u00e1s.   Para poder controlar algo, en primer lugar debemos poder observarlo. En este sentido, para controlar la salida de la  red neuronal , deber\u00edamos poder medir cuan lejos esta la salida que obtuvimos de la que se esperaba obtener. Este es el trabajo de la  funci\u00f3n de p\u00e9rdida  de la  red . Esta funci\u00f3n toma las predicciones que realiza el modelo y los valores objetivos (lo que realmente esperamos que la  red  produzca), y calcula cu\u00e1n lejos estamos de ese valor, de esta manera, podemos capturar que tan bien esta funcionando el modelo para el ejemplo especificado. El truco fundamental del  Deep Learning  es utilizar el valor que nos devuelve esta   funci\u00f3n de p\u00e9rdida  para retroalimentar la   red  y ajustar los  pesos  en la direcci\u00f3n que vayan reduciendo la  p\u00e9rdida  del modelo para cada ejemplo. Este ajuste, es el trabajo del  optimizador , el cu\u00e1l implementa la  propagaci\u00f3n hacia atr\u00e1s .    Resumiendo, el funcionamiento ser\u00eda el siguiente: inicialmente, los  pesos  de cada  capa  son asignados en forma aleatoria, por lo que la  red  simplemente implementa una serie de transformaciones aleatorias. En este primer paso, obviamente la salida del modelo dista bastante del ideal que deseamos obtener, por lo que el valor de la  funci\u00f3n de p\u00e9rdida  va a ser bastante alto. Pero a medida que la  red  va procesando nuevos casos, los  pesos  se van ajustando de forma tal de ir reduciendo cada vez m\u00e1s el valor de la  funci\u00f3n de p\u00e9rdida . Este proceso es el que se conoce como  entrenamiento  de la  red , el cual repetido una suficiente cantidad de veces, generalmente 10 iteraciones de miles de ejemplos, logra que los  pesos  se ajusten a los que minimizan la  funci\u00f3n de p\u00e9rdida . Una  red  que ha minimizado la  p\u00e9rdida  es la que logra los resultados que mejor se ajustan a las salidas objetivo, es decir, que el modelo se encuentra  entrenado .",
            "title": "\u00bfC\u00f3mo funciona el Deep Learning?  "
        },
        {
            "location": "/deeplearning/#arquitecturas-de-deep-learning",
            "text": "La estructura de datos fundamental de una  red neuronal  est\u00e1 vagamente inspirada en el cerebro humano. Cada una de nuestras c\u00e9lulas cerebrales (neuronas) est\u00e1 conectada a muchas otras neuronas por sinapsis. A medida que experimentamos e interactuamos con el mundo, nuestro cerebro crea nuevas conexiones, refuerza algunas conexiones y debilita a los dem\u00e1s. De esta forma, en nuestro cerebro se desarrollan ciertas regiones que se especializan en el procesamiento de determinadas  entradas . As\u00ed vamos a tener un \u00e1rea especializada en la visi\u00f3n, otra que se especializa en la audici\u00f3n, otra para el lenguaje, etc. De forma similar, dependiendo del tipo de  entradas  con las que trabajemos, van a existir distintas  arquitecturas  de  redes neuronales  que mejor se adaptan para procesar esa informaci\u00f3n. Algunas de las arquitecturas m\u00e1s populares son:",
            "title": "Arquitecturas de Deep Learning "
        },
        {
            "location": "/deeplearning/#redes-neuronales-prealimentadas",
            "text": "Las  Redes neuronales prealimentadas  fueron las primeras que se desarrollaron y son el modelo m\u00e1s sencillo. En estas redes la informaci\u00f3n se mueve en una sola direcci\u00f3n: hacia adelante. Los principales exponentes de este tipo de arquitectura son el  perceptr\u00f3n  y el  perceptr\u00f3n multicapa . Se suelen utilizar en problemas de clasificaci\u00f3n simples.",
            "title": "Redes neuronales prealimentadas"
        },
        {
            "location": "/deeplearning/#redes-neuronales-convolucionales",
            "text": "Las  redes neuronales convolucionales  son muy similares a las  redes neuronales  ordinarias como el  perceptron multicapa ; se componen de  neuronas  que tienen  pesos  y  sesgos  que pueden aprender. Cada  neurona  recibe algunas entradas, realiza un  producto escalar  y luego aplica una funci\u00f3n de activaci\u00f3n. Al igual que en el  perceptron multicapa  tambi\u00e9n vamos a tener una  funci\u00f3n de p\u00e9rdida o costo  sobre la \u00faltima capa, la cual estar\u00e1 totalmente conectada. Lo que diferencia a las  redes neuronales convolucionales  es que suponen expl\u00edcitamente que las entradas son im\u00e1genes, lo que nos permite codificar ciertas propiedades en la arquitectura; permitiendo ganar en eficiencia y reducir la cantidad de par\u00e1metros en la red.   En general, las  redes neuronales convolucionales  van a estar construidas con una estructura que contendr\u00e1 3 tipos distintos de capas:   Una capa  convolucional , que es la que le da le nombre a la red.  Una capa de reducci\u00f3n o de  pooling , la cual va a reducir la cantidad de par\u00e1metros al quedarse con las caracter\u00edsticas m\u00e1s comunes.  Una capa clasificadora totalmente conectada, la cual nos va dar el resultado final de la red.   Algunas implementaciones espec\u00edficas que podemos encontrar sobre este tipo de redes son:  inception v3 ,  ResNet ,  VGG16  y  xception , entre otras. Todas ellas han logrado excelentes resultados.",
            "title": "Redes neuronales convolucionales"
        },
        {
            "location": "/deeplearning/#redes-neuronales-recurrentes",
            "text": "Los seres humanos no comenzamos nuestro pensamiento desde cero cada segundo, sino que los mismos tienen una persistencia. Las  Redes neuronales prealimentadas  tradicionales no cuentan con esta persistencia, y esto parece una deficiencia importante. Las  Redes neuronales recurrentes  abordan este problema. Son redes con bucles de retroalimentaci\u00f3n, que permiten que la informaci\u00f3n persista.  Una  Red neural recurrente  puede ser pensada como una red con m\u00faltiples copias de ella misma, en las que cada una de ellas pasa un mensaje a su sucesor. Esta naturaleza en forma de cadena revela que las  Redes neurales recurrentes  est\u00e1n \u00edntimamente relacionadas con las secuencias y listas; por lo que son ideales para trabajar con este tipo de datos. En los \u00faltimos a\u00f1os, ha habido un \u00e9xito incre\u00edble aplicando  Redes neurales recurrentes   a una variedad de problemas como: reconocimiento de voz, modelado de lenguaje, traducci\u00f3n, subt\u00edtulos de im\u00e1genes y la lista contin\u00faa.  Las  redes de memoria de largo plazo a corto plazo  - generalmente llamadas  LSTMs  - son un tipo especial de  Redes neurales recurrentes , capaces de aprender dependencias a largo plazo. Ellas tambi\u00e9n tienen una estructura como cadena, pero el m\u00f3dulo de repetici\u00f3n tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, tiene cuatro, que interact\u00faan de una manera especial permitiendo tener una memoria a m\u00e1s largo plazo.   Para m\u00e1s informaci\u00f3n sobre diferentes arquitecturas de  redes neuronales  pueden visitar el siguiente art\u00edculo de  wikipedia .",
            "title": "Redes neuronales recurrentes"
        },
        {
            "location": "/deeplearning/#logros-del-deep-learning",
            "text": "En los \u00faltimos a\u00f1os el  Deep Learning  ha producido toda una revoluci\u00f3n en el campo del  Machine Learning , con resultados notables en todos los problemas de  percepci\u00f3n , como  ver  y  escuchar , problemas que implican habilidades que parecen muy naturales e intuitivas para los seres humanos, pero que desde hace tiempo se han mostrado dif\u00edciles para las m\u00e1quinas. En particular, el  Deep Learning  ha logrado los siguientes avances, todos ellos en \u00e1reas hist\u00f3ricamente dif\u00edciles del  Machine Learning .   Un nivel casi humano para la clasificaci\u00f3n de im\u00e1genes.  Un nivel casi humano para el reconocimiento del lenguaje hablado.  Un nivel casi humano en el reconocimiento de escritura.  Grandes mejoras en traducciones de lenguas.  Grandes mejoras en conversaciones  text-to-speech .  Asistentes digitales como Google Now o Siri.  Un nivel casi humano en autos aut\u00f3nomos.  Mejores resultados de b\u00fasqueda en la web.  Grandes mejoras para responder preguntas en lenguaje natural.  Alcanzado Nivel maestro (superior al humano) en varios juegos.   En muchos sentidos, el  Deep Learning  todav\u00eda sigue siendo un campo misterioso para explorar, por lo que seguramente veremos nuevos avances en nuevas \u00e1reas utilizando estas t\u00e9cnicas. Tal vez alg\u00fan d\u00eda el  Deep Learning  ayuda a los seres humanos a hacer ciencia, desarrollar software y mucho m\u00e1s.",
            "title": "Logros del Deep Learning "
        },
        {
            "location": "/deeplearning/#por-que-estos-sorprendentes-resultados-surgen-ahora",
            "text": "Muchos de los conceptos del  Deep Learning  se desarrollaron en los a\u00f1os 80s y 90s, algunos incluso mucho antes. Sin embargo, los primeros resultados exitosos del  Deep Learning  surgieron en los \u00faltimos 5 a\u00f1os. \u00bfqu\u00e9 fue lo que cambio para lograr la popularidad y \u00e9xito de los modelos basados en  Deep Learning  en estos \u00faltimos a\u00f1os?   Si bien existen m\u00faltiples factores para explicar esta  revoluci\u00f3n  del  Deep Learning , los dos principales componentes parecen ser la  disponibilidad de masivos vol\u00famenes de datos , lo que actualmente se conoce bajo el nombre de  Big Data ; y el  progreso en el poder de computo , especialmente gracias a los  GPUs . Entonces, dentro de los factores que explican esta popularidad de los modelos de  Deep Learning  podemos encontrar:    La disponibilidad de conjuntos de datos enormes y de buena calidad . Gracias a la  revoluci\u00f3n digital en que nos encontramos, podemos generar conjuntos de datos enormes con los cuales alimentar a los algoritmos de  Deep Learning , los cuales necesitan de muchos datos para poder  generalizar .    Computaci\u00f3n paralela masiva con  GPUs . En l\u00edneas generales, los modelos de  redes neuronales  no son m\u00e1s que complicados c\u00e1lculos num\u00e9ricos que se realizan en paralelo. Gracias al desarrollo de los  GPUs  estos c\u00e1lculos ahora se pueden realizar en forma mucho m\u00e1s r\u00e1pida, permitiendo que podamos entrenar modelos m\u00e1s profundos y grandes.     Funciones de activaci\u00f3n amigables para  Backpropagation . La  progaci\u00f3n hacia atr\u00e1s  o  Backpropagation  es el algoritmo fundamental que hace funcionar a las  redes neuronales ; pero la forma en que trabaja implica c\u00e1lculos realmente complicados. La transici\u00f3n desde funciones de activaci\u00f3n como  tanh  o  sigmoid  a funciones como  ReLU  o  SELU  han simplificado estos problemas.     Nuevas arquitecturas . Arquitecturas como  Resnets ,  inception  y  GAN  mantienen el campo actualizado y contin\u00faan aumentando las flexibilidad de los modelos.     Nuevas t\u00e9cnicas de  regularizaci\u00f3n . T\u00e9cnicas como  dropout ,  batch normalization  y  data-augmentation  nos permiten entrenar redes m\u00e1s grandes con menos peligro de  sobreajuste .    Optimizadores m\u00e1s robustos . La  optimizaci\u00f3n  es fundamental para el funcionamiento de las  redes neuronales . Mejoras sobre el tradicional procedimiento de  SGD , como  ADAM  han ayudado a mejorar el rendimiento de los modelos.    Plataformas de software . Herramientas como  TensorFlow ,  Theano ,  Keras ,  CNTK ,  PyTorch ,  Chainer , y  mxnet  nos permiten crear prototipos en forma m\u00e1s r\u00e1pida y trabajar con  GPUs  sin tantas complicaciones. Nos permiten enfocarnos en la estructura del modelo sin tener que preocuparnos por los detalles de m\u00e1s bajo nivel.    Otra raz\u00f3n por la que el  Deep Learning  ha tenido tanta repercusi\u00f3n \u00faltimamente adem\u00e1s de ofrecer un mejor rendimiento en muchos problemas; es que el  Deep Learning  esta haciendo la resoluci\u00f3n de problemas mucho m\u00e1s f\u00e1cil, ya que automatiza completamente lo que sol\u00eda ser uno de los pasos m\u00e1s dif\u00edciles y cruciales en el flujo de trabajo de  Machine Learning : la  ingenier\u00eda de atributos . Antes del  Deep Learning , para poder entrenar un modelo, primero deb\u00edamos refinar las  entradas  para adaptarlas al tipo de transformaci\u00f3n del modelo; ten\u00edamos que cuidadosamente  seleccionar los atributos  m\u00e1s representativos y desechar los poco informativos. El  Deep Learning , en cambio, automatiza este proceso; aprendemos todos los atributos de una sola pasada y el mismo modelo se encarga de adaptarse y quedarse con lo m\u00e1s representativo.",
            "title": "\u00bfPor qu\u00e9 estos sorprendentes resultados surgen ahora?  "
        },
        {
            "location": "/deeplearning/#como-mantenerse-actualizado-en-el-campo-de-deep-learning",
            "text": "El campo del  Deep Learning  se mueve muy rapidamente, con varios  papers  que se publican por mes; por tal motivo, mantenerse actualizado con las \u00faltimas tendencias del campo puede ser bastante complicado. Algunos consejos pueden ser:    Estarse atento a las publicaciones en  arxiv , especialmente a la secci\u00f3n de  machine learning . La mayor\u00eda de los  papers  m\u00e1s relevantes, los vamos a poder encontrar en esa plataforma.    Seguir el blog de  keras  en el cual podemos encontrar como implementar varios modelos utilizando esta genial librer\u00eda.    Seguir el blog de  openai  en d\u00f3nde detallan las investigaciones que van realizando, especialmente trabajando con  GANs .    Seguir el blog de  Google research ; en d\u00f3nde se viene haciendo bastante foco en los modelos de  Deep Learning .    Utilizar la secci\u00f3n de Machine Learning de  reddit .    Suscribirse al podcast  Talking machines ; en d\u00f3nde se entrevista a los principales exponentes del campo de la  inteligencia artificial .    Por \u00faltimo, obviamente estar atentos a las  publicaciones que se realizan en  IAAR .",
            "title": "\u00bfC\u00f3mo mantenerse actualizado en el campo de Deep Learning? "
        },
        {
            "location": "/ML/",
            "text": "Introducci\u00f3n al Machine Learning\n\n\n\n\nUna de las ramas de estudio que cada vez esta ganando m\u00e1s popularidad dentro de las \nciencias de la computaci\u00f3n\n es el \naprendizaje autom\u00e1tico\n o \nMachine Learning\n. Muchos de los servicios que utilizamos en nuestro d\u00eda a d\u00eda como google, gmail, netflix, spotify o amazon se valen de las herramientas que les brinda el \nMachine Learning\n para alcanzar un servicio cada vez m\u00e1s personalizado y lograr as\u00ed ventajas competitivas sobre sus rivales. \n\n\n\u00bfQu\u00e9 es Machine Learning?\n\n\nPero, \u00bfqu\u00e9 es exactamente \nMachine Learning\n?. El \nMachine Learning\n es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del \nMachine Learning\n es \ngeneralizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada\n. El ejemplo m\u00e1s t\u00edpico donde podemos ver el uso del \nMachine Learning\n es en el filtrado de los correo basura o spam. Mediante la observaci\u00f3n de miles de correos electr\u00f3nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos. El \nMachine Learning\n combina conceptos y t\u00e9cnicas de diferentes \u00e1reas del conocimiento, como las \nmatem\u00e1ticas\n, \nestad\u00edsticas\n y las \nciencias de la computaci\u00f3n\n; por tal motivo, hay muchas maneras de aprender la disciplina.\n\n\nTipos de Machine Learning\n\n\nEl \nMachine Learning\n tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito, an\u00e1lisis del mercado de valores, clasificaci\u00f3n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob\u00f3tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distingir los distintos tipos de problemas de \nMachine Learning\n con los que nos podemos encontrar.\n\n\nAprendizaje supervisado\n\n\nEn los problemas de \naprendizaje supervisado\n se ense\u00f1a o entrena al \nalgoritmo\n a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos m\u00e1s el \nalgoritmo\n puede aprender sobre el tema. Una vez conclu\u00eddo el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el \nalgoritmo\n de aprendizaje utiliza la experiencia pasada que adquiri\u00f3 durante la etapa de entrenamiento para predecir un resultado. Esto es similar al m\u00e9todo de aprendizaje que se utiliza en las escuelas, donde se nos ense\u00f1an problemas y las formas de resolverlos, para que luego podamos aplicar los mismos m\u00e9todos en situaciones similares.\n\n\nAprendizaje no supervisado\n\n\nEn los problemas de \naprendizaje no supervisado\n el \nalgoritmo\n es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al \nalgoritmo\n lo que representan los datos. La idea es que el \nalgoritmo\n pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos. El \naprendizaje no supervisado\n es similar al m\u00e9todo que utilizamos para aprender a hablar cuando somos bebes, en un principio escuchamos hablar a nuestros padres y no entendemos nada; pero a medida que vamos escuchando miles de conversaciones, nuestro cerebro comenzar\u00e1 a formar un modelo sobre c\u00f3mo funciona el lenguaje y comenzaremos a reconocer patrones y a esperar ciertos sonidos. \n\n\nAprendizaje por refuerzo\n\n\nEn los problemas de aprendizaje por refuerzo, el \nalgoritmo\n aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m\u00e1s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar\u00e1 creando una estrategia ganadora.\n\n\nSobreajuste\n\n\nComo mencionamos cuando definimos al \nMachine Learning\n, la idea fundamental es encontrar patrones que podamos generalizar para luego poder aplicar esta generalizaci\u00f3n sobre los casos que todav\u00eda no hemos observado y realizar predicciones. Pero tambi\u00e9n puede ocurrir que durante el entrenamiento solo descubramos casualidades en los datos que se parecen a patrones interesantes, pero que no generalicen. Esto es lo que se conoce con el nombre de \nsobreajuste\n o sobreentrenamiento\n.\n\n\nEl \nsobreajuste\n es la tendencia que tienen la mayor\u00eda de los \nalgoritmos\n de \nMachine Learning\n a ajustarse a unas caracter\u00edsticas muy espec\u00edficas de los datos de entrenamiento que no tienen relaci\u00f3n causal con la \nfunci\u00f3n objetivo\n que estamos buscando para generalizar. El ejemplo m\u00e1s extremo de un modelo \nsobreajustado\n es un modelo que solo memoriza las respuestas correctas; este modelo al ser utilizado con datos que nunca antes ha visto va a tener un rendimiento azaroso, ya que nunca logr\u00f3 generalizar un patr\u00f3n para predecir.\n\n\nComo evitar el sobreajuste\n\n\nComo mencionamos anteriormente, todos los modelos de \nMachine Learning\n tienen tendencia al \nsobreajuste\n; es por esto que debemos aprender a convivir con el mismo y tratar de tomar medidas preventivas para reducirlo lo m\u00e1s posible. Las dos principales estrategias para lidiar son el \nsobreajuste\n son: la \nretenci\u00f3n de datos\n y la \nvalidaci\u00f3n cruzada\n.\n\n\nEn el primer caso, la idea es dividir nuestro \nconjunto de datos\n, en uno o varios conjuntos de entrenamiento y otro/s conjuntos de evaluaci\u00f3n. Es decir, que no le vamos a pasar todos nuestros datos al \nalgoritmo\n durante el entrenamiento, sino que vamos a \nretener\n una parte de los datos de entrenamiento para realizar una evaluaci\u00f3n de la efectividad del modelo. Con esto lo que buscamos es evitar que los mismos datos que usamos para entrenar sean los mismos que utilizamos para evaluar. De esta forma vamos a poder analizar con m\u00e1s precisi\u00f3n como el modelo se va comportando a medida que m\u00e1s lo vamos entrenando y poder detectar el punto cr\u00edtico en el que el modelo deja de generalizar y comienza a \nsobreajustarse\n a los datos de entrenamiento.\n\n\nLa \nvalidaci\u00f3n cruzada\n es un procedimiento m\u00e1s sofisticado que el anterior. En lugar de solo obtener una simple estimaci\u00f3n de la efectividad de la \ngeneralizaci\u00f3n\n; la idea es realizar un an\u00e1lisis estad\u00edstico para obtener otras medidas del rendimiento estimado, como la media y la varianza, y as\u00ed poder entender c\u00f3mo se espera que el rendimiento var\u00ede a trav\u00e9s de los distintos conjuntos de datos. Esta variaci\u00f3n es fundamental para la evaluaci\u00f3n de la confianza en la estimaci\u00f3n del rendimiento.\nLa \nvalidaci\u00f3n cruzada\n tambi\u00e9n hace un mejor uso de un conjunto de datos limitado; ya que a diferencia de la simple divisi\u00f3n de los datos en uno el entrenamiento y otro de evaluaci\u00f3n; la \nvalidaci\u00f3n cruzada\n calcula sus estimaciones sobre todo el \nconjunto de datos\n mediante la realizaci\u00f3n de m\u00faltiples divisiones e intercambios sistem\u00e1ticos entre datos de entrenamiento y datos de evaluaci\u00f3n.\n\n\nPasos para construir un modelo de machine learning\n\n\nConstruir un modelo de \nMachine Learning\n, no se reduce solo a utilizar un \nalgoritmo\n de aprendizaje o utilizar una librer\u00eda de \nMachine Learning\n; sino que es todo un proceso que suele involucrar los siguientes pasos:\n\n\n\n\n\n\nRecolectar los datos\n. Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una \nAPI\n o desde una base de datos. Podemos tambi\u00e9n utilizar otros dispositivos que recolectan los datos por nosotros; o utilizar datos que son de dominio p\u00fablico. El n\u00famero de opciones que tenemos para recolectar datos no tiene fin!. Este paso parece obvio, pero es uno de los que m\u00e1s complicaciones trae y m\u00e1s tiempo consume. \n\n\n\n\n\n\nPreprocesar los datos\n. Una vez que tenemos los datos, tenemos que asegurarnos que tiene el formato correcto para nutrir nuestro \nalgoritmo\n de aprendizaje. Es pr\u00e1cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m\u00e1s sencillo que el paso anterior.\n\n\n\n\n\n\nExplorar los datos\n. Una vez que ya tenemos los datos y est\u00e1n con el formato correcto, podemos realizar un pre an\u00e1lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg\u00fan patr\u00f3n en los mismos que nos facilite la construcci\u00f3n del modelo. En esta etapa suelen ser de mucha utilidad las medidas estad\u00edsticas y los gr\u00e1ficos en 2 y 3 dimensiones para tener una idea visual de como se comportan nuestros datos. En este punto podemos detectar \nvalores at\u00edpicos\n que debamos descartar; o encontrar las caracter\u00edsticas que m\u00e1s influencia tienen para realizar una predicci\u00f3n.\n\n\n\n\n\n\nEntrenar el \nalgoritmo\n. Aqu\u00ed es donde comenzamos a utilizar las t\u00e9cnicas de \nMachine Learning\n realmente. En esta etapa nutrimos al o los \nalgoritmos\n de aprendizaje con los datos que venimos procesando en las etapas anteriores. La idea es que los \nalgoritmos\n puedan extraer informaci\u00f3n \u00fatil de los datos que le pasamos para luego poder hacer predicciones. \n\n\n\n\n\n\nEvaluar el \nalgoritmo\n. En esta etapa ponemos a prueba la informaci\u00f3n o conocimiento que el \nalgoritmo\n obtuvo del entrenamiento del paso anterior. Evaluamos que tan preciso es el algoritmo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el \nalgoritmo\n cambiando algunos par\u00e1metros hasta lograr un rendimiento aceptable.  \n\n\n\n\n\n\nUtilizar el modelo\n. En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu\u00ed tambi\u00e9n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores. \n\n\n\n\n\n\nLibrer\u00edas de Python para machine learning\n\n\nComo siempre me gusta comentar, una de las grandes ventajas que ofrece \nPython\n sobre otros lenguajes de programaci\u00f3n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer\u00edas de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de \nMachine Learning\n, las principales librer\u00edas que podemos utilizar son: \n\n\nScikit-Learn\n\n\nScikit-learn\n es la principal librer\u00eda que existe para trabajar con \nMachine Learning\n, incluye la implementaci\u00f3n de un gran n\u00famero de \nalgoritmos\n de aprendizaje. La podemos utilizar para \nclasificaciones\n, \nextraccion de caracter\u00edsticas\n, \nregresiones\n, \nagrupaciones\n, \nreducci\u00f3n de dimensiones\n, \nselecci\u00f3n de modelos\n, o \npreprocesamiento\n. Posee una \nAPI\n que es consistente en todos los modelos y se integra muy bien con el resto de los paquetes cient\u00edficos que ofrece \nPython\n. Esta librer\u00eda tambi\u00e9n nos facilita las tareas de evaluaci\u00f3n, diagnostico y \nvalidaciones cruzadas\n ya que nos proporciona varios m\u00e9todos de f\u00e1brica para poder realizar estas tareas en forma muy simple. \n\n\nStatsmodels\n\n\nStatsmodels\n es otra gran librer\u00eda que hace foco en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios. Al igual que \nScikit-learn\n, tambi\u00e9n se integra muy bien con el resto de los paquetes cientificos de \nPython\n. Si deseamos ajustar modelos lineales, hacer una an\u00e1lisis estad\u00edstico, o tal vez un poco de modelado predictivo, entonces \nStatsmodels\n es la librer\u00eda ideal. Las pruebas estad\u00edsticas que ofrece son bastante amplias y abarcan tareas de validaci\u00f3n para la mayor\u00eda de los casos. \n\n\nPyMC\n\n\npyMC\n es un m\u00f3dulo de \nPython\n que implementa modelos estad\u00edsticos bayesianos, incluyendo la \ncadena de Markov Monte Carlo(MCMC)\n. \npyMC\n  ofrece funcionalidades para hacer el an\u00e1lisis bayesiano lo mas simple posible. Incluye los modelos \nbayesianos\n,  distribuciones estad\u00edsticas y herramientas de diagnostico  para la covarianza de los modelos. Si queremos realizar un an\u00e1lisis \nbayesiano\n esta es sin duda la librer\u00eda a utilizar.\n\n\nNTLK\n\n\nNLTK\n es la librer\u00eda l\u00edder para el procesamiento del lenguaje natural o \nNLP\n por sus siglas en ingl\u00e9s. Proporciona interfaces f\u00e1ciles de usar a m\u00e1s de 50 cuerpos y recursos l\u00e9xicos, como \nWordNet\n, junto con un conjunto de bibliotecas de procesamiento de texto para la clasificaci\u00f3n, tokenizaci\u00f3n, el etiquetado, el an\u00e1lisis y el razonamiento sem\u00e1ntico. \n\n\nObviamente, aqu\u00ed solo estoy listando unas pocas de las muchas librer\u00edas que existen en \nPython\n para trabajar con problemas de \nMachine Learning\n, los invito a realizar su propia investigaci\u00f3n sobre el tema.\n\n\nAlgoritmos m\u00e1s utilizados\n\n\nLos \nalgoritmos\n  que m\u00e1s se suelen utilizar en los problemas de \nMachine Learning\n son los siguientes:\n\n\n\n\nRegresi\u00f3n Lineal\n\n\nRegresi\u00f3n Log\u00edstica\n\n\nArboles de Decision\n\n\nRandom Forest\n\n\nSVM\n o M\u00e1quinas de vectores de soporte.\n\n\nKNN\n o K vecinos m\u00e1s cercanos.\n\n\nK-means\n\n\n\n\nTodos ellos se pueden aplicar a casi cualquier problema de datos y obviamente estan todos implementados por la excelente librer\u00eda de \nPython\n, \nScikit-learn\n. Veamos algunos ejemplos de ellos.\n\n\nRegresi\u00f3n Lineal\n\n\nSe utiliza para estimar los valores reales (costo de las viviendas, el n\u00famero de llamadas, ventas totales, etc.) basados en variables continuas. La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar una mejor l\u00ednea recta con respecto a los puntos. Esta l\u00ednea de mejor ajuste se conoce como l\u00ednea de regresi\u00f3n y esta representada por la siguiente ecuaci\u00f3n lineal:\n\n\n$$Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{n}X_{n}$$\n\n\nVeamos un peque\u00f1o ejemplo de como se implementa en \nPython\n. En este ejemplo voy a utilizar el dataset Boston que ya viene junto con \nScikit-learn\n y es ideal para practicar con \nRegresiones Lineales\n; el mismo contiene precios de casas de varias \u00e1reas de la ciudad de Boston. \n\n\n# importando pandas, numpy y matplotlib\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# importando los datasets de sklearn\nfrom sklearn import datasets\n\nboston = datasets.load_boston()\nboston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\nboston_df['TARGET'] = boston.target\nboston_df.head() # estructura de nuestro dataset.\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nCRIM\n\n      \nZN\n\n      \nINDUS\n\n      \nCHAS\n\n      \nNOX\n\n      \nRM\n\n      \nAGE\n\n      \nDIS\n\n      \nRAD\n\n      \nTAX\n\n      \nPTRATIO\n\n      \nB\n\n      \nLSTAT\n\n      \nTARGET\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0.00632\n\n      \n18.0\n\n      \n2.31\n\n      \n0.0\n\n      \n0.538\n\n      \n6.575\n\n      \n65.2\n\n      \n4.0900\n\n      \n1.0\n\n      \n296.0\n\n      \n15.3\n\n      \n396.90\n\n      \n4.98\n\n      \n24.0\n\n    \n\n    \n\n      \n1\n\n      \n0.02731\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n6.421\n\n      \n78.9\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n396.90\n\n      \n9.14\n\n      \n21.6\n\n    \n\n    \n\n      \n2\n\n      \n0.02729\n\n      \n0.0\n\n      \n7.07\n\n      \n0.0\n\n      \n0.469\n\n      \n7.185\n\n      \n61.1\n\n      \n4.9671\n\n      \n2.0\n\n      \n242.0\n\n      \n17.8\n\n      \n392.83\n\n      \n4.03\n\n      \n34.7\n\n    \n\n    \n\n      \n3\n\n      \n0.03237\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n6.998\n\n      \n45.8\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n394.63\n\n      \n2.94\n\n      \n33.4\n\n    \n\n    \n\n      \n4\n\n      \n0.06905\n\n      \n0.0\n\n      \n2.18\n\n      \n0.0\n\n      \n0.458\n\n      \n7.147\n\n      \n54.2\n\n      \n6.0622\n\n      \n3.0\n\n      \n222.0\n\n      \n18.7\n\n      \n396.90\n\n      \n5.33\n\n      \n36.2\n\n    \n\n  \n\n\n\n\n\n# importando el modelo de regresi\u00f3n lineal\nfrom sklearn.linear_model import LinearRegression\n\nrl = LinearRegression() # Creando el modelo.\nrl.fit(boston.data, boston.target) # ajustando el modelo\n\n# haciendo las predicciones\npredicciones = rl.predict(boston.data)\npredicciones_df = pd.DataFrame(predicciones, columns=['Pred'])\npredicciones_df.head() # predicciones de las primeras 5 lineas\n\n\n\n\n\n  \n\n    \n\n      \n\n      \nPred\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n30.008213\n\n    \n\n    \n\n      \n1\n\n      \n25.029861\n\n    \n\n    \n\n      \n2\n\n      \n30.570232\n\n    \n\n    \n\n      \n3\n\n      \n28.608141\n\n    \n\n    \n\n      \n4\n\n      \n27.942882\n\n    \n\n  \n\n\n\n\n\n# Calculando el desvio\nnp.mean(boston.target - predicciones)\n\n\n\n\nComo podemos ver, el desv\u00edo del modelo es peque\u00f1o, por lo que sus resultados para este ejemplo son bastante confiables.\n\n\nRegresi\u00f3n Log\u00edstica\n\n\nLos modelos lineales, tambi\u00e9n pueden ser utilizados para clasificaciones; es decir, que primero ajustamos el modelo lineal a la probabilidad de que una cierta clase o categor\u00eda ocurra y, a luego, utilizamos una funci\u00f3n para crear un umbral en el cual especificamos el resultado de una de estas clases o categor\u00edas. La funci\u00f3n que utiliza este modelo, no es ni m\u00e1s ni menos que la funci\u00f3n log\u00edstica.\n\n\n$$f(x) = \\frac{1}{1 + e^{-1}}$$\n\n\nVeamos, aqu\u00ed tambi\u00e9n un peque\u00f1o ejemplo en \nPython\n.\n\n\n# Creando un dataset de ejemplo \nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1000, n_features=4)\n\n# Importando el modelo\nfrom sklearn.linear_model import LogisticRegression\n\nrlog = LogisticRegression() # Creando el modelo\n\n# Dividiendo el dataset en entrenamiento y evaluacion\nX_entrenamiento = X[:-200]\nX_evaluacion = X[-200:]\ny_entrenamiento = y[:-200]\ny_evaluacion = y[-200:]\n\nrlog.fit(X_entrenamiento, y_entrenamiento) #ajustando el modelo\n\n# Realizando las predicciones\ny_predic_entrenamiento = rlog.predict(X_entrenamiento) \ny_predic_evaluacion = rlog.predict(X_evaluacion)\n\n# Verificando la exactitud del modelo\nentrenamiento = (y_predic_entrenamiento == y_entrenamiento).sum().astype(float) / y_entrenamiento.shape[0]\nprint(\"sobre datos de entrenamiento: {0:.2f}\".format(entrenamiento))\nevaluacion = (y_predic_evaluacion == y_evaluacion).sum().astype(float) / y_evaluacion.shape[0]\nprint(\"sobre datos de evaluaci\u00f3n: {0:.2f}\".format(evaluacion))\n\nsobre datos de entrenamiento: 0.95\nsobre datos de evaluaci\u00f3n: 0.94\n\n\n\n\nComo podemos ver en este ejemplo tambi\u00e9n nuestro modelo tiene bastante precisi\u00f3n clasificando las categor\u00edas de nuestro dataset.\n\n\nArboles de decisi\u00f3n\n\n\nLos \nArboles de Decision\n son diagramas con construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema.\nLos \nArboles de Decision\n est\u00e1n compuestos por nodos interiores, nodos terminales y ramas que emanan de los nodos interiores. Cada nodo interior en el \u00e1rbol contiene una prueba de un atributo, y cada rama representa un valor distinto del atributo. Siguiendo las ramas desde el nodo ra\u00edz hacia abajo, cada ruta finalmente termina en un nodo terminal creando una segmentaci\u00f3n de los datos. Veamos aqu\u00ed tambi\u00e9n un peque\u00f1o ejemplo en \nPython\n.\n\n\n# Creando un dataset de ejemplo\nX, y = datasets.make_classification(1000, 20, n_informative=3)\n\n# Importando el arbol de decisi\u00f3n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nad = DecisionTreeClassifier(criterion='entropy', max_depth=5) # Creando el modelo\nad.fit(X, y) # Ajustando el modelo\n\n#generando archivo para graficar el arbol\nwith open(\"mi_arbol.dot\", 'w') as archivo_dot:\n    tree.export_graphviz(ad, out_file = archivo_dot)\n\n# utilizando el lenguaje dot para graficar el arbol.\n!dot -Tjpeg mi_arbol.dot -o arbol_decision.jpeg\n\n\n\n\nLuego de usar el lenguaje \ndot\n para convertir nuestro arbol a formato jpeg, ya podemos ver la imagen del mismo.\n\n\n\n\n# verificando la precisi\u00f3n\nprint(\"precisi\u00f3n del modelo: {0: .2f}\".format((y == ad.predict(X)).mean()))\n\nprecisi\u00f3n del modelo:  0.94\n\n\n\n\nEn este ejemplo, nuestro \u00e1rbol tiene una precisi\u00f3n del 89%. Tener en cuenta que los \nArboles de Decision\n tienen tendencia al \nsobreajuste\n.\n\n\nRandom Forest\n\n\nEn lugar de utilizar solo un arbol para decidir, \u00bfpor qu\u00e9 no utilizar todo un bosque?!!. Esta es la idea central detr\u00e1s del \nalgoritmo\n de \nRandom Forest\n. Tarbaja construyendo una gran cantidad de \narboles de decision\n muy poco profundos, y luego toma la clase que\ncada \u00e1rbol eligi\u00f3. Esta idea es muy poderosa en \nMachine Learning\n. Si tenemos en cuenta que un sencillo clasificador entrenado podr\u00eda tener s\u00f3lo el 60 por ciento de precisi\u00f3n, podemos entrenar un mont\u00f3n de clasificadores que sean por lo general acertados y luego podemos utilizar la sabidur\u00eda de todos los aprendices juntos.\nCon \nPython\n los podemos utilizar de la siguiente manera:\n\n\n# Creando un dataset de ejemplo\nX, y = datasets.make_classification(1000)\n\n# Importando el random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier() # Creando el modelo\nrf.fit(X, y) # Ajustando el modelo\n\n# verificando la precisi\u00f3n\nprint(\"precisi\u00f3n del modelo: {0: .2f}\".format((y == rf.predict(X)).mean()))\n\nprecisi\u00f3n del modelo:  1.00\n\n\n\n\nSVM o M\u00e1quinas de vectores de soporte\n\n\nLa idea detr\u00e1s de \nSVM\n es encontrar un plano que separe los grupos dentro de los datos de la mejor forma posible. Aqu\u00ed, la separaci\u00f3n significa que la elecci\u00f3n\ndel plano maximiza el margen entre los puntos m\u00e1s cercanos en el plano; \u00e9stos puntos se denominan vectores de soporte. Pasemos al ejemplo.\n\n\n# importanto SVM\nfrom sklearn import svm\n\n# importando el dataset iris\niris = datasets.load_iris()\nX = iris.data[:, :2]  # solo tomamos las primeras 2 caracter\u00edsticas\ny = iris.target\n\nh = .02  # tama\u00f1o de la malla del grafico\n\n# Creando el SVM con sus diferentes m\u00e9todos\nC = 1.0  # parametro de regulacion SVM \nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\nlin_svc = svm.LinearSVC(C=C).fit(X, y)\n\n# crear el area para graficar\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# titulos de los graficos\ntitles = ['SVC con el motor lineal',\n          'LinearSVC',\n          'SVC con el motor RBF',\n          'SVC con el motor polinomial']\n\n\nfor i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n    # Realizando el gr\u00e1fico, se le asigna un color a cada punto\n    plt.subplot(2, 2, i + 1)\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n    # Graficando tambien los puntos de datos\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('largo del petalo')\n    plt.ylabel('ancho del petalo')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(titles[i])\n\nplt.show()\n\n\n\n\n\n\nKNN o k vecinos m\u00e1s cercanos\n\n\nEste es un m\u00e9todo de clasificaci\u00f3n no param\u00e9trico, que estima el valor de la probabilidad a posteriori de que un elemento $x$ pertenezca a una clase en particular a partir de la informaci\u00f3n proporcionada por el conjunto de prototipos.\nLa regresi\u00f3n \nKNN\n se calcula simplemente tomando el promedio del punto k m\u00e1s cercano al punto que se est\u00e1 probando. \n\n\n\n# Creando el dataset iris\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# importando KNN \nfrom sklearn.neighbors import KNeighborsRegressor\n\nknnr = KNeighborsRegressor(n_neighbors=10) # Creando el modelo con 10 vecinos\nknnr.fit(X, y) # Ajustando el modelo\n\n# Verificando el error medio del modelo\nprint(\"El error medio del modelo es: {:.2f}\".format(np.power(y - knnr.predict(X),\n2).mean()))\n\nEl error medio del modelo es: 0.02\n\n\n\n\nK-means\n\n\nK-means\n es probablemente uno de los algoritmos de agrupamiento m\u00e1s conocidos y, en un sentido m\u00e1s amplio, una de las t\u00e9cnicas de aprendizaje no supervisado m\u00e1s conocidas.\n\nK-means\n es en realidad un \nalgoritmo\n muy simple que funciona para reducir al m\u00ednimo la suma de las distancias cuadradas desde la media dentro del agrupamiento. Para hacer esto establece primero un n\u00famero previamente especificado de conglomerados, K, y luego va asignando cada observaci\u00f3n a la agrupaci\u00f3n m\u00e1s cercana de acuerdo a su media. Veamos el ejemplo\n\n\n# Creando el dataset\ngrupos, pos_correcta = datasets.make_blobs(1000, centers=3,\ncluster_std=1.75)\n\n# Graficando los grupos de datos\nf, ax = plt.subplots(figsize=(7, 5))\ncolores = ['r', 'g', 'b']\n\nfor i in range(3):\n    p = grupos[pos_correcta == i]\n    ax.scatter(p[:,0], p[:,1], c=colores[i],\n               label=\"Grupo {}\".format(i))\n\nax.set_title(\"Agrupamiento perfecto\")\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n# importando KMeans\nfrom sklearn.cluster import KMeans\n\n# Creando el modelo\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(grupos) # Ajustando el modelo\n\n# verificando los centros de los grupos\nkmeans.cluster_centers_\n\n# Graficando segun modelo\nf, ax = plt.subplots(figsize=(7, 5))\ncolores = ['r', 'g', 'b']\n\nfor i in range(3):\n    p = grupos[pos_correcta == i]\n    ax.scatter(p[:,0], p[:,1], c=colores[i],\n               label=\"Grupo {}\".format(i))\n\nax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n           s=100, color='black', label='Centros')\n\nax.set_title(\"Agrupamiento s/modelo\")\nax.legend()\n\nplt.show()",
            "title": "Machine Learning"
        },
        {
            "location": "/ML/#introduccion-al-machine-learning",
            "text": "Una de las ramas de estudio que cada vez esta ganando m\u00e1s popularidad dentro de las  ciencias de la computaci\u00f3n  es el  aprendizaje autom\u00e1tico  o  Machine Learning . Muchos de los servicios que utilizamos en nuestro d\u00eda a d\u00eda como google, gmail, netflix, spotify o amazon se valen de las herramientas que les brinda el  Machine Learning  para alcanzar un servicio cada vez m\u00e1s personalizado y lograr as\u00ed ventajas competitivas sobre sus rivales.",
            "title": "Introducci\u00f3n al Machine Learning"
        },
        {
            "location": "/ML/#que-es-machine-learning",
            "text": "Pero, \u00bfqu\u00e9 es exactamente  Machine Learning ?. El  Machine Learning  es el dise\u00f1o y estudio de las herramientas inform\u00e1ticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del  Machine Learning  es  generalizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada . El ejemplo m\u00e1s t\u00edpico donde podemos ver el uso del  Machine Learning  es en el filtrado de los correo basura o spam. Mediante la observaci\u00f3n de miles de correos electr\u00f3nicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos. El  Machine Learning  combina conceptos y t\u00e9cnicas de diferentes \u00e1reas del conocimiento, como las  matem\u00e1ticas ,  estad\u00edsticas  y las  ciencias de la computaci\u00f3n ; por tal motivo, hay muchas maneras de aprender la disciplina.",
            "title": "\u00bfQu\u00e9 es Machine Learning?"
        },
        {
            "location": "/ML/#tipos-de-machine-learning",
            "text": "El  Machine Learning  tiene una amplia gama de aplicaciones, incluyendo motores de b\u00fasqueda, diagn\u00f3sticos m\u00e9dicos, detecci\u00f3n de fraude en el uso de tarjetas de cr\u00e9dito, an\u00e1lisis del mercado de valores, clasificaci\u00f3n de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y rob\u00f3tica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distingir los distintos tipos de problemas de  Machine Learning  con los que nos podemos encontrar.",
            "title": "Tipos de Machine Learning"
        },
        {
            "location": "/ML/#aprendizaje-supervisado",
            "text": "En los problemas de  aprendizaje supervisado  se ense\u00f1a o entrena al  algoritmo  a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos m\u00e1s el  algoritmo  puede aprender sobre el tema. Una vez conclu\u00eddo el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el  algoritmo  de aprendizaje utiliza la experiencia pasada que adquiri\u00f3 durante la etapa de entrenamiento para predecir un resultado. Esto es similar al m\u00e9todo de aprendizaje que se utiliza en las escuelas, donde se nos ense\u00f1an problemas y las formas de resolverlos, para que luego podamos aplicar los mismos m\u00e9todos en situaciones similares.",
            "title": "Aprendizaje supervisado"
        },
        {
            "location": "/ML/#aprendizaje-no-supervisado",
            "text": "En los problemas de  aprendizaje no supervisado  el  algoritmo  es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al  algoritmo  lo que representan los datos. La idea es que el  algoritmo  pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos. El  aprendizaje no supervisado  es similar al m\u00e9todo que utilizamos para aprender a hablar cuando somos bebes, en un principio escuchamos hablar a nuestros padres y no entendemos nada; pero a medida que vamos escuchando miles de conversaciones, nuestro cerebro comenzar\u00e1 a formar un modelo sobre c\u00f3mo funciona el lenguaje y comenzaremos a reconocer patrones y a esperar ciertos sonidos.",
            "title": "Aprendizaje no supervisado"
        },
        {
            "location": "/ML/#aprendizaje-por-refuerzo",
            "text": "En los problemas de aprendizaje por refuerzo, el  algoritmo  aprende observando el mundo que le rodea. Su informaci\u00f3n de entrada es el feedback o retroalimentaci\u00f3n que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo m\u00e1s practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminar\u00e1 creando una estrategia ganadora.",
            "title": "Aprendizaje por refuerzo"
        },
        {
            "location": "/ML/#sobreajuste",
            "text": "Como mencionamos cuando definimos al  Machine Learning , la idea fundamental es encontrar patrones que podamos generalizar para luego poder aplicar esta generalizaci\u00f3n sobre los casos que todav\u00eda no hemos observado y realizar predicciones. Pero tambi\u00e9n puede ocurrir que durante el entrenamiento solo descubramos casualidades en los datos que se parecen a patrones interesantes, pero que no generalicen. Esto es lo que se conoce con el nombre de  sobreajuste  o sobreentrenamiento .  El  sobreajuste  es la tendencia que tienen la mayor\u00eda de los  algoritmos  de  Machine Learning  a ajustarse a unas caracter\u00edsticas muy espec\u00edficas de los datos de entrenamiento que no tienen relaci\u00f3n causal con la  funci\u00f3n objetivo  que estamos buscando para generalizar. El ejemplo m\u00e1s extremo de un modelo  sobreajustado  es un modelo que solo memoriza las respuestas correctas; este modelo al ser utilizado con datos que nunca antes ha visto va a tener un rendimiento azaroso, ya que nunca logr\u00f3 generalizar un patr\u00f3n para predecir.",
            "title": "Sobreajuste"
        },
        {
            "location": "/ML/#como-evitar-el-sobreajuste",
            "text": "Como mencionamos anteriormente, todos los modelos de  Machine Learning  tienen tendencia al  sobreajuste ; es por esto que debemos aprender a convivir con el mismo y tratar de tomar medidas preventivas para reducirlo lo m\u00e1s posible. Las dos principales estrategias para lidiar son el  sobreajuste  son: la  retenci\u00f3n de datos  y la  validaci\u00f3n cruzada .  En el primer caso, la idea es dividir nuestro  conjunto de datos , en uno o varios conjuntos de entrenamiento y otro/s conjuntos de evaluaci\u00f3n. Es decir, que no le vamos a pasar todos nuestros datos al  algoritmo  durante el entrenamiento, sino que vamos a  retener  una parte de los datos de entrenamiento para realizar una evaluaci\u00f3n de la efectividad del modelo. Con esto lo que buscamos es evitar que los mismos datos que usamos para entrenar sean los mismos que utilizamos para evaluar. De esta forma vamos a poder analizar con m\u00e1s precisi\u00f3n como el modelo se va comportando a medida que m\u00e1s lo vamos entrenando y poder detectar el punto cr\u00edtico en el que el modelo deja de generalizar y comienza a  sobreajustarse  a los datos de entrenamiento.  La  validaci\u00f3n cruzada  es un procedimiento m\u00e1s sofisticado que el anterior. En lugar de solo obtener una simple estimaci\u00f3n de la efectividad de la  generalizaci\u00f3n ; la idea es realizar un an\u00e1lisis estad\u00edstico para obtener otras medidas del rendimiento estimado, como la media y la varianza, y as\u00ed poder entender c\u00f3mo se espera que el rendimiento var\u00ede a trav\u00e9s de los distintos conjuntos de datos. Esta variaci\u00f3n es fundamental para la evaluaci\u00f3n de la confianza en la estimaci\u00f3n del rendimiento.\nLa  validaci\u00f3n cruzada  tambi\u00e9n hace un mejor uso de un conjunto de datos limitado; ya que a diferencia de la simple divisi\u00f3n de los datos en uno el entrenamiento y otro de evaluaci\u00f3n; la  validaci\u00f3n cruzada  calcula sus estimaciones sobre todo el  conjunto de datos  mediante la realizaci\u00f3n de m\u00faltiples divisiones e intercambios sistem\u00e1ticos entre datos de entrenamiento y datos de evaluaci\u00f3n.",
            "title": "Como evitar el sobreajuste"
        },
        {
            "location": "/ML/#pasos-para-construir-un-modelo-de-machine-learning",
            "text": "Construir un modelo de  Machine Learning , no se reduce solo a utilizar un  algoritmo  de aprendizaje o utilizar una librer\u00eda de  Machine Learning ; sino que es todo un proceso que suele involucrar los siguientes pasos:    Recolectar los datos . Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una  API  o desde una base de datos. Podemos tambi\u00e9n utilizar otros dispositivos que recolectan los datos por nosotros; o utilizar datos que son de dominio p\u00fablico. El n\u00famero de opciones que tenemos para recolectar datos no tiene fin!. Este paso parece obvio, pero es uno de los que m\u00e1s complicaciones trae y m\u00e1s tiempo consume.     Preprocesar los datos . Una vez que tenemos los datos, tenemos que asegurarnos que tiene el formato correcto para nutrir nuestro  algoritmo  de aprendizaje. Es pr\u00e1cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m\u00e1s sencillo que el paso anterior.    Explorar los datos . Una vez que ya tenemos los datos y est\u00e1n con el formato correcto, podemos realizar un pre an\u00e1lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg\u00fan patr\u00f3n en los mismos que nos facilite la construcci\u00f3n del modelo. En esta etapa suelen ser de mucha utilidad las medidas estad\u00edsticas y los gr\u00e1ficos en 2 y 3 dimensiones para tener una idea visual de como se comportan nuestros datos. En este punto podemos detectar  valores at\u00edpicos  que debamos descartar; o encontrar las caracter\u00edsticas que m\u00e1s influencia tienen para realizar una predicci\u00f3n.    Entrenar el  algoritmo . Aqu\u00ed es donde comenzamos a utilizar las t\u00e9cnicas de  Machine Learning  realmente. En esta etapa nutrimos al o los  algoritmos  de aprendizaje con los datos que venimos procesando en las etapas anteriores. La idea es que los  algoritmos  puedan extraer informaci\u00f3n \u00fatil de los datos que le pasamos para luego poder hacer predicciones.     Evaluar el  algoritmo . En esta etapa ponemos a prueba la informaci\u00f3n o conocimiento que el  algoritmo  obtuvo del entrenamiento del paso anterior. Evaluamos que tan preciso es el algoritmo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el  algoritmo  cambiando algunos par\u00e1metros hasta lograr un rendimiento aceptable.      Utilizar el modelo . En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu\u00ed tambi\u00e9n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores.",
            "title": "Pasos para construir un modelo de machine learning"
        },
        {
            "location": "/ML/#librerias-de-python-para-machine-learning",
            "text": "Como siempre me gusta comentar, una de las grandes ventajas que ofrece  Python  sobre otros lenguajes de programaci\u00f3n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer\u00edas de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de  Machine Learning , las principales librer\u00edas que podemos utilizar son:",
            "title": "Librer\u00edas de Python para machine learning"
        },
        {
            "location": "/ML/#scikit-learn",
            "text": "Scikit-learn  es la principal librer\u00eda que existe para trabajar con  Machine Learning , incluye la implementaci\u00f3n de un gran n\u00famero de  algoritmos  de aprendizaje. La podemos utilizar para  clasificaciones ,  extraccion de caracter\u00edsticas ,  regresiones ,  agrupaciones ,  reducci\u00f3n de dimensiones ,  selecci\u00f3n de modelos , o  preprocesamiento . Posee una  API  que es consistente en todos los modelos y se integra muy bien con el resto de los paquetes cient\u00edficos que ofrece  Python . Esta librer\u00eda tambi\u00e9n nos facilita las tareas de evaluaci\u00f3n, diagnostico y  validaciones cruzadas  ya que nos proporciona varios m\u00e9todos de f\u00e1brica para poder realizar estas tareas en forma muy simple.",
            "title": "Scikit-Learn"
        },
        {
            "location": "/ML/#statsmodels",
            "text": "Statsmodels  es otra gran librer\u00eda que hace foco en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios. Al igual que  Scikit-learn , tambi\u00e9n se integra muy bien con el resto de los paquetes cientificos de  Python . Si deseamos ajustar modelos lineales, hacer una an\u00e1lisis estad\u00edstico, o tal vez un poco de modelado predictivo, entonces  Statsmodels  es la librer\u00eda ideal. Las pruebas estad\u00edsticas que ofrece son bastante amplias y abarcan tareas de validaci\u00f3n para la mayor\u00eda de los casos.",
            "title": "Statsmodels"
        },
        {
            "location": "/ML/#pymc",
            "text": "pyMC  es un m\u00f3dulo de  Python  que implementa modelos estad\u00edsticos bayesianos, incluyendo la  cadena de Markov Monte Carlo(MCMC) .  pyMC   ofrece funcionalidades para hacer el an\u00e1lisis bayesiano lo mas simple posible. Incluye los modelos  bayesianos ,  distribuciones estad\u00edsticas y herramientas de diagnostico  para la covarianza de los modelos. Si queremos realizar un an\u00e1lisis  bayesiano  esta es sin duda la librer\u00eda a utilizar.",
            "title": "PyMC"
        },
        {
            "location": "/ML/#ntlk",
            "text": "NLTK  es la librer\u00eda l\u00edder para el procesamiento del lenguaje natural o  NLP  por sus siglas en ingl\u00e9s. Proporciona interfaces f\u00e1ciles de usar a m\u00e1s de 50 cuerpos y recursos l\u00e9xicos, como  WordNet , junto con un conjunto de bibliotecas de procesamiento de texto para la clasificaci\u00f3n, tokenizaci\u00f3n, el etiquetado, el an\u00e1lisis y el razonamiento sem\u00e1ntico.   Obviamente, aqu\u00ed solo estoy listando unas pocas de las muchas librer\u00edas que existen en  Python  para trabajar con problemas de  Machine Learning , los invito a realizar su propia investigaci\u00f3n sobre el tema.",
            "title": "NTLK"
        },
        {
            "location": "/ML/#algoritmos-mas-utilizados",
            "text": "Los  algoritmos   que m\u00e1s se suelen utilizar en los problemas de  Machine Learning  son los siguientes:   Regresi\u00f3n Lineal  Regresi\u00f3n Log\u00edstica  Arboles de Decision  Random Forest  SVM  o M\u00e1quinas de vectores de soporte.  KNN  o K vecinos m\u00e1s cercanos.  K-means   Todos ellos se pueden aplicar a casi cualquier problema de datos y obviamente estan todos implementados por la excelente librer\u00eda de  Python ,  Scikit-learn . Veamos algunos ejemplos de ellos.",
            "title": "Algoritmos m\u00e1s utilizados"
        },
        {
            "location": "/ML/#regresion-lineal",
            "text": "Se utiliza para estimar los valores reales (costo de las viviendas, el n\u00famero de llamadas, ventas totales, etc.) basados en variables continuas. La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar una mejor l\u00ednea recta con respecto a los puntos. Esta l\u00ednea de mejor ajuste se conoce como l\u00ednea de regresi\u00f3n y esta representada por la siguiente ecuaci\u00f3n lineal:  $$Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{n}X_{n}$$  Veamos un peque\u00f1o ejemplo de como se implementa en  Python . En este ejemplo voy a utilizar el dataset Boston que ya viene junto con  Scikit-learn  y es ideal para practicar con  Regresiones Lineales ; el mismo contiene precios de casas de varias \u00e1reas de la ciudad de Boston.   # importando pandas, numpy y matplotlib\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# importando los datasets de sklearn\nfrom sklearn import datasets\n\nboston = datasets.load_boston()\nboston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\nboston_df['TARGET'] = boston.target\nboston_df.head() # estructura de nuestro dataset.  \n   \n     \n       \n       CRIM \n       ZN \n       INDUS \n       CHAS \n       NOX \n       RM \n       AGE \n       DIS \n       RAD \n       TAX \n       PTRATIO \n       B \n       LSTAT \n       TARGET \n     \n   \n   \n     \n       0 \n       0.00632 \n       18.0 \n       2.31 \n       0.0 \n       0.538 \n       6.575 \n       65.2 \n       4.0900 \n       1.0 \n       296.0 \n       15.3 \n       396.90 \n       4.98 \n       24.0 \n     \n     \n       1 \n       0.02731 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       6.421 \n       78.9 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       396.90 \n       9.14 \n       21.6 \n     \n     \n       2 \n       0.02729 \n       0.0 \n       7.07 \n       0.0 \n       0.469 \n       7.185 \n       61.1 \n       4.9671 \n       2.0 \n       242.0 \n       17.8 \n       392.83 \n       4.03 \n       34.7 \n     \n     \n       3 \n       0.03237 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       6.998 \n       45.8 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       394.63 \n       2.94 \n       33.4 \n     \n     \n       4 \n       0.06905 \n       0.0 \n       2.18 \n       0.0 \n       0.458 \n       7.147 \n       54.2 \n       6.0622 \n       3.0 \n       222.0 \n       18.7 \n       396.90 \n       5.33 \n       36.2 \n     \n     # importando el modelo de regresi\u00f3n lineal\nfrom sklearn.linear_model import LinearRegression\n\nrl = LinearRegression() # Creando el modelo.\nrl.fit(boston.data, boston.target) # ajustando el modelo\n\n# haciendo las predicciones\npredicciones = rl.predict(boston.data)\npredicciones_df = pd.DataFrame(predicciones, columns=['Pred'])\npredicciones_df.head() # predicciones de las primeras 5 lineas  \n   \n     \n       \n       Pred \n     \n   \n   \n     \n       0 \n       30.008213 \n     \n     \n       1 \n       25.029861 \n     \n     \n       2 \n       30.570232 \n     \n     \n       3 \n       28.608141 \n     \n     \n       4 \n       27.942882 \n     \n     # Calculando el desvio\nnp.mean(boston.target - predicciones)  Como podemos ver, el desv\u00edo del modelo es peque\u00f1o, por lo que sus resultados para este ejemplo son bastante confiables.",
            "title": "Regresi\u00f3n Lineal"
        },
        {
            "location": "/ML/#regresion-logistica",
            "text": "Los modelos lineales, tambi\u00e9n pueden ser utilizados para clasificaciones; es decir, que primero ajustamos el modelo lineal a la probabilidad de que una cierta clase o categor\u00eda ocurra y, a luego, utilizamos una funci\u00f3n para crear un umbral en el cual especificamos el resultado de una de estas clases o categor\u00edas. La funci\u00f3n que utiliza este modelo, no es ni m\u00e1s ni menos que la funci\u00f3n log\u00edstica.  $$f(x) = \\frac{1}{1 + e^{-1}}$$  Veamos, aqu\u00ed tambi\u00e9n un peque\u00f1o ejemplo en  Python .  # Creando un dataset de ejemplo \nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1000, n_features=4)\n\n# Importando el modelo\nfrom sklearn.linear_model import LogisticRegression\n\nrlog = LogisticRegression() # Creando el modelo\n\n# Dividiendo el dataset en entrenamiento y evaluacion\nX_entrenamiento = X[:-200]\nX_evaluacion = X[-200:]\ny_entrenamiento = y[:-200]\ny_evaluacion = y[-200:]\n\nrlog.fit(X_entrenamiento, y_entrenamiento) #ajustando el modelo\n\n# Realizando las predicciones\ny_predic_entrenamiento = rlog.predict(X_entrenamiento) \ny_predic_evaluacion = rlog.predict(X_evaluacion)\n\n# Verificando la exactitud del modelo\nentrenamiento = (y_predic_entrenamiento == y_entrenamiento).sum().astype(float) / y_entrenamiento.shape[0]\nprint(\"sobre datos de entrenamiento: {0:.2f}\".format(entrenamiento))\nevaluacion = (y_predic_evaluacion == y_evaluacion).sum().astype(float) / y_evaluacion.shape[0]\nprint(\"sobre datos de evaluaci\u00f3n: {0:.2f}\".format(evaluacion))\n\nsobre datos de entrenamiento: 0.95\nsobre datos de evaluaci\u00f3n: 0.94  Como podemos ver en este ejemplo tambi\u00e9n nuestro modelo tiene bastante precisi\u00f3n clasificando las categor\u00edas de nuestro dataset.",
            "title": "Regresi\u00f3n Log\u00edstica"
        },
        {
            "location": "/ML/#arboles-de-decision",
            "text": "Los  Arboles de Decision  son diagramas con construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema.\nLos  Arboles de Decision  est\u00e1n compuestos por nodos interiores, nodos terminales y ramas que emanan de los nodos interiores. Cada nodo interior en el \u00e1rbol contiene una prueba de un atributo, y cada rama representa un valor distinto del atributo. Siguiendo las ramas desde el nodo ra\u00edz hacia abajo, cada ruta finalmente termina en un nodo terminal creando una segmentaci\u00f3n de los datos. Veamos aqu\u00ed tambi\u00e9n un peque\u00f1o ejemplo en  Python .  # Creando un dataset de ejemplo\nX, y = datasets.make_classification(1000, 20, n_informative=3)\n\n# Importando el arbol de decisi\u00f3n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nad = DecisionTreeClassifier(criterion='entropy', max_depth=5) # Creando el modelo\nad.fit(X, y) # Ajustando el modelo\n\n#generando archivo para graficar el arbol\nwith open(\"mi_arbol.dot\", 'w') as archivo_dot:\n    tree.export_graphviz(ad, out_file = archivo_dot)\n\n# utilizando el lenguaje dot para graficar el arbol.\n!dot -Tjpeg mi_arbol.dot -o arbol_decision.jpeg  Luego de usar el lenguaje  dot  para convertir nuestro arbol a formato jpeg, ya podemos ver la imagen del mismo.   # verificando la precisi\u00f3n\nprint(\"precisi\u00f3n del modelo: {0: .2f}\".format((y == ad.predict(X)).mean()))\n\nprecisi\u00f3n del modelo:  0.94  En este ejemplo, nuestro \u00e1rbol tiene una precisi\u00f3n del 89%. Tener en cuenta que los  Arboles de Decision  tienen tendencia al  sobreajuste .",
            "title": "Arboles de decisi\u00f3n"
        },
        {
            "location": "/ML/#random-forest",
            "text": "En lugar de utilizar solo un arbol para decidir, \u00bfpor qu\u00e9 no utilizar todo un bosque?!!. Esta es la idea central detr\u00e1s del  algoritmo  de  Random Forest . Tarbaja construyendo una gran cantidad de  arboles de decision  muy poco profundos, y luego toma la clase que\ncada \u00e1rbol eligi\u00f3. Esta idea es muy poderosa en  Machine Learning . Si tenemos en cuenta que un sencillo clasificador entrenado podr\u00eda tener s\u00f3lo el 60 por ciento de precisi\u00f3n, podemos entrenar un mont\u00f3n de clasificadores que sean por lo general acertados y luego podemos utilizar la sabidur\u00eda de todos los aprendices juntos.\nCon  Python  los podemos utilizar de la siguiente manera:  # Creando un dataset de ejemplo\nX, y = datasets.make_classification(1000)\n\n# Importando el random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier() # Creando el modelo\nrf.fit(X, y) # Ajustando el modelo\n\n# verificando la precisi\u00f3n\nprint(\"precisi\u00f3n del modelo: {0: .2f}\".format((y == rf.predict(X)).mean()))\n\nprecisi\u00f3n del modelo:  1.00",
            "title": "Random Forest"
        },
        {
            "location": "/ML/#svm-o-maquinas-de-vectores-de-soporte",
            "text": "La idea detr\u00e1s de  SVM  es encontrar un plano que separe los grupos dentro de los datos de la mejor forma posible. Aqu\u00ed, la separaci\u00f3n significa que la elecci\u00f3n\ndel plano maximiza el margen entre los puntos m\u00e1s cercanos en el plano; \u00e9stos puntos se denominan vectores de soporte. Pasemos al ejemplo.  # importanto SVM\nfrom sklearn import svm\n\n# importando el dataset iris\niris = datasets.load_iris()\nX = iris.data[:, :2]  # solo tomamos las primeras 2 caracter\u00edsticas\ny = iris.target\n\nh = .02  # tama\u00f1o de la malla del grafico\n\n# Creando el SVM con sus diferentes m\u00e9todos\nC = 1.0  # parametro de regulacion SVM \nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\nlin_svc = svm.LinearSVC(C=C).fit(X, y)\n\n# crear el area para graficar\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# titulos de los graficos\ntitles = ['SVC con el motor lineal',\n          'LinearSVC',\n          'SVC con el motor RBF',\n          'SVC con el motor polinomial']\n\n\nfor i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n    # Realizando el gr\u00e1fico, se le asigna un color a cada punto\n    plt.subplot(2, 2, i + 1)\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n    # Graficando tambien los puntos de datos\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('largo del petalo')\n    plt.ylabel('ancho del petalo')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(titles[i])\n\nplt.show()",
            "title": "SVM o M\u00e1quinas de vectores de soporte"
        },
        {
            "location": "/ML/#knn-o-k-vecinos-mas-cercanos",
            "text": "Este es un m\u00e9todo de clasificaci\u00f3n no param\u00e9trico, que estima el valor de la probabilidad a posteriori de que un elemento $x$ pertenezca a una clase en particular a partir de la informaci\u00f3n proporcionada por el conjunto de prototipos.\nLa regresi\u00f3n  KNN  se calcula simplemente tomando el promedio del punto k m\u00e1s cercano al punto que se est\u00e1 probando.   \n# Creando el dataset iris\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# importando KNN \nfrom sklearn.neighbors import KNeighborsRegressor\n\nknnr = KNeighborsRegressor(n_neighbors=10) # Creando el modelo con 10 vecinos\nknnr.fit(X, y) # Ajustando el modelo\n\n# Verificando el error medio del modelo\nprint(\"El error medio del modelo es: {:.2f}\".format(np.power(y - knnr.predict(X),\n2).mean()))\n\nEl error medio del modelo es: 0.02",
            "title": "KNN o k vecinos m\u00e1s cercanos"
        },
        {
            "location": "/ML/#k-means",
            "text": "K-means  es probablemente uno de los algoritmos de agrupamiento m\u00e1s conocidos y, en un sentido m\u00e1s amplio, una de las t\u00e9cnicas de aprendizaje no supervisado m\u00e1s conocidas. K-means  es en realidad un  algoritmo  muy simple que funciona para reducir al m\u00ednimo la suma de las distancias cuadradas desde la media dentro del agrupamiento. Para hacer esto establece primero un n\u00famero previamente especificado de conglomerados, K, y luego va asignando cada observaci\u00f3n a la agrupaci\u00f3n m\u00e1s cercana de acuerdo a su media. Veamos el ejemplo  # Creando el dataset\ngrupos, pos_correcta = datasets.make_blobs(1000, centers=3,\ncluster_std=1.75)\n\n# Graficando los grupos de datos\nf, ax = plt.subplots(figsize=(7, 5))\ncolores = ['r', 'g', 'b']\n\nfor i in range(3):\n    p = grupos[pos_correcta == i]\n    ax.scatter(p[:,0], p[:,1], c=colores[i],\n               label=\"Grupo {}\".format(i))\n\nax.set_title(\"Agrupamiento perfecto\")\nax.legend()\n\nplt.show()   # importando KMeans\nfrom sklearn.cluster import KMeans\n\n# Creando el modelo\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(grupos) # Ajustando el modelo\n\n# verificando los centros de los grupos\nkmeans.cluster_centers_\n\n# Graficando segun modelo\nf, ax = plt.subplots(figsize=(7, 5))\ncolores = ['r', 'g', 'b']\n\nfor i in range(3):\n    p = grupos[pos_correcta == i]\n    ax.scatter(p[:,0], p[:,1], c=colores[i],\n               label=\"Grupo {}\".format(i))\n\nax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n           s=100, color='black', label='Centros')\n\nax.set_title(\"Agrupamiento s/modelo\")\nax.legend()\n\nplt.show()",
            "title": "K-means"
        },
        {
            "location": "/about/",
            "text": "IAAR\n es la comunidad argentina de inteligencia artificial\n\n\nAgrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo \u00e9tico y humanitario de las tecnolog\u00edas cognitivas.\n\n\nObjetivos\n\n\n\n\nImpulsar el desarrollo y apoyar emprendimientos escalables, contribuyendo a la industria argentina en general, y a la Industria del Conocimiento argentina en particular.\n\n\nEntrenar y formar profesionales en Inteligencia Artificial y Ciencia de Datos que el pa\u00eds necesita y necesitar\u00e1 en un mundo cada vez m\u00e1s competitivo.\n\n\nCrear e incubar nuevos grupos interdisciplinarios de investigaci\u00f3n que exporten producci\u00f3n cient\u00edfica hacia el mundo.\n\n\nDivulgar el concepto, la importancia y los impactos tecnol\u00f3gicos, sociales y econ\u00f3micos de la Inteligencia Artificial al p\u00fablico argentino en general, ayudando a periodistas y difusores a lograr \u00e9ste cometido sin sensacionalismos.\n\n\n\n\nValores\n\n\n\n\nInclusi\u00f3n:\n Tecnolog\u00eda al servicio de la comunidad en su conjunto.\n\n\nDesarrollo:\n Industria argentina para el mundo.\n\n\nEmpat\u00eda:\n La vision de todos los afectados es tenida en cuenta.\n\n\nComunicaci\u00f3n:\n Abierta, honesta, directa y efectiva.\n\n\nTransparencia:\n Todas las operaciones estan abiertas a la comunidad.",
            "title": "Sobre IAAR"
        },
        {
            "location": "/about/#iaar-es-la-comunidad-argentina-de-inteligencia-artificial",
            "text": "Agrupa a ingenieros, desarrolladores, emprendedores, investigadores, entidades gubernamentales y empresas en pos del desarrollo \u00e9tico y humanitario de las tecnolog\u00edas cognitivas.",
            "title": "IAAR es la comunidad argentina de inteligencia artificial"
        },
        {
            "location": "/about/#objetivos",
            "text": "Impulsar el desarrollo y apoyar emprendimientos escalables, contribuyendo a la industria argentina en general, y a la Industria del Conocimiento argentina en particular.  Entrenar y formar profesionales en Inteligencia Artificial y Ciencia de Datos que el pa\u00eds necesita y necesitar\u00e1 en un mundo cada vez m\u00e1s competitivo.  Crear e incubar nuevos grupos interdisciplinarios de investigaci\u00f3n que exporten producci\u00f3n cient\u00edfica hacia el mundo.  Divulgar el concepto, la importancia y los impactos tecnol\u00f3gicos, sociales y econ\u00f3micos de la Inteligencia Artificial al p\u00fablico argentino en general, ayudando a periodistas y difusores a lograr \u00e9ste cometido sin sensacionalismos.",
            "title": "Objetivos"
        },
        {
            "location": "/about/#valores",
            "text": "Inclusi\u00f3n:  Tecnolog\u00eda al servicio de la comunidad en su conjunto.  Desarrollo:  Industria argentina para el mundo.  Empat\u00eda:  La vision de todos los afectados es tenida en cuenta.  Comunicaci\u00f3n:  Abierta, honesta, directa y efectiva.  Transparencia:  Todas las operaciones estan abiertas a la comunidad.",
            "title": "Valores"
        },
        {
            "location": "/contact/",
            "text": "Contacto\n\n\nPonerse en contacto con nosotros es muy f\u00e1cil. Completa el siguiente formulario:\n\n\n\n    \n\n        \nSu nombre\n\n        \n\n    \n\n    \n\n        \nSu email\n\n        \n\n    \n\n    \n\n        \nMensaje",
            "title": "Contacto"
        },
        {
            "location": "/contact/#contacto",
            "text": "Ponerse en contacto con nosotros es muy f\u00e1cil. Completa el siguiente formulario:  \n     \n         Su nombre \n         \n     \n     \n         Su email \n         \n     \n     \n         Mensaje",
            "title": "Contacto"
        }
    ]
}